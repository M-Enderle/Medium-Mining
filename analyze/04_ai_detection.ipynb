{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "455453cc",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56c3fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os, sys\n",
    "import platform\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path.cwd().resolve().parent\n",
    "analyze_root = project_root / \"analyze\"\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "if analyze_root not in sys.path:\n",
    "    sys.path.append(str(analyze_root))\n",
    "\n",
    "import torch\n",
    "torch._dynamo.disable()\n",
    "\n",
    "import numpy as np\t\n",
    "from analyze.generated_text_detector.generated_text_detector.utils.text_detector import GeneratedTextDetector\n",
    "\n",
    "from database.database import engine\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from scipy.stats import ttest_ind\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53aee9d",
   "metadata": {},
   "source": [
    "### Preprocess Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d0a27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unmark_element(markdown_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Cleans and strips markdown content, leaving behind only the semantic text\n",
    "    ready for an embedding model.\n",
    "\n",
    "    Args:\n",
    "        markdown_text: The raw markdown string.\n",
    "\n",
    "    Returns:\n",
    "        A cleaned text string.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- 1. Initial Cleaning and Normalization ---\n",
    "\n",
    "    # 1.1 REMOVE LINKS AND IMAGE TAGS: Remove the pattern [text](url) and ![text](url)\n",
    "    text = re.sub(r'\\!?\\[.*?\\]\\s*\\(.*?\\)', '', markdown_text, flags=re.DOTALL)\n",
    "    text = re.sub(r'Zoom image will be displayed', '', text)\n",
    "    text = re.sub(r'http[s]?://miro.medium.com/v2/resize:.*?\\.png', '', text)\n",
    "\n",
    "    # 1.2 Remove Extraneous Backslashes (e.g., escaping in \\- or \\.)\n",
    "    text = re.sub(r'\\\\-', '-', text)\n",
    "    text = re.sub(r'\\\\([`*_{}\\[\\]()#+.!])', r'\\1', text)\n",
    "    \n",
    "    # 1.3 Normalize Newlines: Convert multiple newlines/whitespace into a single space\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # --- 2. Markdown Structure Stripping ---\n",
    "\n",
    "    # 2.1 Remove Headings (Setext style: === or --- lines)\n",
    "    text = re.sub(r'\\n[=-]{2,}\\s*$', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # 2.2 Remove Blockquotes/Code Fences (Markers: > and ```)\n",
    "    text = re.sub(r'^\\s*>\\s?', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'```[a-zA-Z]*\\s*', ' ', text)\n",
    "    text = re.sub(r'`', ' ', text)\n",
    "    \n",
    "    # 2.3 Remove List Markers (e.g., 1. or - or *)\n",
    "    text = re.sub(r'^\\s*\\d+\\.\\s', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'^\\s*[\\-\\*]\\s', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # 2.4 Remove Emphasis Markers (e.g., **, *, __, _)\n",
    "    text = re.sub(r'(\\*\\*|__)', '', text) # Bold/Strong\n",
    "    text = re.sub(r'(\\*|_)', '', text)    # Italic/Emphasis\n",
    "\n",
    "    # 2.5 Remove remaining HTML tags (like '<hibernate-mapping>') which are often in code\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    \n",
    "    # --- 3. Final Text Polishing ---\n",
    "\n",
    "    # 3.1 Normalize Whitespace again: Collapse all multiple spaces into one\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    # 3.2 Lowercasing (Optional but recommended for many embedding models)\n",
    "    text = text.lower()\n",
    "\n",
    "    return text.replace(\"Press enter or click to view image in full size.\", \"\").strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677b0bd2",
   "metadata": {},
   "source": [
    "### Sentence Analysis Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b29668",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window_words(text, size=256, step=1):\n",
    "    \"\"\"Split text into overlapping chunks of approx. `size` characters.\"\"\"\n",
    "    words = text.split()\n",
    "    for i in range(0, len(words), step):\n",
    "        chunk = words[i]\n",
    "        j = 0\n",
    "        while len(chunk) < size and i + j + 1 < len(words):\n",
    "            j += 1\n",
    "            if len(chunk) + len(words[i + j]) + 1 <= size:  # +1 for space\n",
    "                chunk += \" \" + words[i + j]\n",
    "            else:\n",
    "                break\n",
    "        yield chunk    \n",
    "\n",
    "\n",
    "def single_sentence_chunks(text, size=256):\n",
    "    \"\"\"Split text into individual sentences.\"\"\"\n",
    "    sentences = re.split(r'(?<=[.!?]) +', text)\n",
    "    for sentence in sentences:\n",
    "        if len(sentence) <= size:\n",
    "            yield sentence\n",
    "        else:\n",
    "            # cut sentence into smaller parts if too long, split by words, end when size is reached or we find a punctuation\n",
    "            words = sentence.split()\n",
    "            sub_chunk = \"\"\n",
    "            for word in words:\n",
    "                if len(sub_chunk) + len(word) + 1 <= size or \".!?\" in word:\n",
    "                    if sub_chunk:\n",
    "                        sub_chunk += \" \"\n",
    "                    sub_chunk += word\n",
    "                else:\n",
    "                    if sub_chunk:\n",
    "                        yield sub_chunk\n",
    "                    sub_chunk = word\n",
    "            if sub_chunk:\n",
    "                yield sub_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f34451",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_scores(text, chunks, results, sliding_windows_approach=True):\n",
    "    \"\"\"Aggregate model scores per word (supports sliding or non-sliding).\"\"\"\n",
    "    words = text.split()\n",
    "    scores = np.zeros((len(words), 2), dtype=float)\n",
    "\n",
    "    word_index = 0\n",
    "    for i, res in enumerate(results):\n",
    "        score = res['score']\n",
    "        # if res['label'] == 'Real':\n",
    "        #     score = 1 - score\n",
    "\n",
    "        chunk_words = chunks[i].split()\n",
    "\n",
    "        for j, _ in enumerate(chunk_words):\n",
    "            idx = (i + j) if sliding_windows_approach else word_index\n",
    "            if idx >= len(scores):\n",
    "                break\n",
    "\n",
    "            scores[idx, 0] += score\n",
    "            scores[idx, 1] += 1\n",
    "\n",
    "            if not sliding_windows_approach:\n",
    "                word_index += 1\n",
    "\n",
    "    # Normalize counts\n",
    "    valid = scores[:, 1] > 0\n",
    "    scores[valid, 0] /= scores[valid, 1]\n",
    "    scores[~valid, 0] = np.nan\n",
    "    scores = scores[:, :1]\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "def visualize_scores(text, scores):\n",
    "    \"\"\"Display text with word-level color highlighting based on score.\"\"\"\n",
    "    highlighted_text = \"\"\n",
    "    words = text.split()\n",
    "\n",
    "    MAX_VAL = 200\n",
    "    WHITE_VAL = 255\n",
    "\n",
    "    for i, word in enumerate(words):\n",
    "        score = scores[i, 0] if i < len(scores) else 0\n",
    "\n",
    "        if score <= 0.5:\n",
    "            ratio = score / 0.5  \n",
    "            r = int(WHITE_VAL * ratio)\n",
    "            g = int(MAX_VAL + (WHITE_VAL - MAX_VAL) * ratio)\n",
    "            b = int(WHITE_VAL * ratio)\n",
    "        else:\n",
    "            ratio = (score - 0.5) / 0.5\n",
    "            r = int(WHITE_VAL - (WHITE_VAL - MAX_VAL) * ratio)\n",
    "            g = int(WHITE_VAL * (1 - ratio))\n",
    "            b = int(WHITE_VAL * (1 - ratio))\n",
    "\n",
    "        color = f\"rgb({r},{g},{b})\"\n",
    "        highlighted_text += f'<span style=\"background-color: {color}\">{word} </span>'\n",
    "\n",
    "    display(HTML(f\"<div style='line-height:1.6; font-size:16px'>{highlighted_text}</div>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485c28e5",
   "metadata": {},
   "source": [
    "### AI Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546090cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "class DetectorWrapper:\n",
    "    def __init__(self, model_name=\"SuperAnnotate/ai-detector-low-fpr\", device=device, max_len=512):\n",
    "        if device is None:\n",
    "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.detector = GeneratedTextDetector(\n",
    "            model_name,\n",
    "            device=device,\n",
    "            preprocessing=True,\n",
    "            max_len=max_len,\n",
    "        )\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __call__(self, chunks):\n",
    "        scores = [s[0][-1] for s in [self.detector.detect(c) for c in chunks]]\n",
    "        # score = self.detector.detect(text)\n",
    "        labels = [\"Fake\" if score >= 0.5 else \"Real\" for score in scores]\n",
    "        return [{\"label\": label, \"score\": score} for label, score in zip(labels, scores)]\n",
    "    \n",
    "pipe = DetectorWrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556d6666",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ai_detection(text: str, pipe: callable, size: int =256, approach: str = \"word\", vis: bool =False, verbose: bool =False):\n",
    "    \"\"\"Full pipeline: preprocess → split → infer → aggregate → visualize.\"\"\"\n",
    "    cleaned = unmark_element(text)\n",
    "    \n",
    "    if approach == \"sentence\":\n",
    "        chunks = list(single_sentence_chunks(cleaned, size=size))\n",
    "    elif approach == \"words\":\n",
    "        chunks = list(sliding_window_words(cleaned, size=size))\n",
    "    else:\n",
    "        raise ValueError(\"Invalid approach. Choose from 'sentence' or 'words'.\")\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Text length: {len(cleaned)} characters.\")\n",
    "        print(f\"Text split into {len(chunks)} chunks.\")\n",
    "\n",
    "    results = pipe(chunks)\n",
    "    scores = aggregate_scores(cleaned, chunks, results, \"window\" in approach)\n",
    "\n",
    "    avg_score = np.nanmean(scores[:, 0])\n",
    "    \n",
    "    if vis:\n",
    "        visualize_scores(cleaned, scores)\n",
    "    return scores, avg_score\n",
    "\n",
    "\n",
    "# --- 7. Get scores and average ---\n",
    "def get_ai_scores(text, pipe, vis=False):\n",
    "    text = unmark_element(text)\n",
    "    word_approach_window, word_approach_window_avg = ai_detection(text, pipe, size=94, approach=\"words\", vis=vis) # Average word length ~4.7, average sentence length ~20 words --> 4.7 * 20 = 94\n",
    "    sentence_approach, sentence_approach_avg = ai_detection(text, pipe, size=256, approach=\"sentence\", vis=vis)\n",
    "    average_ai_score = (word_approach_window_avg + sentence_approach_avg) / 2\n",
    "    average_approach = (word_approach_window + sentence_approach) / 2\n",
    "    return word_approach_window, sentence_approach, average_approach, average_ai_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc05b1da",
   "metadata": {},
   "source": [
    "### Test one case (for report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cd1b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql(\"SELECT * FROM medium_articles\", engine)\n",
    "random_text = df[df['full_article_text'].str.len() < 2000]['full_article_text'].sample().iloc[0]\n",
    "\n",
    "vis = False\n",
    "wa, sa, avg_approach, average_ai_score = get_ai_scores(random_text, pipe, vis=vis)\n",
    "\n",
    "print(\"Average AI Score:\", average_ai_score)\n",
    "_ = visualize_scores(random_text, avg_approach)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a480e753",
   "metadata": {},
   "source": [
    "### Run AI Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553e7079",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = min(len(df[df['is_free'] == True]), len(df[df['is_free'] == False]))\n",
    "\n",
    "free_articles = df[df['is_free'] == True].sample(sample_size, random_state=42)\n",
    "paid_articles = df[df['is_free'] == False].sample(sample_size, random_state=42)\n",
    "balanced_df = pd.concat([free_articles, paid_articles]).reset_index(drop=True)\n",
    "\n",
    "balanced_df['word_approach_scores'] = None\n",
    "balanced_df['sentence_approach_scores'] = None\n",
    "balanced_df['average_approach_scores'] = None\n",
    "balanced_df['average_ai_score'] = np.nan\n",
    "\n",
    "for i, row in tqdm(balanced_df.iterrows(), total=balanced_df.shape[0]):\n",
    "    chunks = row['full_article_text']\n",
    "    if pd.isna(chunks) or len(chunks.strip()) == 0:\n",
    "        balanced_df.at[i, 'word_approach_scores'] = (np.array([]), np.nan, np.array([]), np.nan)\n",
    "        balanced_df.at[i, 'sentence_approach_scores'] = (np.array([]), np.nan, np.array([]), np.nan)\n",
    "        balanced_df.at[i, 'average_approach_scores'] = (np.array([]), np.nan, np.array([]), np.nan)\n",
    "        balanced_df.at[i, 'average_ai_score'] = np.nan\n",
    "        print(\"Empty text, skipping.\")\n",
    "        continue\n",
    "    try:\n",
    "        wa, sa, avg_approach, average_ai_score = get_ai_scores(chunks, pipe, vis=False)\n",
    "        balanced_df.at[i, 'word_approach_scores'] = wa.tolist()\n",
    "        balanced_df.at[i, 'sentence_approach_scores'] = sa.tolist()\n",
    "        balanced_df.at[i, 'average_approach_scores'] = avg_approach.tolist()\n",
    "        balanced_df.at[i, 'average_ai_score'] = average_ai_score\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing article {i+1}: {e}\")\n",
    "\n",
    "# save df as pickle\n",
    "balanced_df.to_pickle(\".\\grammar_analysis\\medium_articles_with_ai_scores.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cce027",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33116a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_df = pd.read_pickle(\".\\grammar_analysis\\medium_articles_with_ai_scores.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4c72f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "free_df = balanced_df[balanced_df['is_free'] == True]\n",
    "paid_df = balanced_df[balanced_df['is_free'] == False]\n",
    "\n",
    "free_scores = free_df['average_ai_score'].dropna()\n",
    "paid_scores = paid_df['average_ai_score'].dropna()\n",
    "\n",
    "# set sample size to the smaller of the two groups\n",
    "min_size = min(len(free_scores), len(paid_scores))\n",
    "free_scores = free_scores.sample(min_size, random_state=42)\n",
    "paid_scores = paid_scores.sample(min_size, random_state=42)\n",
    "\n",
    "t_stat, p_value = ttest_ind(free_scores, paid_scores, equal_var=False)\n",
    "print(f\"\\nT-test between Free and Paid articles:\")\n",
    "print(f\"T-statistic: {t_stat:.4f}, P-value: {p_value:.4e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d961170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure we have independent DataFrames\n",
    "free_articles_df = free_df.copy()\n",
    "paid_articles_df = paid_df.copy()\n",
    "\n",
    "# filter out old articles\n",
    "cutoff_date = datetime.datetime(2012, 1, 1)\n",
    "free_articles_df = free_articles_df[free_articles_df[\"date_published\"] > cutoff_date]\n",
    "paid_articles_df = paid_articles_df[paid_articles_df[\"date_published\"] > cutoff_date]\n",
    "\n",
    "# extract year-month\n",
    "free_articles_df.loc[:, 'year_month'] = pd.to_datetime(free_articles_df['date_published']).dt.to_period('M')\n",
    "paid_articles_df.loc[:, 'year_month'] = pd.to_datetime(paid_articles_df['date_published']).dt.to_period('M')\n",
    "\n",
    "# group by month and compute average AI score\n",
    "free_monthly_ai = free_articles_df.groupby('year_month')['average_ai_score'].mean()\n",
    "paid_monthly_ai = paid_articles_df.groupby('year_month')['average_ai_score'].mean()\n",
    "\n",
    "# convert to DataFrames\n",
    "free_data = pd.DataFrame({\n",
    "    'month': free_monthly_ai.index.to_timestamp(),\n",
    "    'avg_ai_score': free_monthly_ai.values,\n",
    "    'type': 'Free Articles'\n",
    "})\n",
    "paid_data = pd.DataFrame({\n",
    "    'month': paid_monthly_ai.index.to_timestamp(),\n",
    "    'avg_ai_score': paid_monthly_ai.values,\n",
    "    'type': 'Paid Articles'\n",
    "})\n",
    "combined_data = pd.concat([free_data, paid_data])\n",
    "\n",
    "window = 12\n",
    "\n",
    "# compute rolling averages for trend lines (6-month window)\n",
    "combined_data['trend'] = combined_data.groupby('type')['avg_ai_score'].transform(\n",
    "    lambda x: x.rolling(window=window, min_periods=1).mean()\n",
    ")\n",
    "\n",
    "# plot main lines\n",
    "plt.figure(figsize=(12, 6))\n",
    "datapoints = sns.lineplot(\n",
    "    data=combined_data,\n",
    "    x='month',\n",
    "    y='avg_ai_score',\n",
    "    hue='type',\n",
    "    style='type',\n",
    "    marker='o',\n",
    "    linewidth=1.5,\n",
    "    alpha=0.5\n",
    ")\n",
    "\n",
    "for line in datapoints.lines:\n",
    "    lbl = line.get_label()\n",
    "    if \"trend\" in lbl:\n",
    "        continue\n",
    "    line.set_label(f\"{lbl} (data points)\")\n",
    "\n",
    "# plot trend lines (thicker and darker)\n",
    "trend = sns.lineplot(\n",
    "    data=combined_data,\n",
    "    x='month',\n",
    "    y='trend',\n",
    "    hue='type',\n",
    "    linewidth=3\n",
    ")\n",
    "\n",
    "for line in trend.lines:\n",
    "    lbl = line.get_label()\n",
    "    if \"data points\" in lbl:\n",
    "        continue\n",
    "    line.set_label(f\"{lbl} (trend over {window} months)\")\n",
    "\n",
    "plt.legend(title='Article Type')\n",
    "\n",
    "plt.title('Average AI Score Over Time (Free vs Paid Articles)')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Average AI Score')\n",
    "sns.despine()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('average_ai_score_over_time_with_trend.png', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d394013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign half-year as string label\n",
    "def to_half_year(date):\n",
    "    year = date.year\n",
    "    half = 1 if date.month <= 6 else 2\n",
    "    return f\"{year}-H{half}\"\n",
    "\n",
    "free_articles_df['half_year'] = pd.to_datetime(free_articles_df['date_published']).apply(to_half_year)\n",
    "paid_articles_df['half_year'] = pd.to_datetime(paid_articles_df['date_published']).apply(to_half_year)\n",
    "\n",
    "# combine into one DataFrame\n",
    "combined_data_half = pd.concat([\n",
    "    free_articles_df.assign(type='Free Articles'),\n",
    "    paid_articles_df.assign(type='Paid Articles')\n",
    "])\n",
    "\n",
    "# filter by cutoff\n",
    "combined_data_half = combined_data_half[\n",
    "    pd.to_datetime(combined_data_half['date_published']) >= pd.Timestamp('2017-01-01')\n",
    "]\n",
    "\n",
    "# sort by time order for prettier axis\n",
    "half_order = sorted(combined_data_half['half_year'].unique())\n",
    "\n",
    "# plot boxplot\n",
    "plt.figure(figsize=(12, 7))\n",
    "sns.boxplot(\n",
    "    data=combined_data_half,\n",
    "    x='half_year',\n",
    "    y='average_ai_score',\n",
    "    hue='type',\n",
    "    order=half_order\n",
    ")\n",
    "plt.title('AI Score Distribution Over Half-Year Periods (Free vs Paid Articles)')\n",
    "plt.xlabel('Half-Year Period')\n",
    "plt.ylabel('AI Score')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Article Type', loc='upper left')\n",
    "sns.despine()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig('ai_score_boxplot_half_year.png', dpi=300)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ea7b72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medium-mining-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
