{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4635c4ca",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc8b417",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os, sys\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "import language_tool_python\n",
    "from sqlalchemy import text\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.autonotebook import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693d3d50",
   "metadata": {},
   "source": [
    "### Add project to path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cfb2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = Path.cwd()\n",
    "project_root = current_dir.parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "\n",
    "\n",
    "from src.database.database import engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3b6005",
   "metadata": {},
   "source": [
    "### Preprocess Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b65196",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unmark_element(markdown_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Cleans and strips markdown content, leaving behind only the semantic text\n",
    "    ready for an embedding model.\n",
    "\n",
    "    Args:\n",
    "        markdown_text: The raw markdown string.\n",
    "\n",
    "    Returns:\n",
    "        A cleaned text string.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- 1. Initial Cleaning and Normalization ---\n",
    "\n",
    "    # 1.1 REMOVE LINKS AND IMAGE TAGS: Remove the pattern [text](url) and ![text](url)\n",
    "    text = re.sub(r'\\!?\\[.*?\\]\\s*\\(.*?\\)', '', markdown_text, flags=re.DOTALL)\n",
    "    text = re.sub(r'Zoom image will be displayed', '', text)\n",
    "    text = re.sub(r'http[s]?://miro.medium.com/v2/resize:.*?\\.png', '', text)\n",
    "\n",
    "    # 1.2 Remove Extraneous Backslashes (e.g., escaping in \\- or \\.)\n",
    "    text = re.sub(r'\\\\-', '-', text)\n",
    "    text = re.sub(r'\\\\([`*_{}\\[\\]()#+.!])', r'\\1', text)\n",
    "    \n",
    "    # 1.3 Normalize Newlines: Convert multiple newlines/whitespace into a single space\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # --- 2. Markdown Structure Stripping ---\n",
    "\n",
    "    # 2.1 Remove Headings (Setext style: === or --- lines)\n",
    "    text = re.sub(r'\\n[=-]{2,}\\s*$', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # 2.2 Remove Blockquotes/Code Fences (Markers: > and ```)\n",
    "    text = re.sub(r'^\\s*>\\s?', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'```[a-zA-Z]*\\s*', ' ', text)\n",
    "    text = re.sub(r'`', ' ', text)\n",
    "    \n",
    "    # 2.3 Remove List Markers (e.g., 1. or - or *)\n",
    "    text = re.sub(r'^\\s*\\d+\\.\\s', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'^\\s*[\\-\\*]\\s', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # 2.4 Remove Emphasis Markers (e.g., **, *, __, _)\n",
    "    text = re.sub(r'(\\*\\*|__)', '', text) # Bold/Strong\n",
    "    text = re.sub(r'(\\*|_)', '', text)    # Italic/Emphasis\n",
    "\n",
    "    # 2.5 Remove remaining HTML tags (like '<hibernate-mapping>') which are often in code\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    \n",
    "    # --- 3. Final Text Polishing ---\n",
    "\n",
    "    # 3.1 Normalize Whitespace again: Collapse all multiple spaces into one\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    # 3.2 Lowercasing (Optional but recommended for many embedding models)\n",
    "    text = text.lower()\n",
    "\n",
    "    return text.replace(\"Press enter or click to view image in full size.\", \"\").strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9809dbcf",
   "metadata": {},
   "source": [
    "### Setup languagetool server (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e47607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this to true if you want to run a local LanguageTool server\n",
    "# https://internal1.languagetool.org/snapshots/LanguageTool-latest-snapshot.zip\n",
    "local_server = False\n",
    "language_tool_path = r\"C:\\Path\\To\\LanguageTool\\languagetool-server.jar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6ac152",
   "metadata": {},
   "outputs": [],
   "source": [
    "if local_server:\n",
    "\ttry:\n",
    "\t\t# Start LanguageTool server\n",
    "\t\tlt_process = subprocess.Popen(\n",
    "\t\t\t['cmd', '/c', 'start', '', language_tool_path],\n",
    "\t\t\tstdout=subprocess.PIPE,\n",
    "\t\t\tstderr=subprocess.PIPE\n",
    "\t\t)\n",
    "\t\ttime.sleep(5)  # Wait for server to start\n",
    "\t\tlt_server = 'http://localhost:8081'\n",
    "\texcept Exception as e:\n",
    "\t\tprint(f\"Error starting LanguageTool server: {e}\")\n",
    "\t\tprint(f\"Download it from\\nhttps://internal1.languagetool.org/snapshots/LanguageTool-latest-snapshot.zip\")\n",
    "\t\tlt_server = None\n",
    "else:\n",
    "\tlt_server = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7c0c0b",
   "metadata": {},
   "source": [
    "### Detect grammatical errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd6be5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGUAGE_TOOL = language_tool_python.LanguageTool(\"en-US\", remote_server=lt_server)\n",
    "\n",
    "def analyze_text(text, tool=LANGUAGE_TOOL):\n",
    "\n",
    "    def ignore_spelling_mistakes(rule):\n",
    "        return rule.message == \"Possible spelling mistake found.\"\n",
    "\n",
    "    def disable_rule_code_in_text(rule):\n",
    "        disabled_code_in_text_rules = {\n",
    "                \"LC_AFTER_PERIOD\",\n",
    "                \"CAPITALIZATION\",\n",
    "                \"CAPITALIZATION_NNP_DERIVED\",\n",
    "                \"COMMA_PERIOD_CONFUSION\",\n",
    "                \"COMMA_PERIOD\",\n",
    "                \"DOUBLE_PUNCTUATION\",\n",
    "                \"ELLIPSIS\",\n",
    "                \"MISSING_HYPHEN\",\n",
    "                \"APOSTROPHE_IN_DATES\",\n",
    "                \"APOSTROPHE_PLURAL_\",\n",
    "                \"WORD_CONTAINS_UNDERSCORE\",\n",
    "                \"NON_STANDARD_WORD\",\n",
    "                \"NON_STANDARD_ALPHABETIC_CHARACTERS\",\n",
    "                \"NON_ENGLISH_CHARACTER_IN_A_WORD\",\n",
    "                \"UNICODE_CASING\",\n",
    "                \"ID_CASING\",\n",
    "                \"SPACE_BEFORE_PARENTHESIS\",\n",
    "                \"CONSECUTIVE_SPACES\",\n",
    "                \"SPACE_BETWEEN_NUMBER_AND_WORD\",\n",
    "                \"UNIT_SPACE\",\n",
    "                \"CURRENCY_SPACE\",\n",
    "                \"HYPHEN_TO_EN\",\n",
    "                \"HYPHEN_TO_EM_OR_COMMA\",\n",
    "                \"TWO_HYPHENS\",\n",
    "                \"NON_STANDARD_COMMA\",\n",
    "                \"SENT_START_NUM\",\n",
    "                \"SENT_START_CONJUNCTION\",\n",
    "                \"EN_QUOTES\",\n",
    "                \"TYPOGRAPHICAL_APOSTROPHE\",\n",
    "                \"DOUBLE_APOSTROPHE\",\n",
    "                \"APOSTROPHE_VS_QUOTE\",\n",
    "                \"GERMAN_QUOTES\",\n",
    "                \"OXFORD_COMMA_CASING\",\n",
    "                \"SERIAL_COMMA_OFF\",\n",
    "                \"SERIAL_COMMA_ON\",\n",
    "                \"EN_SPLIT_WORDS_HYPHEN\",\n",
    "                \"SENTENCE_FRAGMENT\",\n",
    "                \"AI_EN_LECTOR_MISSING_PUNCTUATION\",\n",
    "            }\n",
    "        return rule.ruleId in disabled_code_in_text_rules\n",
    "\n",
    "    with tool:\n",
    "        matches = tool.check(text)\n",
    "\n",
    "    filters = [ignore_spelling_mistakes, disable_rule_code_in_text]\n",
    "\n",
    "    filtered = [rule for rule in matches if not any(f(rule) for f in filters)]\n",
    "    grammar_count = len(filtered)\n",
    "\n",
    "    # Count per category\n",
    "    grammar_category_counts = {}\n",
    "    for match in filtered:\n",
    "        cat = match.category or \"Unknown\"\n",
    "        grammar_category_counts[cat] = grammar_category_counts.get(cat, 0) + 1\n",
    "\n",
    "    return {\n",
    "        \"grammar_count\": grammar_count,\n",
    "        \"grammar_category_counts\": grammar_category_counts\n",
    "    }, True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0a5737",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql(\"SELECT * FROM medium_articles LIMIT 5\", engine)\n",
    "tqdm.pandas(desc=\"Processing articles with grammar analysis\")\n",
    "\n",
    "def process_text(full_text):\n",
    "\tunmarked = unmark_element(full_text)\n",
    "\tresult, success = analyze_text(unmarked)\n",
    "\tif success:\n",
    "\t\treturn result\n",
    "\telse:\n",
    "\t\treturn None\n",
    "\t\n",
    "results_series = df['full_article_text'].progress_apply(process_text)\n",
    "failed_mask = results_series.isna()\n",
    "failed = df[failed_mask]['full_article_text'].tolist()\n",
    "print(f\"Processed {len(df)} articles, {len(failed)} failed.\")\n",
    "\n",
    "successful_results_df = results_series.dropna().apply(pd.Series)\n",
    "\n",
    "df_with_grammar = df.join(successful_results_df)\n",
    "df_with_grammar.to_pickle(r\".\\grammar_analysis\\medium_articles_with_grammar_analysis.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958ab2f6",
   "metadata": {},
   "source": [
    "### Analyze grammatical errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdce9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_path = r\".\\grammar_analysis\\medium_articles_with_grammar_analysis.pkl\"\n",
    "df = pd.read_pickle(df_path)\n",
    "print(\"Processed articles:\",len(df[df['grammar_count'].notna()]))\n",
    "print(\"Failed articles:\",len(df[df['grammar_count'].isna()]))\n",
    "print(\"Columns in DataFrame:\")\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7912042e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_grammar = df.copy()\n",
    "df_with_grammar = df_with_grammar[df_with_grammar['text_length'] > 0]\n",
    "\n",
    "free_posts = df_with_grammar[df_with_grammar['is_free'] == True]\n",
    "paid_posts = df_with_grammar[df_with_grammar['is_free'] == False]\n",
    "min_size = min(len(free_posts), len(paid_posts))\n",
    "print(f\"Using sample size of {min_size} for both groups.\")\n",
    "\n",
    "free_sample = free_posts.sample(min_size, random_state=42)\n",
    "paid_sample = paid_posts.sample(min_size, random_state=42)\n",
    "free_grammar = free_sample['grammar_count']\n",
    "paid_grammar = paid_sample['grammar_count']\n",
    "\n",
    "t_stat, p_value = ttest_ind(free_grammar, paid_grammar, equal_var=False)\n",
    "print(f\"\\nT-test between Free and Paid articles for grammar_count:\")\n",
    "print(f\"T-statistic: {t_stat:.4f}, P-value: {p_value:.4e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae9b98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cohens_d(x, y):\n",
    "    n1 = len(x); n2 = len(y)\n",
    "    m1 = np.mean(x); m2 = np.mean(y)\n",
    "    s1 = np.std(x, ddof=1); s2 = np.std(y, ddof=1)\n",
    "    s_pooled = np.sqrt(((n1 - 1) * s1**2 + (n2 - 1) * s2**2) / (n1 + n2 - 2))\n",
    "    d = (m1 - m2) / s_pooled\n",
    "    return d, m1, m2, s1, s2, s_pooled\n",
    "\n",
    "def hedges_g(d, n1, n2):\n",
    "    df = n1 + n2 - 2\n",
    "    J = 1 - (3 / (4 * df - 1))\n",
    "    return d * J\n",
    "\n",
    "def cohens_d_ci_approx(d, n1, n2, alpha=0.05):\n",
    "    var_d = (n1 + n2) / (n1 * n2) + (d**2) / (2 * (n1 + n2))\n",
    "    se = np.sqrt(var_d)\n",
    "    z = stats.norm.ppf(1 - alpha/2)\n",
    "    lower = d - z * se\n",
    "    upper = d + z * se\n",
    "    return lower, upper, se\n",
    "\n",
    "# compute\n",
    "d, m1, m2, s1, s2, s_pooled = cohens_d(free_grammar.values, paid_grammar.values)\n",
    "g = hedges_g(d, len(free_grammar), len(paid_grammar))\n",
    "ci_lower, ci_upper, se = cohens_d_ci_approx(d, len(free_grammar), len(paid_grammar))\n",
    "\n",
    "print(f\"Cohen's d: {d:.4f}\")\n",
    "print(f\"Hedges' g (bias-corrected): {g:.4f}\")\n",
    "print(f\"Means -> free: {m1:.4f}, paid: {m2:.4f}\")\n",
    "print(f\"SDs   -> free: {s1:.4f}, paid: {s2:.4f}, pooled: {s_pooled:.4f}\")\n",
    "print(f\"Approx. 95% CI for d: [{ci_lower:.4f}, {ci_upper:.4f}] (SE={se:.4e})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2d6599",
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar_columns = [col for col in df.columns if col not in ['id', 'url_id', 'title', 'author_id', 'date_published', 'date_modified', 'date_created', 'description', 'publisher_type', 'is_free', 'claps', 'comments_count', 'full_article_text', 'read_time', 'type', 'tags', 'num_images', 'language', 'text_length', 'word_approach_scores', 'sentence_approach_scores', 'average_approach_scores', 'average_ai_score', 'grammar_count']]\n",
    "# normalize by text length if desired\n",
    "norm_df = pd.concat([free_posts, paid_posts], axis=0, ignore_index=True)\n",
    "for col in grammar_columns:\n",
    "\t# ensure word_count column exists (use words instead of chars)\n",
    "\tif 'word_count' not in norm_df.columns:\n",
    "\t\tnorm_df['word_count'] = norm_df['full_article_text'].fillna('').str.split().str.len().replace(0, np.nan)\n",
    "\n",
    "\t# normalize by word count (avoid division by zero)\n",
    "\tnorm_df[col] = norm_df[col] / norm_df['word_count']\n",
    "\n",
    "summary_list = []\n",
    "\n",
    "for col in grammar_columns:\n",
    "    for is_free, group in norm_df.groupby('is_free'):\n",
    "        values = group[col].dropna()\n",
    "        summary_list.append({\n",
    "            'Grammar Category': col,\n",
    "            'is_free': is_free,\n",
    "            'count': len(values),\n",
    "            'mean': values.mean(),\n",
    "            'median': values.median(),\n",
    "            'std': values.std(),\n",
    "            'min': values.min(),\n",
    "            'max': values.max(),\n",
    "            '25%': values.quantile(0.25),\n",
    "            '75%': values.quantile(0.75)\n",
    "        })\n",
    "\n",
    "normalized_summary = pd.DataFrame(summary_list)\n",
    "normalized_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233bfa6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_errors = (\n",
    "    norm_df.groupby('is_free')[grammar_columns]\n",
    "    .mean()\n",
    "    .T\n",
    "    .reset_index()\n",
    "    .melt(id_vars='index', var_name='is_free', value_name='Average Errors per Article')\n",
    ")\n",
    "\n",
    "avg_errors.rename(columns={'index': 'Grammar Category'}, inplace=True)\n",
    "\n",
    "mean_diff = (\n",
    "    normalized_summary.pivot(index='Grammar Category', columns='is_free', values='mean')\n",
    "    .assign(diff=lambda df_: df_[True] - df_[False])\n",
    "    .sort_values('diff', ascending=False)\n",
    ")\n",
    "sorted_categories = mean_diff.index.tolist()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "ax = sns.barplot(\n",
    "    data=avg_errors,\n",
    "    x='Grammar Category',\n",
    "    y='Average Errors per Article',\n",
    "    hue='is_free',\n",
    "    order=sorted_categories,\n",
    "    palette=list(reversed(sns.color_palette(None, 2)))\n",
    ")\n",
    "plt.title('Average Grammar Errors per Word (Free vs Paid)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yscale('log')\n",
    "plt.tight_layout()\n",
    "sns.despine()\n",
    "\n",
    "# --- Add labels on top of bars ---\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(\n",
    "        container,\n",
    "        fmt='%.2e',           \n",
    "        label_type='edge',    \n",
    "        fontsize=9,\n",
    "        padding=2\n",
    "    )\n",
    "\n",
    "plt.savefig(r'.\\grammar_analysis\\average_grammar_errors_per_word.png', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8923bb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medium-mining-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
