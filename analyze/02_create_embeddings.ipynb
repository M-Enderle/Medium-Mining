{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6035547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Moritz\\Desktop\\Medium-Mining\\.venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "from database.database import MediumArticle\n",
    "from database.database import get_session\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import os\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "from tensorboard.plugins import projector\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "session = get_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fb62bd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ce49005ebeb40d08af70839a3fd945f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles published since 2020-01-01 in English: 35185\n"
     ]
    }
   ],
   "source": [
    "# query all articles\n",
    "articles_df = pd.read_sql(session.query(MediumArticle).statement, session.bind)\n",
    "articles_df[\"text_length\"] = articles_df[\"full_article_text\"].apply(lambda x: len(x.split()))\n",
    "\n",
    "articles_filtered = articles_df[articles_df[\"date_published\"] > datetime.datetime(2020, 1, 1)]\n",
    "articles_filtered = articles_filtered[articles_filtered[\"language\"] == \"en\"]\n",
    "print(f\"Number of articles published since 2020-01-01 in English: {len(articles_filtered)}\")\n",
    "\n",
    "free_articles_df = articles_filtered[articles_filtered[\"is_free\"] == True]\n",
    "paid_articles_df = articles_filtered[articles_filtered[\"is_free\"] == False]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d9d569",
   "metadata": {},
   "source": [
    "## Build Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "027336f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed Free Articles:\n",
      "Free Article 1:\n",
      "this is a very rare combination anyone will explain in detail to use maven, struts2, hibernate and maven archetype as maven-archetype-webapp to create a end to end application. in this blog, i attempted to explain each step of creating a maven-based struts 2 + hibernate application. it was a time-consuming task, but at the end of the day, it was completed and delivered successfully. lets start our journey to go through and learn complete implementation. we are going to use eclipse as an editor to develop maven + struts2 + hibernate application. follow below steps to create our very first struts application project. 1. open eclipse. 2. click on file in top navigation menu and than click on new. there will be an option maven project start coming. select maven project will take you to the below screen. do not make any change and click on next. 3. in the filter section type maven-archetype-webapp will give you list of options to select. select below below second highlighted option red in color and click on next. 4. enter group id and artifact id as highlighted below red in color. click on next will create a project. 5. it will create below folder structure 6. open pom.xml and place below highlighted dependency red in color. hibernate implementation ======================== 7. lets configure hibernate first in the project. inside src we have main folder. right click on the main folder and create one java folder as shown below highlighted red in color. it will also create a folder hierarchy highlighted green in color. 8. right click on the folder highlighted green in color in previous step and create package with name com.javadoubts.practisehibernate in the same package create user.java class with below content. 9. in the same package and parallel to user.java class, create userdao.java class with below content. 10. inside src folder and parallel to java folder, create once more folder with the name of resources as highlighted below red in color. 11. paste below hibernate.cfg.xml and user.hbm.xml files inside resource folder created in above step having below inline content. hibernate.cfg.xml user.hbm.xml important note:after adding both the above xml files. right on resource folder select build path and than use as source folder. right click on resource folder -\\> build path -\\> use as source folder \\ contains all bean class mappings. \\ create mapping in between class and table. name attribute value represents to class name and table attribute value represents to mysql table name. \\ represents to class declare class level variable. name attribute value represents to class level variable and type represents to data type of that variable. \\ represents to column mysql table. name attribute value represents to column name. \\ represents to unique id attribute in class map to the primary key of the database table. \\ this helps to generate unique id for given column. 12. open mysql command prompt. create and data inside user table with the help of below query. > create table user(id int not null autoincrement, name varchar(20), primary key(id)); > > insert into user value(1, “john”) 13. open userdao.java class, right click and run as java application will give below output. struts implementation ===================== 14. in web.xml folder below content to load index.jsp file as soon as server starts up and also load struts2 configurations. 15. create struts.xml inside src/main/java folder highlighted as blue in color. 16. place below content inside struts.xml file create in previous step. \\ tag will help us to create an action. class: class attribute in action tag will take class with complete package name as value. method: declare method name which which belong to a class declared in class attribute in action tag. name: name attribute in action tag is to map an action which will get call after hitting below url as an example. e.g. → \\ result tags will maintain a mapping. this mapping will be responsible for calling an action or interceptor at the time of the request reaching the server and also for loading a specific jsp as in response. 17. update user.java class with below content to add execuet() method. 18. place below highlighted file with below inline content inside webapp folder: error.jsp ============= index.jsp ============= welcome.jsp =============== 19. right click on the project root folder practic and select run as and than run on server. below screen will appear as soon as server gets started. hit below url in chrome browser to test struts application struts2 + hibernate integration ================================ 20. update userdao.java class with below highlighted changes red in color. as part of update change method name from main(string\\21. update user.java class also with below highlighted changes red in color. 22. again, right click on the project root folder practic and select run as and than run on server. below screen will appear as soon as server gets started. after entering one and submit form will take us to welcome.jsp and show below output to print name. i hope you found out this article interesting and informative. please share it with your friends to spread the knowledge. you can follow me for upcoming blogs . thank you!\n",
      "--------------------------------------------------------------------------------\n",
      "Free Article 2:\n",
      "written by , principal product manager at voicemod > the graveyard is filled with exquisitely designed startups and products to scale to millions of users who never got the slightest bit of traction. don’t become one of them. in this article, i’m going to explain one of the most prevalent issues when creating products: overengineering. in my experience, overengineering has killed more products than the absence of good development practices. this is something that product managers, founders, investors, or really anyone involved in a digital product or service can learn from. before getting into detail, allow me to share a bit about my background. before becoming a product manager, i was an engineer. my formal training was in computer science, although i’ve always worked closer to the business side than coding itself. i’ve created, scaled, and managed both engineering and product teams and speak of overengineering from experience. in fact, i am guilty of having caused and suffered the consequences myself. for this reason, i know firsthand just how crucial it is to understand what it is, what it costs, and how to prevent it. what is overengineering? ======================== if we look at the wikipedia definition, overengineering refers to designing a product in a more complex way than what’s necessary: > overengineering (or over-engineering, or over-kill) is the act of designing a product or providing a solution to a problem in an overly complicated manner, where a simpler solution can be demonstrated to exist with the same efficiency and effectiveness as that of the original design. — in a software context, i often refer to this definition from : > code or design that solves problems you don’t have. now, you might be thinking, who would design or code to solve a problem that they don’t have? it seems ridiculous, right? hold on to your chair because, after a two-decade-long career and having done it myself, i can assure you that overengineering is not the exception; it’s the norm. overengineering causes ====================== nobody commits overengineering with bad intentions. on many occasions, it happens because we try to anticipate the future and prepare for the unknown. when we code a feature, it’s easy to think that we can make it future-proof by investing a little more time “just in case.” the reality is that nine times out of ten, that “just in case” never comes. but along the way, we lose valuable time and increase the complexity of the project, which often has long-lasting impact. the general problem — another common cause of overengineering is lack of experience. the more senior you are, the less prone you are to overengineer because you’ve already experienced quite a few situations in which artificial complexity has exploded in your face. the learning curve, with respect to the engineer’s experience, usually follows a pattern very similar to this: 1. you start by programming in a straightforward way. 2. you discover a paradigm such as object-oriented programming and jump on the bandwagon. 3. you read about design patterns and start looking for the opportunity to implement them in every situation. 4. after a few years of having suffered unnecessary complexity, you go back to writing straightforward code. code complexity vs. experience — loosely defined requirements also contribute to this problem. if an engineer does not have a well-defined problem, they are more likely to overengineer to protect themselves from uncertainty. boredom can also lead to overengineering. if an engineer does not have exciting challenges to face, they may end up complicating something simply by trying something new. overengineering consequences ============================ i wasn’t kidding when i said that overengineering kills startups. it has two particularly perverse effects on any system. on one hand, it increases our development costs. if our engineers don’t choose the simplest solution to address a problem, our expenses in time and money increase, preventing us from iterating faster. overengineering also increases our maintenance costs. simple code is much easier to program, test, and modify. the complexity can grow exponentially when we complicate it, impacting our iteration speed. for these reasons, i can’t say this enough: overengineering kills products, more often than does the absence of good engineering practices. this is because, in order to benefit from good practices, you first need to have a product-market fit. and overengineering can prevent you from achieving that in the first place. overengineering examples ======================== the first example of overengineering that comes to mind is microservices-based architectures. this emerged like a wave a few years ago, and they should have taken down more projects than those they have consolidated. i put them as an example of overengineering because they are not necessary in 99% of cases, especially for a startup that has to find a market fit and will benefit significantly from using a more straightforward architectural pattern like a . if you succeed in finding market fit and it turns out that you need to switch to microservices due to issues scaling, that’s not a bad problem to have. premature optimization is often another typical example of overengineering. a common situation is preparing a system to absorb a large amount of traffic with an overly complicated infrastructure setup when you still don’t have users. in most cases, having a monolith running on a single server is all you will need to validate your business model. one of the worst examples of premature optimization occurs when we spend a lot of time designing a system in an effort to avoid repeating ourselves in the future and having to throw away part of the work we’ve done. it doesn’t matter how perfect your design or implementation is if it never comes to fruition because you go broke first. the worst code on the planet that helps you validate a hypothesis is better than standing still for fear of not repeating yourself. related to the above, software rewrites are an obvious example of overengineering. engineers typically don’t like to work on legacy codebases. their natural tendency is to do everything from scratch. but as joel spolski wrote more than 20 years ago in , rewrites rarely serve their purpose and can even take your business away. it may sound obvious, but your client doesn’t care how elegant your system is on the inside. they care that you help them solve their problem. every minute you invest in not giving them value is time wasted. how to prevent overengineering? =============================== the best way to prevent overengineering is to help your engineers become true product engineers. you can do this by involving them in the day-to-day business, explaining the “why” after each initiative, and linking it with the metrics that matter for the organization and its vision. it’s important to bring engineers closer to our users, inviting them to interviews and discovery sessions with them. you want your team to empathize with the users’ problems, so they can quickly discard any engineering effort that won’t solve them as efficiently as possible. don’t expect your engineering team to be motivated to avoid complexity if you treat them as mere production chain resources whose sole task is to implement user stories from a backlog. they need to understand the “why” behind each decision. it’s important to firmly define the problem in order to reduce ambiguity. your engineers need to know the “why,” but they also need to know what to expect. the more you’re able to narrow down the problem, the less reason they’ll have to overengineer a solution. an excellent way to define the expectations of a system is by using service objectives with . engineers are some of the most creative profiles in any company. if your team trusts your criteria, day-to-day ideas or initiatives will likely arise on their part that may reveal a “what if” scenario they’re considering. when you think that this might be the case, ask: how does this help solve our current users’ problems? what happens if we don’t do it now? the answers will help you to determine if it is a possible case of overengineering or not. lastly, more directed toward founders, prioritize hiring senior engineers who have already had a few experiences in product companies. look for the “war scars” during interviews. ask about their most painful experiences and how they dealt with them. stick with those who put the user and simplicity ahead of simply technology solutions. mental models to prevent overengineering ======================================== yagni ===== the problem of overengineering is so prevalent in the industry that engineers themselves have a term to refer to adding code “just in case:” , an acronym for “you are not going to need it.” yagni tries to prevent you from adding anything that is not strictly necessary to solve the problem you have in front of you. because the reality is that most likely, “you won’t need it.” kiss ==== the term kiss, “keep it simple stupid,” refers to the fact that simple systems are easier to repair, evolve, and maintain. therefore, simplicity should be one of the goals of any design, avoiding any unnecessary complexity. worse is better with “worse is better,” we emphasize that there comes a point when having fewer options is preferable to having more. it’s this way because it can simplify the use of our product, making it attractive to a broader segment of the market. in other words, it encourages us to maintain the minimally essential functionalities of a product, avoiding adding fat that can add complexity to it. final thoughts ============== overengineering has the potential to destroy your startup: adding unnecessary complexity. increasing development and maintenance costs. reducing your iteration speed. thus avoiding you from achieving a market fit. unfortunately, overengineering happens all the time. for this reason, it is vital to know what this looks like and try to prevent it by involving and bringing your engineers closer to your customers’ real problems. every minute we invest in development that doesn’t solve an actual customer problem is a minute wasted. avoid falling into the “just in case” trap. remember, the graveyard is filled with exquisitely designed startups and products to scale to millions of users who never got the slightest bit of traction. don’t become one of them. originally published at on february 13th, 2022.\n",
      "--------------------------------------------------------------------------------\n",
      "Free Article 3:\n",
      "when i decided to take a sabbatical from my career, i knew that i would miss conducting research. while taking a break from the day-to-day work was much needed, i still enjoyed conducting qualitative research. so, to fill the void, i decided to do my own research project. i had many options available, but the topic that personally interested me was understanding individual contributors within the tech industry at a deeper level. you see i very much considered myself dedicated to this path, and yet i never really felt there was much guidance about this path — especially ways to perform this path at a high level. from my experience, most organizations define and coach success as part of the people management path. even my managers, as wonderful, helpful, and inspiring as they were, specifically chose not to stay on the ic path and as such, i felt often struggled to provide guidance on this path. so, with my time during my sabbatical, i went out and looked for guidance. i reached out to my colleagues who seemed dedicated to, or at least enjoying, the path of an ic. and not just any ics, but those who have been ics for many years, who i knew were well respected among their peers, and performed their roles with much success. i call these people power ics or as i will shorten for this writing: pics. my goal for this project was simply to understand these pics further and for myself to personally grow from the process. i talked to them about their journey, motivations, and how to be a successful ic at a high level in the tech industry. i ended up talking to 8 colleagues who were working as ics within corporations with an average of 15+ years of career experience. they represented a mix of user experience researchers, ux designers, product managers, and product marketing managers. i purposefully refrained from talking to my engineering colleagues, as i felt the ic path for engineers/developers is a bit more common, expected, and quite frankly too unique to be comparable to my role as a ux researcher. while i approached it like a research project, my larger goal was to simply learn from others and reconnect with old colleagues. it was never my intention to share out any learnings, yet my researcher brain couldn’t help but synthesize interesting trends from those conversations. so here is my attempt to share out these top trends and insights from the conversations. this is not a formative or exhaustive research project on ics, but rather a discussion of the simple and broad observations that i hope you find interesting. power ics don’t have it all figured out, but they know themselves well -------------------------------------------------------------------------- as i sought out people who saw themselves as ics, i went into the project hypothesizing that they had it all figured out. my thought was that they knew deeply they wanted to be an ic and had chosen their path with as much certainty as one chooses a meal off a menu. i was wrong. i suppose i shouldn’t have been surprised that these pics’ lives were as “zig zaggy” as everyone’s. how they landed as a pic was not 100% planned, although it wasn’t a complete accident either. i understood this through the way they responded to my questions. when i asked questions such as “what motivates you?” or “why be an ic?”, they responded thoughtfully and genuinely. i could tell they had no canned responses nor trained talking points they’ve learned from books or career coaches. instead, they responded deeply, reflectively, and sometimes curious towards my line of questioning. it was as if it was the first time thinking through the question — although i’m certain it wasn’t. i could tell they were still thinking their path through, even some stating that outright. while some knew they were on the path that was a fit for them, they were honest in admitting that their path could very well change. but while they acknowledged their path was not always clear, they clearly knew themselves well. i was often surprised at how well they understood what they wanted and didn’t want out of their career. what they enjoyed, didn’t enjoy, and aspects in their work/life that truly made them happy. one word that came up over and over again was “energy”, as they constantly asked themselves, “is this how i want to spend my energy?”. energy is the central resource of the pic. realizing the scarcity of this resource, they were careful in how they chose to spend their energy in their career and life in general. in fact, they talked about what they don’t want to do with their energy as much as what they do. this came about in choosing not to be a people manager and rejecting the tasks that came about with management. tasks such as: engaging in the politics of an organization, the people management process of hiring, firing, promoting, and disciplining others, and rejecting dedicating their time to discussions and meetings. the fact that these pics started with “no” and what they don’t want, i believe, was not because they were overly negative people. i believe instead that this position was needed because pursuing the management path, as they would admit, was often the expected path. thus, they needed to be able to confidently say “no” to the tasks and management path they were expected to go down and where their career naturally led them. some even dabbled in the management path only to discover later they did not want to spend their energy in this manner. in the same vein, they felt the management path would take them down a path that would try and shape them to be someone they are not. i heard variations of “i’m not a people leader”, “i’m not a salesman, “i’m not aggressive enough”, or “i don’t have an a type personality” — all personality traits they felt were the wardrobe of a manager. whether or not this perception is true is debatable, but the larger point is that knew themselves well enough to perceive that this path would ask them to go against the grain of who they are. finally underlining their question “is this how i want to spend my energy” was a more fundamental question of “is this going to make me happy?” these pics admitted that pursuing a management path will earn them more money and more prestige but questioned if it would make them happier. this led to a flood of other questions as to what they really care about. “is this work that i enjoy?”, “does this work fit with my natural skills and make me a better version of who i am?”, “does the work give me meaning and purpose?”. money and titles were certainly nice, but it was not what was motivating to them — as several honestly admitted. as one pic put it, “i don’t aspire to build a team. i don’t aspire to be a millionaire. i want to do interesting intellectual work with kind-hearted people”. again, i believe knowing themselves well was necessary for saying “no” to the expected path of management. i asked if they felt a stigma from others by staying down the ic path. most did not feel there was a stigma, however, they did feel it was expected of them to go down management from colleagues and the tech culture in general. so much that they had to wrestle with this decision themselves — if everyone else is expecting me to go down the management path, shouldn’t i as well? so they had to be very clear to themselves and what they did and did not want and be honest about what motivated them. otherwise, they might be caught up in the current taking them down the management river. but with every clear “no”, comes a “”. so what were they saying yes to? if aspects of management weren’t motivating, what was motivating them? power ics are motivated by curiosity ---------------------------------------- what stood out to me most with these pics was that at their core was curiosity. they had a hunger to learn and satisfied this hunger in many ways. for some, it was learning skills, whether they be new tools of their trade or learning tangential skills represented in other roles. for others, it was being able to explore a new topic/product area. and not just scratch the surface, but really go deep in understanding this area as they found interest in both big picture thinking as well as getting into the “nuts and bolts” of a problem. finally, there was a more abstract interest in ideas and desiring the opportunity to experiment and “play” within a space of ideas and tackle really interesting problems. surrounding this was the desire to not be stagnant and be able to explore a variety of topics. once they’ve exhausted their understanding of a problem space deeply, they desired the ability to move on to the next. and to them, this is what they believed the ic track allowed them to do: the freedom to explore and express their curiosity. while certain tasks can be redundant, pics did not see their work as an ic as boring and even seemed puzzled when i inquired how they manage to keep their lives fresh or interesting. they naturally believed that working as an ic, especially in the tech industry where things change so rapidly, put them in the best possible place to explore and find fresh problems to solve. they often questioned whether their colleagues in middle management positions had that same freedom as they observed them less able to move around industries, fields, and problem spaces effortlessly. to the pic there was always a new product space just beyond the horizon to explore and grow from. this leads us to their next motivation: producing. the power ic is not just motivated to build things. it’s their goal. ------------------------------------------------------------------------ pics wanted to get their hands dirty and create something. now each role has different ways of getting your hands dirty, but each pic desired to have an active hand in the creation of a product. they wanted to have a very active role in the creation process and a tangible outcome they could confidently point to and say, “i helped produce that”. now, this is what i expected going into these interviews. if there was anything i heard about ics it was the common description that they are “”. so hearing this was not surprising. but what i found interesting about this motivation was how it tied to their career goals. none of the pics explicitly talked about their career goals and not in the language i often hear in corporate talk about goals (e.g. 5 year plans, growing their career, building a personal brand, etc). that’s not to say they weren’t goal-oriented, but the sense i got from these pics was that personal growth was not tied to who they are or who they can become, but instead what they can explore and produce. a career was not perceived as an end in itself with the outcome being higher titles, prestige, or money. while these outcomes were appreciated and a practical necessity, as i mentioned before, many openly rejected them as their primary motivators. instead, the perception of their career was as more of a means-to-an-end. they didn’t seem to perceive their career as a “thing” they were working to build, but rather their career is working for them by providing an outlet to express their curiosity and skills. as such, goals were very much associated with the product. was the product successful? and even more importantly, was i able to leave my mark on the product and team? was i able to have true impact? impact, as i learned, was crucial for the pic. impact equals success ------------------------- if their core motivations are curiosity and getting their hands dirty, then the outcome that matters is impact. a very tangible impact, which they believed being an ic is the right place for impact. i came to these pics with this question: how can you have impact as an ic? underlining this was a premise that i have noticed colleagues go into management specifically thinking they can have more impact within an organization. the pics i spoke with rejected this premise and in fact felt the opposite was true. they believed that the “real” impact, or at least the impact they were looking for, happened within the ic path. the impact of the actual production and creation of the “thing” and not merely talking about the thing. since goals were tied to products, the real impact was measurable and visible. they acknowledged that there is a power dynamic within an organization and larger decision-making is often out of their control as an ic. but at the same time, they’ve noticed some of their management peers become detached from the product and sometimes even the customers. they feared losing this connection as they believed that a close connection to products and customers was necessary from having the impact they were looking for. the pics also looked at how to have a broad impact as possible. greater impact either within their organization or scaling their influence to other teams. i heard the word “altitude” come up a lot among the pics. as ics can get into weeds on issues, they understood that they also needed to “rise up” to see the broader landscape. this was how many saw themselves growing as an ic and what they believed helped become a power ic. yet there is a delicate balance here because they still want to stay in touch with the product and customers. they are looking for that goldilocks role where they have the high “altitude” to achieve greater influence, but are still not detached from the day-to-day meaningful product impact. as one pic put it, “i don’t want to move so far up to the top that i lose that connection (with the product and customer). but still high enough that i can have influence.” scaling their impact fundamentally came down to people. to have a great impact, pics realized they could no longer just work independently as they did in their early years as an ic. instead, they needed to learn how to bring others into their process and how to effectively bring people together with their work. whether it be with upper management or collaborating with other ics. this also means growing their field and knowledge base for others. in one sense, they needed to be mentors. power ics want to mentor, not manage ---------------------------------------- all the pics i spoke to desired to mentor others. this was somewhat surprising to me as i logically assumed the fact that they avoided people management meant a desire to move away from grooming others. while they were saying no to many aspects of management, the one aspect that did intrigue them and perhaps drew them closer to considering management was having the opportunity to mentor other junior ics. in fact, they often sought out ways to help groom junior and senior peers without having to be a manager. a pic, who held a management position, remarked, “i like the mentoring aspect of managing people. i hate the focus on writing, delivering feedback, the bureaucracy of it. drives me batty”. in general, they alluded to a difference between mentorship and management. with a manager, they believed, existed a tricky power dynamic. while a mentor, on the other hand, has more of a collaborative peer relationship which they believed may be more conducive to learning and growing. one pic remarked, “it’s totally different. as a manager, you are responsible for their career. they feel they need to listen to you, even if it is just suggestions. it’s a whole different dynamic.” and their desire for mentorship didn’t mean just 1-on-1 relationships. some expressed their mentorship desire by growing and helping others through organizational initiatives such as facilitating training or classes. while an ic role can seem isolated, they were in fact very sensitive to their relationships with others — again they perceived growing impact came down to people and facilitating important relationships. with this in mind, how did they see their relationships with their own managers? pics desire managers who provide autonomy, support, and compassion. ----------------------------------------------------------------------- i was intrigued to talk to these pics about managers because as seasoned ics with lots of experience i was curious as to whether they even cared about who managed them. for junior ics, managers can be extremely important as they can help support and develop skills. but these pics had plenty of skills. so was it even important to them who was their manager? get mark wehner’s stories in your inbox --------------------------------------- join medium for free to get updates from this writer. subscribesubscribeas it turned out, these pics very much cared for who was managing them. it wasn’t so much who their manager was, for example, they didn’t necessarily have to have a similar background (i.e. if i am a researcher, they don’t need to come from a research background). it was much more about what they do and were able to provide. there were three common aspects of managers between these pics autonomy: let me do the things i can: as a power ic striving to have impact, it was important that they have autonomy. as one pic put it, “a manager who gives me space and appreciates what i can bring to the table”. this went beyond not wanting to be micromanaged. there was a much deeper level of having the ability to earn the trust of their manager such that they don’t just take a high-level goal and run with it, but that they are also able to help define what that high-level goal is. it’s not just doing the work, but being part of defining the work. one pic described it as: “being part of deciding a project. it’s not just, ‘we need personas’, and i go out and do personas. i should be a part of defining what we need to be doing.” this desire to go beyond just executing a goal and defining the goal, i believe, is a big difference between how power ics operate compared to my observations of lower-level ics support: help me with the things i can’t do: if autonomy was allowing the pic to shine with the things they can do and excel at, support was clearing the path of the obstacles they can’t do or have difficulty doing. for example, corporate politics was an area that many pics did not want to engage in, nor did they feel was a strength. pics appreciated managers who helped navigate them through potentially hairy political situations. as one pic remarked,“i know that my manager will always have my back. even if there is disagreement, there is still a shared respect”. it was always a bonus if their manager was someone who complimented their own skills, and someone who excelled in areas they knew was not one of their strengths or expertise. a pic stated this plainly as, “the good managers that i’ve had they’ve had experience in areas i didn’t or were smarter in areas i wasn’t or struggled with or quite frankly wasn’t passionate about”. compassion: care about me personally: compassion came down to the simple fact of whether their manager cared about them as a person. now perhaps this is important to everyone, not just ics, or even in the tech industry. but i think this aspect was especially salient for pics because their role and goals were so tied to the building of products. perhaps it was important that they be perceived and treated as more than just a resource — more than just a cog in the production wheel. underlining this aspect was desiring a manager who has honest and transparent, such that they could see that compassion. one pic described this as,“having a real authentic person who brings their ‘real self’ to work and cares about me as a human first.” when all was said and done, the way pics talked about the managers they enjoyed the most in their career were people that they viewed as peers more than managers per se. as one stated, “it’s less about management and more about collaboration and intellectual trust. i can trust you to be a resource for me and you can trust me and what i bring.” conclusion: how might you support or evolve into a pic? ----------------------------------------------------------- as a researcher, i’m always looking for the “so what” from my research. if this were a normal research project i would work with my stakeholders to discuss and on the implications from these insights. however, this is not a normal research project nor do i have any concrete stakeholders to collaborate with. so instead, i’ll leave you with some questions. questions to think about, whether you are a manager or an ic to think through your own takeaways from these interviews. if you are an ic, how to think about evolving into a power ic how do or don’t you relate to these pics i spoke with? which of these motivations speaks most to you and is your current role helping to satisfy these motivations? what is important to you in your manager? are you communicating this to them? how do you want to spend your energy? do you know yourself well enough to state this clearly? how might you try and scale your impact? are you seeing at a high altitude to have increased impact? are you building relationships or merely working independently? are you helping to define goals and not just execute goals? are you looking for mentorship opportunities — whether it be receiving or providing mentorship? are you just curious or are you looking for ways to execute on that curiosity in a meaningful way? are you exposing yourself to as many problems surrounding your organization and not just the ones you’ve been assigned? are you a catalyst for solving those problems? if you are a manager, how to think about supporting a pic effectively how might you provide autonomy, support, and compassion, the three aspects they desire out of a manager? if for some reason you aren’t able to provide this, is there someone else in your organization who can? how might you offer opportunities that allow them to mentor other ics? how might you tap into their curiosity and provide a productive space for them to explore and satisfy their curiosity? how might you help them scale their impact and allow them to gain altitude to not just execute goals, but be a part of defining those goals? how might you be mindful of working with your pic at personal goal setting, recognizing that at a deeper level what motivates them may not fit within the corporate laddering culture? remember that as a manager, and someone who chose to not stay on the ic path, what motivated you might not be the same as your pic. ask and learn — you may be surprised! parting thought: engage and listen ---------------------------------- finally, i would recommend to everyone to spend time engaging and talking to other colleagues about their career journey. and by this, i don’t mean seeking others to advise you on your own career — although that is certainly important as well. instead, listen to others tell their story about their career path, their motivations, and how they perceive their role — especially those who may be on the same path as you. as interesting as these insights were to think through, what i gained most from this process was simply listening to others tell their story. how their story related or didn’t relate to my own, helped me think about my own path and gave me a better understanding of myself. some of my interviewees commented that our conversation felt like a therapy session, which made me smile because i definitely found the conversations therapeutic as well. so as you hike down your career path, take a moment every now and then to greet those along your trail and ask/listen to them tell the story of their journey — it will help increase the meaning and purpose you find in your own trek. i’m still interested in learning more from pics. feel free to connect and contact me as i’d love to listen to your story or share/learn from each other.\n",
      "--------------------------------------------------------------------------------\n",
      "Free Article 4:\n",
      "in this blog we will read about below configurations in fusion application (fa) to overcome casdk-0004 error. 1. whitelisting of oic ip address in fa. 2. how to enable location based access control (lbac) in fa. 3. how can i ensure that i always have access to the security console? 4. how can i disable location-based access when i am not signed in to the application? 5. how can i disable location-based access when i am locked out of the application? 6. if “location based access” tab not visible in security console in fa, how to enable it ? background ============== while testing connection for oracle erp adapter from oracle integration cloud (oic), following exception is returned: casdk-0004: unauthorized \\ authentication failure. please check if the credentials provided are valid. step ==== the issue can be reproduced at will with the following steps: 1. log into oic 2. create a new connection using oracle erp cloud adapter 3. click on test connection after entering all connection details. cause ===== the issue was caused by the following setup: location based access control (lbac) enabled in fusion applications. lbac was enabled in fusion applications which is restricting access to only the ip’s listed in lbac setup. oic server ip was not among the trusted ip’s in lbac. hence access from oic was restricted. solution ======== to resolve the issue, perform the following actions: 1. log into fusion applications as user with it security manager role 2. navigate to tools →security console — \\>administration — \\>location based access 3. if location based access control is enabled, ensure that ip for oic server or it’s network firewall ip is included in white list ip’s. getting oic server ip address : login to oic \\> go to user icon\\> about 4. save location based access tab not visible in security console? ============================================================== follow the below steps to enable “location based access” tab for your user. goal: --------- user cannot see location based access tab in security console — administration tab after upgrade to r13 19a or higher. solution: --------- follow the steps given below: 1. go to setup and maintenance under profile drop menu in the top right corner of the fusion application. 2. go to top right corner and click on the icon and select from the new menu “search” 3. search for “manage administrator profile values” 4. in application filter choose “applications security”. 5. modify the value for profile option aseadministerlocationbasedaccesscontrol 6. in profile value change from “no” to “yes” and then click on save\\&close test erp connection in oic it will should be 100% completed. how can i ensure that i always have access to the security console? ======================================================================= if location-based access is enabled, you must add your computer’s ip address to the allowlist. also ensure that the it security manager role is granted public access. even if you have to sign in from an unregistered computer, you can still access the security console and other tasks associated with the it security manager role. how can i disable location-based access when i am not signed in to the application? ======================================================================================== you want to disable location-based access but you’re locked out of the application and can’t sign in to the security console. you must request access to the administration activity page using the url provided to the administrators. make sure you have the following privileges: aseadministerssopriv aseadminstersecuritypriv after you request access to the administration activity page, you get an email at your registered email id containing a url with the following format: https://\\/hcmui/faces/adminactivity click the url and you’re directed to a secure administrator activity page. select the disable location based access option and click submit. you receive a confirmation that location-based access is disabled. immediately, you’re redirected to the oracle applications cloud page where you can sign in using your registered user name and password, and gain access to tasks and data as earlier. how can i disable location-based access when i am locked out of the application? ===================================================================================== if you’re locked out of the application for some reason, use the following administration activity url to disable location-based access. only an administration user with the it security manager job role can perform this unlock operation. https://\\/hcmui/faces/adminactivity ensure that the following email notification templates are enabled: ora administration activity requested template. ora location based access disabled confirmation template. thanks.\n",
      "--------------------------------------------------------------------------------\n",
      "Free Article 5:\n",
      "photo by on 在背景執行緒執行一段任務，未完成時就離開 activity 是有可能造成 memory leak 的。如下程式碼是一段使用背景執行緒請求網路資料： 1. 呼叫 networkcall() 模擬 10 秒後取得資料。 2. 使用 runonuithread 將取得的更新到 ui 上。 //未執行完離開app會造成memory leak object : thread() { override fun run() { //背景執行緒取得資料 val data = networkcall() //更新到 ui 上 runonuithread { binding.result.text = data } } }.start()//模擬網路請求資料 private fun networkcall(): string { sleep(10000) return \"data1\" } 上面這段程式碼，如果我們在背景執行緒未執行完成時就離開 activity，就會因為背景執行緒仍在執行，activity 無法被釋放，造成 memory leak。 使用 coroutine 處理非同步 ------------------ 這個問題將使用 coroutine 的非同步處理來解決。coroutine 是在 kotlin 用來方便處理非同步需求的一個框架。有著易開發、好管理的好處，而且符合結構化並發 (structured concurrency) 架構。讓你寫非同步就跟同步一樣的簡單。另外 coroutine 也是 android 進行非同步程式設計時，官方的推薦解決方案。 首先修改網路請求資料的 networkcall function。 1. 將 function 加上 suspend ，使變成 suspending function。 2. 使用 withcontext(dispatchers.io) 切換至背景執行緒 3. 回傳資料 //模擬網路請求資料 private suspend fun networkcall(): string { val data = withcontext(dispatchers.io){ delay(10000) \"data1\" } return data } 接著在 activity 就可以透過 lifecyclescope.launch 來呼叫 networkcall 取得資料，當執行 networkcall 時會切換至背景執行緒，取得資料後就會再回到 ui 執行緒，這種寫法非常方便，讓寫非同步就跟寫同步一樣。 get evan chen’s stories in your inbox ------------------------------------- join medium for free to get updates from this writer. subscribesubscribe使用 lifecyclescope.launch 建立一個 coroutine 來執行 networkcall。 lifecyclescope.launch { //取得資料 val data = networkcall() //回到ui執行緒 binding.result.text = data } 在lifecyclescope 裡的這段程式碼的生命週期將與 activity 的生命週期一致，所以當 activity 被銷毀，coroutine 執行中的任務也將被取消，也就不會發生當離開 activity 時，背景執行緒仍在執行。 coroutine 的階層管理 --------------- coroutine 在處理多個執行緒時，尤其是有階層關系時，比起使用 thread，更來得容易管理，也能減少發生 memory leak 的機會， 如下例，我們新增了兩個 job 分別處理網路請求 networkcall 與其子任務 networkcall2 。當我們使用 job.join 等待所有的 coroutine 工作完成時。像這樣的階層關系，使用 coroutine 就非常方便，會幫你處理好等到所有的子 coroutine 都完成才算是完成。 job = lifecyclescope.launch { try { val data = networkcall() val job2 = launch { val data2 = networkcall2() } } catch (e: cancellationexception) { println(\"cancel done\") } }lifecyclescope.launch { job?.join() println(\"all done\") } 再看另一個範例是 coroutine 的取消。我們對 job 呼叫了 job.cancelandjoin 來取消這個 coroutine 的執行，子 coroutine 也會被取消。這是 coroutine 的一個很棒的地方，不用擔心會有子任務在父任務取消後仍在背景執行。 job = lifecyclescope.launch { try { val data = networkcall() //child val job2 = launch { val data2 = networkcall2() } } catch (e: cancellationexception) { println(\"cancel done\") } }lifecyclescope.launch { job?.cancelandjoin() println(\"all done\") } 如果父 coroutine 取消或失敗了，我們不會希望要還有在背地裡執行的執行緒，因為這容易產生 memory leak。以上舉的這幾個 coroutine 範例，其實都是在確保當一個任務不再需要被執行，其子任務也都將被取消。 最後，coroutine 已經是現在開發 android 一定會使用的，好處是非同步的處理更方便、不易出錯、減少 memory leak 機會。這邊只是初步的介紹 coroutine 如何減少 memory leak，coroutine 的詳細介紹請見 。 參考： 下一編：\n",
      "--------------------------------------------------------------------------------\n",
      "Free Article 6:\n",
      "decentralized options vaults (dovs) are structured products offered on-chain, closely related to tradfi counterparts that combine several financial instruments to create a whole new payoff curve. the core function of dovs is to use investors’ capital and employ options trading strategies in a completely automated and decentralized manner. these strategies may include covered calls, protective puts, or straddles. before dovs, option strategies were only available to accredited investors through over-the-counter (otc) trading or by self-execution on option exchanges like deribit or ftx. dovs allow their investors to earn true return on investment (roi), not just passive income from liquidity provision. the capital is being actively invested in trading strategies, and is independent of the amount of trading being conducted i.e. there is no token “yield” in the traditional defi sense. from scratch, dovs have grown exponentially to become the dominant part of the \\~$1 billion defi option tvl, with notionals trading in billions of dollars every month. picture source: over the past several months, defi ecosystems across different blockchains have witnessed a massive surge in new projects focused on structured product vaults with the objective of offering users sustainable yield. given the infancy of these popular products, we aim to define some very basic suggestions on what a builder must keep in mind while launching dovs. we aim to critically analyze core functionality behind dovs, from the trading strategies used to the technology behind the vaults, and how we believe these vaults can be improved. how do dovs work? ================= as mentioned previously, dovs allow investors to deposit capital and then automatically trade popular options strategies with it. the most common options strategies employed by dovs include covered calls and protective puts. let’s look at these strategies in greater detail. through a covered call strategy, when vault sells a call option, it will also own the underlying security for that option. similarly, through a protective put, when a vault sells a put option it will also sell the underlying security for that option. let’s explore this in greater detail below. selling covered call options vs selling put options: ---------------------------------------------------- let’s first look at the two of the most popular option writing strategies: selling covered calls and selling puts. a covered call is a financial strategy wherein the investor selling a call option owns an equivalent amount of the underlying security. this strategy can be created by holding a long position (spot) on an asset and then selling call options on that same asset to generate an income stream. this strategy entails very low risk as the investor’s ownership of the underlying can serve as a cover if the call option expires in the money and the buyer of the call option chooses to exercise. dov users have a choice to choose either covered call or covered put strategy. if the user feels bearish, they can opt-in for the covered call position and earn yield as the market goes down. vice versa for users who feel bullish, they can opt for the covered put option to earn yield. let’s take a deeper dive into the two: selling a covered call: ----------------------- - by selling a call option, you have the obligation to deliver the asset (e.g. eth) at a predetermined price (strike price) to the option buyer if they exercise the option. - if the seller of the call option also owns the underlying security (fully collateralized), the option is considered “covered” because they can deliver the instrument without purchasing it on the open market at possibly unfavorable pricing. writing call options should be the same size as the underlying long position. for example, if you own 1 eth, you can sell 1 covered call option. owning the underlying asset is necessary because selling naked call options without owning the underlying asset exposes the options writer to unlimited risk. - a covered call will limit the trader’s potential upside profit, capping gains above the strike price. for example, if you sell a call option with a strike price of $2,000, you are giving away profit of the underlying asset above $2,000. - the goal of selling a covered call option is for the option to expire worthless, allowing the trader to earn the option’s premium while keeping 100% of the underlying asset as collateral - a covered call serves as a short-term hedge on a long stock position and allows investors to earn income via the premium received for writing the option. covered call selling typically does well during choppy or bearish environments as the spot price is unlikely to breach the call strike. covered call option strategy. source: - covered calls should be fully collateralized so that the risk does not come from selling the option as it is covered by the crypto the writer owns. · if strike price \\ writer still makes option premium + gains on holding the crypto. · if strike price \\ then if crypto assets rocket in price, loss can potentially be unlimited. selling a put option: --------------------- by selling a put option, you have the obligation to buy the asset (e.g. eth) at a predetermined price (strike price) from the option buyer if they exercise the option selling a put is an options strategy used to generate yield (income) when traders believe the underlying asset price is unlikely to fall below the strike price before the expiration date note that the writer of a put option will lose money on the trade if the price of the underlying asset drops below the strike price at expiration the goal of selling a put option is for the option to expire worthless, allowing the trader to earn the option’s premium while keeping 100% of their collateral source: what should buidlers keep in mind? ================================== adoption of erc-4626: ---------------------- the main problem developers face while building tokenized vaults is the integration of different yield bearing tokens. for example, in a situation where you want to build a defi app which requires integrating the tokens of each protocol, you will need to research each one, know their model of accruing yields, and adjust it into your code base. if you want to integrate vdai of maker dao; steth on curve; and so on, you will need to understand the peculiarities of their smart contracts and build custom solutions to successfully integrate each of them into your defi app. apart from how stressful and time-consuming this process of integrating different yield-bearing tokens can be, it also increases smart contract risk because of potential errors. developers will need to spend more time checking for potential loopholes in the adapters, and in some cases, they might even need to outsource it to smart contract auditors, which can be costly. this is more important now that attackers are breaching the integrity of a lot of protocols and defi apps. hence, the creation of the erc-4626 standard. what exactly is erc-4626? it is a standard api for tokenized yield bearing vaults that represent shares of a single underlying erc-20 token. the vault standard introduces the concept of shares as a way of getting fractional ownership out of the entire pool. these shares refer to yield-bearing tokens. when users put any funds into the vault, the deposit function triggers the smart contract to mint a corresponding amount of shares to the depositors. the withdrawal function helps owners burn shares in exchange for assets. the erc-4626 is a compatible extension of the erc-20 standard. as a result, most of the usual variables, events, and functions that are applicable in erc-20 token contracts still work with the erc-4626 vault standard. while there are a couple of prominent token standards at the moment, the defi world still has a recurring problem regarding tokenized vaults. defi aggregators have always found it quite stressful to aggregate several yield-bearing tokens because there was no standard. but now erc-4626 makes it possible to have details of yield-bearing tokens with one api call. the standard makes the integration of protocol easier and less prone to errors. since there is a common standard that you can integrate, there is no actual need to build separate adapters any longer. it quickens development and enhances composability. similarly, it reduces cost because builders no longer need to get auditors to help with their adapters and interfaces. most importantly, erc-4626 enhances security among dapps and yield aggregators that are dealing with yield-bearing tokens. who uses this standard? protocols such as balancer linear pools, yearn finance, yield protocol, alchemix v2, convex finance yield pools, and more are already using this protocol actively. the problem of going the extra mile to enhance the security of defi applications with yield-bearing tokens is solved — to a large extent — with the introduction of this baseline standard. use of composability and interoperability among various defi protocols will increase in the next couple of years. it is even possible that this standard will be a launchpad for building and shipping completely new products in the defi ecosystem. can be in line with existing successful dov’s strategies: --------------------------------------------------------- the appeal of the dovs emerges from its seamless accessibility and simplicity for the user. investors simply ‘stake’ their assets into the vaults which are automatically deployed into option strategies. the strategies deployed thus far have been selling covered call and put strategies (which we discussed above) providing the highest base yield available in defi (averagely 15–50%). on top of that, token rewards are distributed, providing an even higher yield for users. in some instances, the collateral in the vault earns staking/governance yields as well, creating three sources of yield in a single vault. this triple layer of option premiums, token rewards, and staking yield creates a significantly high and — more importantly — sustainable yield that is unprecedented in defi. another major problem defi must solve is non-linear liquidations. even centralized exchanges like deribit manage non-linear liquidations with some difficulty. for the liquidation of large option portfolios, the delta (or spot risk) is managed first by executing a perp/futures position against the portfolio. the other greeks in the portfolio (non-linear risk) are then systematically liquidated over time with an active involvement by the intermediary. by using a hybrid defi model where investment, collateral management, price discovery and settlement occur on-chain while non-linear risk management is performed off-chain, dovs provide an elegant solution to this problem. all option contracts traded through dovs are fully collateralized which eliminates the need for liquidations completely. refine options trading strategies to make them more dynamic: ------------------------------------------------------------ all dovs typically only offer covered calls or protective puts and they will trade on these options strategies on a weekly basis regardless of market conditions. while these strategies may minimize risk, dovs are still dependent on market conditions. put simply, covered calls are riskier during bullish markets while protective puts are riskier during bearish markets. covered call strategy faced a significant loss (especially in early 2021) when eth jumped from $740 to $1200 in less than a week. black swan events like this tend to be infrequent but significantly erode the strategy’s profitability. when the contracts being traded through these strategies expire itm, it is the investors’ capital at risk, resulting in extremely low apys and even principle risk. however, these risks can be counteracted by simply accounting for market conditions before trading for strategies. dovs should analyze the market through discretion, considering the option type, strike, maturity, and market factors, to trade rather than just employing automated strategies. for example, to predict extreme events, vaults can analyze the eth price/option strike ratio. if the ratio is greater than 1, it indicates the sold call option is itm whereas a ratio less than 1 indicates the option is otm. this quantitative analysis can be done through a combination of oracles, on-chain data analytics, and well-built code. if dovs begin analyzing options and the market before trading through covered calls or protective puts, it may result in higher profitability for everyone! illusion of ‘good’ yield is ‘bad’ --------------------------------- some of the protocols selling certain strategies in dovs have been found to advertise fake high yield to make their vaults attractive. who doesn’t like double digit apr in a risky environment? there are a number of ways to fake/tweak the numbers from excluding the loss from in-the-money weeks to only calculating the yield as an average of past few weeks (which necessarily smoothes any downfall). hence there’s a need to do away with the tradition of faking the actual numbers. true yield along with all the associated risks must be flashed on the ui to make the whole process transparent for investors. doing otherwise is simply predatory. integrate cross-chain liquidity: --------------------------------- while “omnichain” or “crosschain” have become buzzwords for the nft-enthusiasts, they are especially important in the context of defi. with capital in defi split across so many chains resulting in liquidity fragmentation, it’s necessary for any dov to be functional across chains to their best extent. there are two ways a dov can be cross-chain: deploy on other chains or make use of cross-chain liquidity aggregators and layer zero. while deployment is typically preferred by most defi protocols, it could be expensive owing to the number of virtual machines and various deployment costs. instead, making use of protocols like layer zero or 1inch would allow for dovs to maximize their capital and consequently their profitability. allow for lp tokens deposit --------------------------- most dov deposits are typically single-asset: eth, matic gohm, etc., and there’s nothing much to it. if dovs would allow for users to deposit their lp tokens (polysynth, for example), this would allow for users to yield farm while unlocking liquidity for them to earn from option premiums as well. a protocol named approaches the deposit of lp tokens quite similarly through their “deep liquidity” functionality — lp tokens are used to dynamically unlock liquidity for perp trading as asset prices (and perp prices) increase and decrease. conclusion ========== the appeal of defi option vaults is tremendous. the current strategies that the vaults offer are just vanilla puts and calls, but these will gradually become complex in the future. the product’s structure will make it very difficult to use the vaults as a hedging tool. however, the protocols will be very approaching for bootstrapping liquidity for the options market, eventually making decentralized options more viable. dovs are an essential step toward democratizing finance for the masses. they also play a leading role in increasing demand for organic yield. therefore, they address the issues of the unsustainable and circular yields that defi currently faces. many of us have imagined the allure of ai and tech making their way into the financial industry — most people even believe the work of analysts and researchers to become obsolete as technology continues to develop. when you combine the benefits of automation in finance with decentralization — you get a dov. > authors: > > this research article is authored by researchers: eshana, akhil, and 0xlol in association with paradigm trading. eshana is a quantitative finance research intern at polygon, and is a finance and computer science major at nyu. akhil () is a defi research intern at polygon, and studies finance, math and computer science at nyu stern. 0xlol (, ) heads defi research communications at polygon, with their research interests in defi derivatives, web3 infra, and layer 2 scaling solutions. > > cover picture art: (illustration) and (conceptualization) > > our research partner: > > is a zero-fee, liquidity network for crypto derivatives traders across cefi and defi. the platform provides traders with unified access to multi-asset, multi-protocol liquidity on demand without compromising on price, size, cost and immediacy. the firm’s mission is to create a platform where traders can trade anything, with anyone and settle it anywhere. > > has the largest network of institutional counterparties in crypto with over 1,000 institutional clients trading over $10b per month, including hedge funds, otc desks, lenders, structured product issuers, market makers, and prominent family offices. > > additional readings on dovs: ============================\n",
      "--------------------------------------------------------------------------------\n",
      "Free Article 7:\n",
      "you knew it was coming… ======================= we started our journey in nfts last may, from bnb chain games to wax games, eth, sol, ada, and terra nfts. watching every single p2e and 99% of nfts tokens go to zero had us thinking there are better ways than this. we are at a point where marketing \\= fake engagement and the illusion of a good flip. with us, mainly as degens (me included), it’s easy to offer wl in exchange for likes and retweets. we all enter with 5–10 alts because that’s the game for now. the project mints out (mainly based on fake engagement and nothing proven). a lucky few get a good flip with a rare while the rest run to list under floor. then we all do it again with the next project. this is not what we are doing. we have done zero wl for likes or retweets. ----------------------------------------------- zero twitter collabs -we have a few select collabs only with doxed teams that have been building. no og passes - those will be earned after mint and not be given out for twitter interactions. all wl are given through discord-with no grinding, just simple interaction. my job is to make it fun, not to use wls for fake engagement. we know how much easier it would be to get hype if we did the same as everyone else. we believe that our art is good enough to have a typical art project, merch, and future drops like all the others. but that’s not what we are doing. we want to build a community-driven, balanced ecosystem and are here to stay. the most successful ecosystems we could find (none in crypto) were app games, mainly igg styles like castle clash, clash royal, etc. ------------------------------------------------------------------------------------------------------------------------------------ igg has created multiple ecosystems that get users to spend billions on assets they don’t own on an annual basis. these assets aren’t nfts and can be taken from you anytime. (ask vitalik) there is no ownership. it all belongs to the company. while you don’t own these assets, you can count on the team to have capable r\\&d, devs, marketing, and brand-building skills. you can’t say the same about most nft projects. --------------------------------------------------- while you own the assets, most teams use fake engagement tactics of wls for likes and retweets (also only 30$ to buy 600 likes) to mint out. bank the funds, and don’t take it seriously enough to work full time. how is the ecosystem supposed to survive if the founders are bleeding it dry and/or incompetent? i am part of discord groups that host project founders before they mint. it’s not uncommon to hear, “we are cnfts veterans and know this space well,” and 3 seconds later, say, “we are dropping 5k-7k for 100–150 ada”. at minimum that’s 500k ada. how is this what our ecosystem needs? to us, owners have seen other projects bank so much and they want in. we can’t think of any other explanation. i can’t even blame them. we, as cnft users, make it too easy. they created a flippers market because you know it’s not worth holding. unfortunately, the odds are it’s going under the floor. often a 1/1 is minted by someone who is not looking to be a part of the project and wants quick gains. we get this how other projects incentivize people to mint, which is a part of the gambling mentality. this is not what we are about- a balanced ecosystem is. this isn’t your project if you are looking for a quick flip. it is okay. there are more than enough projects offering those opportunities. we are building something different. we are not for a second saying don’t drop art. nfts is the perfect tool for artists, just drop the team that’s banking 80% of the funds. use nfts to build community users can feel part of. web 3 done right. we are offering a free (you need to cover minting fees) launchpad for artists. not teams, hand-selected artists that are looking to build. we will not ask for money or wls, just to offer some guidance if ever needed and minting services for free through tangz (no affiliation, we hold enough tangz). with a team of only 2 members, our resources are limited so we are going to be very exclusive with the artists we choose. with our graveyard ready to turn rugs into new nfts we are trying to do more than talk. we did say chaos is coming… down below are a few core ideas to explain precisely what we are doing differently ---------------------------------------------------------------------------------- first: auctions of 1/1- where the funds go back to the ecosystem and not to random minter. our ecosystem is more significant than one person. second: certain exclusive blueprints will only be craftable with ada. an example would be: you would need resources and ada to execute the blueprints to craft a breeding house that would allow you to breed bunnies. another example could be an alien skin for your spaceship. third: royalties will start at 5% and go to 0. we are going to implement different aspects of pay-to-play. however, they won’t be necessary to complete the road map. again, they won’t be necessary to complete the road map. our job would be to incentivize you enough to add small amounts of ada into our ecosystem. the goal is to create a ratio that balances blueprints crafted using only resources and/or ada. 100% of funds from p2e go either into building other revues streams (the next medium article covers it) or back to the holders as prizes. we aren’t taking a single cent from those funds. we have yet to take salaries or big payouts. we are building something that we plan on staying here for a while. of course, if we can deliver after months of work and have a vibrant ecosystem, we plan to pay ourselves. we truly believe we have so much to prove before we are anywhere close to that. for us to make any profit we will need to deliver on the road map.1 first. if we were looking for profit, we could do the same 40 ada mint with no ecosystem, just the usual merch, and bank most of the funds. with moran capable of doing the art for free, we could do typical mutation, companion drop like everyone else, and bank more money. that is not what we are about, even though watching other projects mint out for 200k does make it hard. closing note: ============= it may seem like we are calling projects out. it is just how we see it. a big part of this project is transparency not to call out but to educate people to make better choices and rethink before spending their hard-earned money on false engagement and the usual slow rug. we have been part of amazing communities on wax that grow out of other discords. groups of people with cool and fun ideas working together to bring it to life. with weekly streams and open conversions on discord, you truly felt a part. we are hoping and working to create a similar environment. thanks for reading. this form was executed to visualize our vision and have complete clarity with our community. in the next medium article, we will explain why we chose a skeleton approach and funds distribution. moran \\& brandon\n",
      "--------------------------------------------------------------------------------\n",
      "Free Article 8:\n",
      "s3 storage classes can be configured at the object level and a single bucket can contain objects stored across s3 standard, s3 intelligent-tiering, s3 standard-ia, and s3 one zone-ia. we can use s3 lifecycle policies to automatically transition objects between storage classes without any application changes. to apply lifecycle rule we need to create a bucket. step 1: creating a bucket figure : bucket creation step 2: when we enabled s3 lifecycle then only object will move from one storage class to another. if s3 lifecycle transition not required or disabled we can disable it .in that case object will present only in initial storage class and it won't move further. figure: enabling s3 lifecycle step 3: sometimes logs are required for an existing bucket means if we set variable logging \\= true. this will enable the logs for the bucket. this will create the following resource. figure : enabling logs for bucket note : if logs are not enabled logging \\= false for the bucket the above resource will not created. in this case we can't find logs for the bucket. the source code is available in below repository the following resources will be created. figure s3-life cycle and s3 module resources.\n",
      "--------------------------------------------------------------------------------\n",
      "Free Article 9:\n",
      "in one of our , we have already discussed basic concepts hidden behind the decision trees, including the definitions of the decision trees, other core concepts and terminology we use with the algorithm. as we have already discussed all the theoretical parts of the decision tree, we now need to understand how we can use this model practically. this article will be an extension of the above-given article, where we will discuss the implementation of a decision tree using the python and r programming languages. this article will cover the following topics: table of contents ================= implementation of decision tree using the python programming language ===================================================================== 1. data splitting 2. importing and fitting the decision tree model 3. model evaluation implementation of decision tree using the r programming language ================================================================ 1. data splitting 2. importing and fitting the decision tree model 3. model evaluation implementation of a decision tree using the python programming language ======================================================================= to complete this motive of ours, we will take the help of the sklearn python library that will not only help us in fitting the model on data but also help in importing the iris data. with the iris data, we get the four continuous variables that include sepal length, sepal width, petal length, and petal width of the iris flowers and based on these variables or features of the data. iris flowers are separated into three categories: iris setosa, iris versicolour, and iris virginica. let’s import the data sets. from sklearn import datasets data \\= datasets.loadiris() x \\= data.data y \\= data.target print(‘independent variables name \\\\n’, data.featurenames) print(‘shape of independent variables \\\\n’, x.shape) print(‘class names in target variables \\\\n’,data.targetnames) print(‘shape of target variables \\\\n’, y.shape) output: in the data, we get 150 data points and four variables as discussed above. now to model this data using a decision tree, we will use the following steps: data splitting importing and fitting the decision tree model model evaluation let’s start with data splitting. data splitting ============== this step makes two sets of data ( train and test). using the train set, we will train a decision tree mode and using the test set, we will evaluate the trained model. let’s split the data. from sklearn.modelselection import traintestsplit xtrain, xtest, ytrain, ytest \\= traintestsplit(x, y, randomstate \\= 0) let’s check the shape of the spilted sets print(“shape of train data”, xtrain.shape, ytrain.shape) print(“shape of test data”, xtest.shape, ytest.shape) output: importing and fitting the decision tree model ============================================= this step will let us know how to fit the decision tree model on data. the point to be noticed here is that the model from sklearn takes a numpy array form of data to train the model. also, calling the data from the sklearn library comes as a numpy array, so here we are not required to worry about any transformation. we can directly fit the split data. let’s import and train the model. from sklearn import tree clf \\= tree.decisiontreeclassifier(randomstate\\=0).fit(xtrain, ytrain) the above code has called and trained the model using the train data. we can plot this tree to see how the splits worked with the iris data. import matplotlib.pyplot as plt plt.figure(figsize\\=(12, 10)) tree.plottree(clf, featurenames\\= data.featurenames) plt.show() output: here we can see that the in the root node of the decision tree, if the value of petal width is below or equal to 0.8, then iris has a class, and there are 37 samples of such data in whole train data. if the petal width is larger than 0.8 cm, then the iris flower is of a different class. let’s make predictions using the test data. prediction \\= clf.predict(xtest) here in the prediction variable, we have values predicted by the model for the test data. now, we can evaluate our model using the prediction set against the true values. model evaluation ================ this section will use the accuracy score, f1score and confusion matrix to evaluate the model. but, first, their definition is explained below. accuracy score: this gives the results based on the calculation of how many right predictions are made by the model compared to real data. f1score: this gives the harmonic mean of the precision and recall. where precision can be interpreted as the right predicted positive values that belong to the positive class, and recall can be interpreted as the number of positive predicted values made out of all positive examples in the dataset. mathematically, f1 \\= 2 (precision recall) / (precision + recall) confusion matrix: this matrix represents how the model predicted the values in the below-given form of the matrix. let’s calculate the above-defined scores and matrix. from sklearn.metrics import accuracyscore, f1score, confusionmatrix print(‘confusion matrix \\\\n’, confusionmatrix(ytest, prediction)) print(‘accuracy score of our model \\\\n’, accuracyscore(ytest, prediction)) print(‘f1 score of our model \\\\n’,f1score(ytest, prediction, average \\= ‘micro’)) output : here we can see that there is only one value that the model has predicted wrong, and it has achieved a 97 % accuracy with a similar f1score. here this implementation is completed, and in the next section, we will perform the same operations using the r programming language. implementation of a decision tree using the r programming language ================================================================== to work with the same data in the r programming language, we can use the datasets library. using the below codes, we can get the iris data. library(datasets) data(iris) head(iris) output: here we can see what how exactly our data looks like. now we will follow the same steps as we followed using the python programming language. data splitting ============== to complete this step, we will use the catools library. library(catools) sampledata \\= sample.split(iris, splitratio \\= 0.8) traindata \\<- subset(iris, sampledata \\=\\= true) testdata \\<- subset(iris, sampledata \\=\\= false) here we have split the data into an 80/20 ratio, where 80% of the data is from training, and 20% is for testing the model. importing and fitting the decision tree model ============================================= to complete this step, we will use the rpart library that allows us to fit the decision tree to any data. using the below codes, we can call and train the model. library(rpart) clf \\<- rpart(formula \\= species \\~., data \\= traindata, method \\= “class”, control \\= rpart.control(cp \\= 0), parms \\= list(split \\= “information”)) let’s check the model by plotting it. library(rpart.plot) prp(clf, extra \\= 1, faclen\\=0, nn \\= t, box.col\\=c(“green”, “red”)) output: one thing which we can also do here is to use the caret library so that we can check the importance of the feature/variable of our data in data modelling. library(caret) importances \\<- varimp(clf) importances output: here we can see that the petal width is the most important variable in the training of the decision tree model. let’s make predictions from the model. prediction \\<- predict(clf, newdata \\= testdata, type \\= “class”) prediction output: this is how our model has predicted on the test data. model evaluation ================ using only one line of codes we can evaluate our model against various matrices. confusionmatrix(testdata$species, prediction) output: here we have got most of the statistics which can be utilised to evaluate the model and we can also see that model has predicted only 1 wrong values and the accuracy of the model is around 97%. final words =========== the decision tree can be interpreted as an excellent introductory model to the tree-based model family. we can also find its uses as a common baseline model for various models like random forest and gradient boosting. this article has looked at how we can implement a decision tree model using the python and r programming languages. with this, we have also looked at how we can draw and evaluate the model. shortly we are going to cover all such kinds of models and concepts of machine learning and data science. to get all information, you can keep yourself connected to this . references ========== about dsw ========= data science wizards (dsw) is an artificial intelligence and data science start-up that primarily offers platforms, solutions, and services for making use of data as a strategy through ai and data analytics solutions and consulting services to help enterprises in data-driven decisions. dsw’s flagship platform unifyai is an end-to-end ai-enabled platform for enterprise customers to build, deploy, manage, and publish their ai models. unifyai helps you to build your business use case by leveraging ai capabilities and improving analytics outcomes. connect us at contact@datasciencewizards.ai and visit us at\n",
      "--------------------------------------------------------------------------------\n",
      "Free Article 10:\n",
      "photo by on so this is yet another blog on event-driven architecture 😒 🎉 however, instead of talking about the theoretical aspect, i’ll talk about the steps my team and i took when we recently introduced event-driven architecture into our system to break a piece of functionality out of a monolith. i hope this gives you useful insight into what building something like this looks like in a real-world scenario. some context ============ my company, heyjobs, matches essential talent with employers. our goal is to be the . we have a bunch of different codebases and services; however, like many growing companies, a large portion of our business logic is contained in one big, main monolith. and, like most growing companies, we are starting to feel the strain of our growing user-base and developer team so have started the process of slowly deconstructing it. most recently, we used event-driven architecture to split off a chunk of business logic to a separate service outside of the monolith. one of the responsibilities of my team is tailoring job recommendations to our users, and this is the functionality that we wanted to extract out of the monolith and into our new shiny job recommendations service. we base our job recommendations on which jobs a particular user has applied to, bookmarked, or a bunch of other trade secrets… the key takeaway, however, is that we need this data from the monolith in our service to generate recommendations. if only we could replicate the data from the monolith to our service with eventual consistency: event-driven architecture — quick definition event-driven architecture is a software architecture paradigm where events are used to trigger and communicate between decoupled services (commonly used among microservices). an event here means any change of state. event-driven architecture — in our case whenever a record we rely on to recommend jobs is created or updated in the monolith, we send an event and consume it (save it) in the job recommendations service. steps we took ============= (note: we use aws and in this article i’ll refer to specific aws services for simplicity, but these steps can be easily done with another cloud provider) created an event schema registry when implementing an event-driven architecture, it is important to ensure all events have a valid and consistent structure so consumers can process them successfully. we achieved this by storing json event schemas in a centralised registry which all event publishers and subscribers could access. we used to store our schemas. we created a github repository for managing the schema files. the ci/cd pipeline of the repository was configured to automatically run scripts that checked that the schemas were valid json and had backwards compatibility with previous versions before uploading them to aws. we also configured a file so that teams that wanted to subscribe to specific events could be automatically requested for review whenever a pull request was opened which proposed changes to a schema that they were subscribed to. we then created json event schemas for each of the events we needed. prepared our new service to ingest events this involved creating the database to store the records (obviously), creating an sqs queue where events will eventually appear, and implementing aws lambda functions for each event which basically checked events had all the necessary data, extracted and structured the data from the events, and then stored them. published the events we created an sns topic for each event we wanted to publish then implemented functionality in the monolith so that whenever a new record we wanted was created or updated, an event would be sent to the corresponding sns topic. before an event was sent, it would be validated against the event schema to ensure our events had a correct and consistent structure. our event pipeline was complete; we were ingesting events and replicating the records in our service with eventual consistency. backfill the existing data finally, once we had records from the monolith being successfully replicated in our service, we needed to do a one-off backfill to get the existing records. ops + maintenance ================== monitoring + alerting since the underlying infrastructure of the service that was extracted out of the monolith was totally changed, it was vital to create comprehensive monitoring for our system. we used datadog to create alerts for a range of metrics; some essential monitors which we can recommend are: number of messages on the dead letter queues free-able ram, cpu utilisation, read \\& write iops of the database number of sns notifications (if this number becomes unusually low there may be a problem with the publisher) deployment to deploy our service, we used the canary deployment strategy — meaning we deployed our api then gradually routed increasing numbers of user requests to it. this had several advantages: 1. since getting the capacity of infrastructure right the first time can be difficult, gradually increasing the load allowed us to find bottlenecks early and adjust our system without it getting toppled by the traffic 2. testing in production (you can’t beat this level of testing accuracy) 3. testing our alerting. when creating our alerts, we chose most of the threshold more or less arbitrarily. our deployment strategy allowed us to fine-tune our alerting over time. lessons learnt ============== supporting the release not too long after completing our release, our system started to be strained by the traffic. in hindsight, this isn’t surprising, but we failed to account for this when planning our capacity after the release. this meant other sprint items became de-prioritized mid-sprint to allow us to work on re-configuring our infrastructure. next time, we will ensure to keep some capacity free for supporting a release like this. conclusion ========== we have already started experiencing the benefits of owning our own micro-service and aim to keep building on top of this architecture by adding more events for other teams to utilize. i hope these steps can be useful for you and your team if you want to split up your monolith and benefit from this architecture too. interested in joining our team? or check out what we do at .\n",
      "--------------------------------------------------------------------------------\n",
      "Free Article 11:\n",
      "photo by on > 1. start drop-shipping store let’s begin with one of the most popular ways to make money online. drop-shipping’s popularity is growing, indicating its viability as a business model, according to google trends. drop-shipping is a viable method to make money online, as evidenced by success stories such as how an entrepreneur made $9,993 in eight weeks or how a business owner made six figures selling only one product. if you’re unfamiliar with drop-shipping, it’s a business model in which you sell a product to a customer, but the supplier stores, packages, and ships the product on your behalf. shopify drop-shipping gives you access to millions of products that you can sell in your store. some shopify drop-shipping apps allow you to hand-pick your product images, edit item descriptions, and give your business a personalized feel so that customers enjoy shopping with you. what is the best way to make money with drop-shipping? most business owners have concentrated on a few marketing strategies: making use of facebook ads influencers who promote their brands sending direct messages (dms) to potential consumers on social media are you ready to establish a shop but lack inventory photo by on 2. make money with affiliate marketing one of the most popular ways to make money online is through affiliate marketing. its popularity has risen and fallen over the years, but it remains a reliable way to earn money on the internet. the best part about affiliate marketing is that you can partner with a wide range of companies, including shopify, amazon, uber, and fabfitfun. affiliate marketing allows you to make money by advertising other people’s products. you may earn a commission on purchases by marketing retail items, software, apps, and other services. while a commission may appear insignificant, bear in mind that you may be an affiliate for several businesses and use multiple affiliate links in a single blog article. affiliate marketing allows you to make money by advertising other people’s products. photo by on 3. publish an ebook it’s never been easier to publish an ebook with amazon kindle direct publishing. simply write the ebook, format it, design an cover, publish it, and promote it. in 2016, i published many ebooks on amazon, and while they didn’t make me rich, they did bring me some money. you have the option of hiring a writer for your ebook, a graphic designer for the cover, and a manuscript editor to remove problems from the text. when studying the issue, concentrate on keywords based on frequent amazon searches. i frequently utilize the keyword tool, which allows you to find the terms people use when searching and tailor your title to suit. photo by on 4. start a blog blogging is one of the oldest internet money-making tools. people who enjoy writing tend to start blogs with a certain specialization in mind. a blog on procrastination, automobiles, drop-shipping, toys, and so on, for example, is typically specific enough to acquire a dedicated audience while yet being broad enough to cover a lot of material. you may create a blog on a variety of platforms, including shopify (delete the checkout option to avoid paying a membership as you grow it) and wordpress. emphasis on extremely precise keywords with a tight focus when you first start your blog, and then extend into other but still related categories as you develop and dominate new sectors. this will allow you to gradually construct a big blog. keep in mind that design is essential for establishing a positive first impression on visitors. here are 20 blog design ideas to help you get started. there are several methods to earn money through blogging. you can include affiliate links in your posts (be sure to include a disclaimer). you may monetize your blog by carefully putting advertisements in your entries. sponsored articles are a common way for review bloggers to earn money from certain sponsors. bloggers’ websites may also be used to sell digital or physical things. you may also utilize a blog to develop a personal brand that will help you land speaking engagements, media agreements, or large client contracts. photo by on 5. become a freelancer get wifi jay’s stories in your inbox ------------------------------------ join medium for free to get updates from this writer. subscribesubscribethe most straight forward approach to generate money online is to take your present 9-to-5 work and perform it online instead. for example, whether you’re a writer, administrative assistant, graphic designer, teacher, or developer, you may promote your abilities and find clients eager to pay you to use them online. there is also an endless array of work sites for each sort of freelancer. freelancing writers, for example, can apply for employment not just on specialist online writing job boards. but also on general freelance websites such as fiverr, freelancer, upwork, and others. if you discover that your abilities cannot be used directly to generate cash, you might consider monetizing other transferable skills you may have. to generate money online as a freelancer, you must first develop a great portfolio. to begin, this may include performing some free work with some recognized mid-tier businesses. once you’ve built a solid portfolio, you may approach potential large clients to earn extra money online. remember that freelancing is a numbers game: the more individualized your emails and applications, the more likely you will receive a response. photo by on 6. become a writer with a rising interest in content marketing, an increasing number of businesses are searching for writers to fill their online pages with high-quality material. focusing on a specialized specialty is the key to success as a writer. many authors want to be generalists, writing in a variety of genres ranging from gastronomy to technology. however, as a writer, having a particular emphasis sets you distinct and makes it simpler to acquire clients in your target market. when you have specialist experience, you may bring a unique viewpoint to a piece of material. that indicates you’re not simply repeating what every other writer on the internet is saying. and that’s exactly what businesses want to pay for: your opinions, experiences, and intimate knowledge of a niche. send appropriate samples to the recruiter while applying for writing employment. if someone requests a marketing writing example, provide one. please do not send a financial article. or even a fitness one. if a hiring manager can’t view a relevant sample, they won’t know how well you grasp the niche’s industry. apply for positions that match your abilities and expertise. you may discover writing jobs to generate money online on the websites listed below: iwriter fiverr upwork craigslist people per hour freelancer photo by on 7. become an online tutor consider being an online tutor if you want to generate money online on your own time. english writer published a tale of a tutor who earned $3,300 in one semester. surprisingly, he grew his clientele by word of mouth, exposure, and low group pricing. if you have a teaching degree, you may be more likely to find work as a tutor. a linguistic degree or experience is required for this sort of employment. if you’ve given a talk on the subject at a conference or event, you could be considered for an online tutoring, teaching, or mentoring role. while sciences and math are frequently in great demand for teaching professions, english is equally popular among foreign audiences. if you are an expert in a certain subject, tutoring may be the best way for you to generate money quickly. you can find online tutoring jobs on platforms such as: tutor me tutor.com cambly thanks for reading, p.s. subscribe to my newsletter for free money tips, free giveaways and follow me to get updates when i post! leave a tip if you love supporting writers and minorities in the space. this article is for informational purposes only, it should not be considered financial or legal advice. consult a financial professional before making any major financial decisions.\n",
      "--------------------------------------------------------------------------------\n",
      "Free Article 12:\n",
      "repository definition, internals, usage, in a 3-layer architecture =================================================================== in this article, we will reveal aspects of the repository pattern and how to use it properly on a code base. this post is originally posted at 🌊let’s dive into the main aspects of a repository ✒️ definition ============= > a repository encapsulates all the work needed to interact with aggregate, entity models or value objects that lives at out-of-process dependencies repositories are classes that fetch and manipulate models that lives out of our code base process such as a database, a microservice, a 3rd party service, file system io, etc. repositories are boundaries of our code base with the outer world. > a repository syncs the local models with the state on the source or shows the state of models from the source. repository definition ⚙️ internals ============ repositories have the following responsibilities : get client criteria send instructions based on the criteria to the out-of-process dependency (could be more than one) create a new object or a collection of objects with the returned data (denormalization of the data to a specific object type or collection of objects) return the new object or a collection of new objects to the client dedicated to return a specific object type or an abstract type. for example : a blogrepository is dedicated to returning only blog type objects a userrepository is dedicated to returning only abstract user objects (adminuser, commonuser, etc ) repository internals 🌲 usage ======= repository exposes a simple interface to interact with client classes. get petros koulianos 💀☠👽’s stories in your inbox ------------------------------------------------ join medium for free to get updates from this writer. subscribesubscribeclients must have easy access to repository instances with the help of di, service containers, and factories. with the repository pattern, we decouple all the work that needs a client to fetch and create objects. > repositories indicates a place our app communicates with the outer world use repositories in all situations that need to collaborate with external systems. repository usage 📚 repository in a 3-layer architecture ======================================= a repository belongs to the domain layer on a 3-layer architecture. repositories have domain rules and depends on the infrastructure layer to accomplish their responsibilities such as communicating with a database or making an http request on a 3rd party service etc. repository at 3 layer architecture\n",
      "--------------------------------------------------------------------------------\n",
      "Free Article 13:\n",
      "greetings, thanks for reading our blogs we’re so preceded for your support. this is for those who want to improve their self in malware areas. join their clubs! malware removal the discord server is for removing malware from windows and android devices, speeding up computers, and other computer-related issues. all services are provided without charge. malware tech a friendly community for anyone interested in cybersecurity to learn, socialize, and share knowledge with 6941 members. axial a bunch of nerds interested in programming, reverse engineering, and windows internals. and this is their discord group. threat.zone this is us. 🎉🚀🙌🪄👋🎉⚡️ capture the talent get threat.zone’s stories in your inbox --------------------------------------- join medium for free to get updates from this writer. subscribesubscribecapture the talent (ctt) was founded in 2021 to help businesses reduce risk in the hiring process for offensive security candidates. it soon became clear that those same assessment methods could be utilized in both ctf and lab-style environments for competitive events, team-building exercises, and as part of ongoing assessment of internal staff to highlight gaps in knowledge. check out their cool website! martian defense cyber team join the team of cybersecurity experts and learners from all over the world 24/7 to join the martian defense collaboration community. articuno’s zone articuno’s zone is a place where people can ask for help about technology problem that they have, such as computer, malware, virus, operating system, etc. locke they are a friendly new white hat hacking discord server that is open to all. this is the perfect place to learn and grow and share your expertise with other experienced enthusiasts. cybersec students they are a community of cybersecurity students and enthusiasts. are you looking for friends to solve hackthebox or tryhackme challenges with? do you want to learn with others? or do you just want to make new friends and hang out? if yes, then this is what our server is about. red team lounge they are an advanced discord based ctf and we would like to help others learn ethical hacking and programming. cool. thanks for reading! have a nice one. 👋\n",
      "--------------------------------------------------------------------------------\n",
      "Free Article 14:\n",
      "hey everyone, it’s update time. uptober is finally here! so am i bathing in cash? yes! in my dreams. truth is, we’re still down about 0.2%. but there’s been a subtle change in overall market sentiment. like something telling us that good days are coming. or at least that’s how it feels. because in reality things are not looking that great. the market and short-term predictions ====================================== there’s rumors about banks collapsing, europe is falling apart, inflation is still at all-time highs for most of the world and the un is begging the fed to stop raising interest rates. there’s even a rumor that the money printer will be soon turned back on because foreign currencies are dipping like shitcoins. but frustration with the current financial system is what caused the birth of crypto. maybe people will get so mad at banks and governments that they will look for alternatives. we already saw bitcoin inflows surging in the uk as the pound was collapsing. maybe the great decoupling is not that far away. probably not. trading the global financial collapse by betting on crypto would be an unorthodox move to say the least. it’s very unlikely that events like these will leave the crypto markets unaffected. at the current stage, crypto is mostly a tech-driven and innovation-driven industry. and these things don’t perform very well when there’s economic stagnation. so what do all of these mean? it means that the next cpi report and the next fomc meeting are extra important. honestly, i expect inflation numbers to be hot. and i expect the market to tank again. but maybe this will lead us towards the inevitable bottom, and from there i can start to make moves more comfortably. i’ve found so many good opportunities in the last few weeks but i’m too scared to make risky moves right now. especially after what happened last month. it’s better to wait for the inflation numbers and then try to think more long-term. what makes me feel a little bit more comfortable however is that i’m seeing plenty of very attractive prices without feeling like i’m trying to catch a falling knife. for example, i mentioned how after atom 2.0, cosmos is one of my highest-conviction bets in the entire industry. cosmos is now trading at $13 after hitting a high of $16.9 this month and a low of $12.03. so i can finally buy without feeling like i’m overpaying or feeling like i’m trying to fight the market’s momentum. $13 is a great entry point for atom in my opinion and it’s one of the coins that i’ll be buying and staking once i get in. but before we check on any new moves, let’s see how the current portfolio is performing. portfolio ========= dai represents the eth put i profited from 2 weeks ago most coins are still in the red, with premia casually doing a +22% in the last 24hours. let’s go through all the coins one by one: eth --- i’m not messing with my eth holdings this week. i think there’s a solid chance it’s never going below $1k again and also, it’s the highest cap coin i’m holding. i don’t think it would be wise to go full degen right now, and while btc outperforms ethereum historically in october, i don’t think it makes sense to get into bitcoin now either. i’m not totally against it however so don’t be surprised if you see me flip some eth for btc. matic ----- polygon had a very solid week, +12% in the last 7 days. since this is a project i believe long-term (this, eth, atom, and bnb, if you’ve read my bear market you know what i’m talking about), and it’s no longer a merge play, i have 2 options: stake it right now for an estimated apy of \\~19% in binance sell now and try to catch a bottom after the cpi numbers after some heavy consideration, i decided that the second option is fcking stupid. so i just staked my 113 matic and decided that if i wanna play michael burry with this coin i can just short matic on margin near the cpi numbers and then use the profits to buy even more. staked 113 matic with binance at an est. apr of 19% premia ------ holding. obviously. at this point premia could be here just for entertainment purposes. jokes aside, this is a very solid project and i’m waiting for their v3 update that is coming out some time this month. dydx ---- selling. i took a hard l on this move trying to buy a bottom. as much as i love the dydx v4 plan (standalone chain in cosmos to become the biggest decentralized futures exchange), i don’t expect it to happen any time soon. what i do expect to happen soon tho, is the market dumping/shorting the token when they realize that the circulating supply essentially doubles in january because of token unlocks. bnb --- was thinking about selling it last week but bnb is a classic bear market coin. will be holding xmon ---- sudoswap has been overperforming competitors this month, so i’m going to . synapse ------- it’s been a good performer but i’m going to exit my positions and get into other opportunities. cardano ------- since the vasil hard fork is now completed, this pick only makes sense from a long-term perspective. i believe in the vision and the tech and i have staked some cardano in my main portfolio, but it’s not what i need for this challenge. i’m selling. thorchain --------- i want to maintain exposure to the cross-chain sector, so since i sold synapse i’m going to hold. it’s a relatively safe bet long-term in my opinion because it is the market leader for something that will have very high demand in the future. will be looking more into the project this week and figuring out if it makes sense to lock the token for rewards illuvium -------- hard l in this play as well. i believe in this project long term but maybe this is not the best way to stay exposed to the crypto gaming sector. maybe i’ll flip it for blue-chip gaming coins like gala, decentraland or sandbox the moves ========= selling ------- sold 58.12 dydx for $68 (-18.09%) sold 39.3 syn for $51.96 (+3.9%) sold 104 ada for $55 (-8.86%) sold 0.669 ilv for $38.79 (-22%) total amount of cash from sales: $213.75 total cash available: $443.75 buying ------ first and foremost, i’ll buy and stake $100 in x2y2, in the autocompounder. why? i’ve already dedicated an entire post to that, you should read it truth is that at the time of the post x2y2 was almost at an all-time low, since i made the post it’s up 12%, which makes me a little scared to put a lot of money in here but remember, this is a yield play. i kinda wished i pulled the trigger 3 days ago instead of waiting for the challenge update but whatever. i trust my analysis and still consider it . bought $100 of x2y2 at 0.094 (1062 x2y2) and staked it in the auto-compounder i will also buy $100 of optimism. i double down on my layer 2s play and for a good reason. tvl has been surging in these protocols and they’ve been consistently outperforming alternative layer1s like cardano and avalanche in terms of capital inflows. bought $100 of op at $0.862 (116 op) so that leaves us with $243 in cash. from that, i’ll use $100 to buy my new favorite coin, atom. i’ll be also looking to stake it. i’ve talked about how i put atom in the same tier as eth, bnb, and matic in terms of that they’ll be profitable investments. i’ll stake an amount with binance at the crazy est. apr of 35.86% for 120 days, and use the rest of it to play in the cosmos ecosystem. bought $100 of atom at $13.17 (7.65 atom). staked 5 atom with binance at an est. apr of 35.86% total cash left \\= $143. i’m feeling extra degenerate this month. i’ll be looking to spend that extra $43 to xmon in the next dip. as i said, i’m . and the rest of $100 will be used to protect me from downside. instead of playing it safe and keeping capital in cash to avoid getting rekt on a dump, i decided to use that capital to buy puts and futures whenever i believe there’s a significant chance that the market will dump. so i’m essentially hedging my risk through gambling even harder. r/wallstreetbets would be proud. so the updated portfolio after all the moves looks like this: updated tether to include the liquidation losses from the perpetual future 2 weeks ago other opportunities =================== these are things i keep my eyes on and will probably publish analysis in a few days chainlink --------- after the updated tokenomics, this has the potential to be in my high-conviction bets territory doge ---- yes seriously doge. elon is buying twitter so if he decides to incorporate the coin in to the platform somehow, this thing could moon. imx --- a more long-term move because they have a token unlock in the 5th of november. maybe i can find a crazy discount for an otherwise solid project dust ---- i’ve talked before how i want exposure to the degods ecosystem, especially with the y00t reveal coming up. i have to take a look in its tokenomics before i make any decisions tokenized blue-chip nfts ------------------------- the more i get into the nft rabbithole, the more i understand that the boom we saw in 2021 was only the beginning. the nft space is here to stay and og collections like cryptopunks could be super valuable in the future. kucoin offers an option to buy the 1/1000000 of nfts like cryptopunks, bored apes etc. they’ve essentially bought the nft and tokenized it. it’s a good way to get exposure without breaking the bank that’s all i have. i delayed this update two days because i wanted to make sure that i’ve done the proper research to make moves. this is probably the most-action packed post in the series so far. let me know what you think of my moves in the comments. in the following days, expect more posts on the bear market guide series, and the much anticipated nfts post i’ve been working on for so long. as always, if you want to see my moves in real time, get live updates for the challenge, or discuss crypto and web3 with me, follow me on . stay safe, and see y’all next week! remember: this is not financial advice. content is purely for entertainment purposes. also, i know that this image isn’t from the wolf of wall street. it just looked cool as an outro. > new to trading? try or\n",
      "--------------------------------------------------------------------------------\n",
      "Free Article 15:\n",
      "not good enough. some people feel they’re exceedingly great at what they do. some people think they’re good at what they do. some people know they’re average at what they do; no pressure to do better. then there are people (like me) – that feel they’re never good enough. i don’t know the category you belong to, but the feeling of not being good enough sucks. heck! i look at other designers’ portfolios, and i’m like, “damn, can i ever be this good?” i read from other writers and rethink my writing process. i listen to some like-minded people speak and question my ability. i saw a guy a few days ago and immediately told myself: he was created on monday while i was made on saturday evening – evil grin. the list is just endless. however, whenever i show a colleague my designs or deliver a project. on a scale of 100, 60% love it. fair. i get messages from strangers about how my writing has inspired them to do better and how they’re looking forward to my next piece – most of my readers shower me with genuine praises. i have friends that often call me for advice and guidance. quite confusing. i read a quote online, and i loved it. it says, “many of you have a great life, but you don’t appreciate it due to what you admire on social media.” did that send some chills down your spine, also? eureka! to get better at this feeling (not good enough), i came up with a decision: how i feel should not matter compared to what i do. in essence – i should put my best in everything i do. it’s not right to feel less at what you do, but there’s a justification within, knowing you did everything you could; no stone left unturned. i’m not that great guy today; i’m not good enough; but i’ll be every damn thing i need to be – because i’ll do all that is required. i’ll be that great guy; i’ll be the best at what i do. fvck that feeling; i’m striving for excellence. i asked a friend about the business she had just launched, and she told me how it’s not been easy. it’s evident that “the beginnings are always tough” – considering the competition from every angle. listening to her talk reminds me of my exact feelings. but we are not throwing in the towel. it’s tough, always challenging, but we’re facing it head-on. again, fvck the feeling – let’s strive for excellence. let’s give it 100%, and by doing so, we can justify whatever comes our way. i’m optimistic if we continually and consistently provide the best at everything worth doing – success is imminent. it’s all dependent on what i do, not how i feel. whatever you believe hard enough, strong enough, and committed can become a reality. i’m not good enough, but i’m going to do what the greats do; put in my best – consistently. as long as there is a goal, there is no limit to my achievement. what about you? remember what pinky and the brain tell each other? pinky: what are we going to do tonight? brain: the same thing we do every day – try to take over the world. what are you going to do today? and the day after? and the week later? and year after year?\n",
      "--------------------------------------------------------------------------------\n",
      "Preprocessed Paid Articles:\n",
      "Paid Article 1:\n",
      "image by author introduction ============ you might be seeking a career opportunity in ai. or, you can be curious and eager to learn ai. either way, this article is exactly for you. like my other articles, i try to explain concepts very basically. > if you can’t explain it simply, you don’t understand it well enough. > > albert einstein if you are complete stranger to these concepts, do not worry this article will serve you too. now, let's get started with programming language. there are several programming languages that exist in the market. java, python, r, ruby, c, html. if you aim to research ai or plan to code, i suggest you choose python. you might be asking why? are the main reasons why to choose python. among the group that already choose python, stick with me. also, for others, this information might be helpful too. nowadays, ai-based technologies come into our lives frequently.\n",
      "--------------------------------------------------------------------------------\n",
      "Paid Article 2:\n",
      "more when you plan to deploy your workload on public cloud, you need a secure environment that can be operated efficiently. the (cis) published cis oracle cloud infrastructure foundations benchmark, a set of step-by-step security configuration best practices for oracle cloud, back in sep 2020. to help customers quickly implement a tenancy, that is secure from the get-go, oracle a-team recently published a terraform-based template to setup a landing zone on oracle cloud that meets the security guidance prescribed in the cis oracle cloud infrastructure foundations benchmark. having an oci tenancy, setup with cis benchmark, helps setup foundational security measures in oracle cloud tenancy that eliminates implementation guesswork for security professionals. following these best practices also minimizes complexity and enables security teams better manage risk and audit the use of oracle cloud infrastructure for critical, and regulated information systems. use oci landing zone to accelerate government of canada (goc) guardrails configuration ====================================================================================== the landing zone was originally developed to meet the cis benchmark for oracle cloud (which is widely used in the industry as a security benchmark), however, the same script also enables most of the technical controls defined in the goc guardrails within oci. the oci landing zone template greatly accelerates the implementation of goc cloud guardrails and quickly meet the 30-day and authority-to-operate (ato) compliance requirements. you can reach out to your oracle cloud account team to get the mapping document between cis benchmark and goc cloud guardrails as part of the gc cloud operationalization framework. oci landing zone ================ the oci landing zone template helps save time by automating the setup of a cloud environment for running secure and scalable workloads while implementing an initial security baseline through the creation of core resources. oci landing zone terraform-based template covers the following areas: press enter or click to view image in full size how to setup landing zone on oci ================================ here are the steps to implement a landing zone on oracle cloud infrastructure (oci). step 1: download terraform template ----------------------------------- download the terraform template to local machine from the github repository. refer to the below link to access the terraform template. download the zip file to your local machine. press enter or click to view image in full size step 2: use oracle resource manager (orm) stack to run the terraform template ----------------------------------------------------------------------------- log in to oracle cloud infrastructure using tenancy, and user credentials. i am sure you are aware that oci also allows you to set up multi-factor authentication for oci users. you can refer to the below link on setting up mfa for oci users. you can set up landing zone on oci by an administrator or a user with narrower permissions. in case you want to follow the latter path, there are some identity and access management policies you need to configure as pre-requisites. oci administrator can create these policies using the pre-configuration module. the steps shown below are to provision the landing zone on oci by the administrator. login to oci console \\>\\> resource manager \\>\\> stack press enter or click to view image in full sizeclick create stack press enter or click to view image in full sizeselect my configuration. under stack configuration, you can select either folder (if you unzip the earlier downloaded zip file) or select zip file to upload the zip file. press enter or click to view image in full sizeonce uploaded, you will see the stack information. provide the following details: working directory: file path to the directory from which to run terraform (let it be default one) name: name of the stack (you can change it to either environment say production or qa or any appropriate name) press enter or click to view image in full sizeclick next step 3: input configuration details ----------------------------------- environment details region: region for resource deployment service label: unique label that gets prepended to each resource created via landing zone advanced configuration compartment: new or existing (enclosing compartment under which newer compartments are created) policies: new or reuse existing policies group: new or reuse existing iam groups users: this is out of the scope of the landing zone script. oci customers will create this user and assign it to the appropriate group. for example, the network administrator user should be part of network admin group. press enter or click to view image in full sizenetwork details cidr block: list cidr blocks to create vcn. you can add one or many (up to 9) vcn; each will be for individual vcn. enable checkbox to override vcn name (of your choice). press enter or click to view image in full sizethe landing zone script helps you create hub and spoke network model. you can opt for this under advanced configuration. if opted, the vcn cidr block(s) mentioned here are considered vcn for spoke (in hub and spoke model). now, let us get back to vcn setup details. custom name for vcn: provide vcn custom name inbound ssh cidr list — external ip range allowed to make inbound ssh connection inbound https cidr list — external ip range allowed to make inbound https connection outbound https cidr list — external ip range allowed to make outbound https connection press enter or click to view image in full size enhanced network configuration using advanced options. press enter or click to view image in full size no internet access — this decides internet / nat gateway availability. if selected then internet and nat gateways are not created, otherwise it will be created. deploy hub \\& spoke architecture — if selected, it will ask for the dmz zone vcn cidr range. in case opted, provide subnet and size of subnet in dmz vcn. press enter or click to view image in full size connect landing zone to on-premise network — if opted, it asks for configuration details of dynamic routing gateway. if selected drg is provisioned, otherwise not. notification details oracle cloud provides monitoring service that uses metrics to monitor resources and alarms to notify when these metrics meet alarm-specific triggers. the oci landing zone template helps alert network and security administrators for specific events. provide network and security administrator email address. they will receive alert notification press enter or click to view image in full sizeyou can refer to the below link to set up alert and notification on oracle cloud infrastructure (oci) resource. cloud guard details select enable or disable oci cloud guard configuration. press enter or click to view image in full sizerefer to the oci cloud guard documentation . logging details (audit and network logs) oci audit log (consolidate to object storage bucket) opt to collect and consolidate audit log to object storage bucket. press enter or click to view image in full sizeif you enable check box to create service connector for audit logs, this will ask for to activate service connector to connect audit log to object storage bucket. press enter or click to view image in full sizeyou can refer to the below link to know “who” did “what” “when” and from “where” on oci compute instance using oci audit. oci vcn flow log (consolidate to object storage bucket) press enter or click to view image in full sizeif you enable the check box to create service connector for vcn flow logs, this will ask to activate service connector to connect vcn flow logs to the object storage bucket. vulnerability scanning enable or disable vulnerability scanning for oci resources. when enabled, scanning recipe gets created and scanning targets are created for each compartment landing zone creates. you also define scanning schedule and scanning day when to scan resources for vulnerability identification. press enter or click to view image in full sizethat’s all. once you provide all these details, click next press enter or click to view image in full sizeit displays the summary screen with all your inputs. kindly validate the same and if any modifications are required, click previous if not, click create. press enter or click to view image in full sizein order to run the terraform script, you follow steps — plan \\>\\> apply. you can enable the checkbox run apply if you want to run terraform script once it creates the stack (plan and apply together). press enter or click to view image in full sizeif you didn’t select the run apply checkbox, click apply to run the terraform job. press enter or click to view image in full sizeyou can monitor the terraform execution logs for progress and errors (if any). once the job turns to available (active) state, you can validate the configured oci resources under various services. press enter or click to view image in full sizepress enter or click to view image in full size step 4: verify and validate your oci tenancy against cis foundation benchmark ----------------------------------------------------------------------------- the oracle a-team has created a compliance checking script that checks tenancy’s configuration against the cis oci foundations benchmark. this script is independent of the terraform code downloaded earlier and can be used against any new or existing oci tenancy. script location: https://raw.githubusercontent.com/oracle-quickstart/oci-cis-landingzone-quickstart/main/scripts/cisreports.py you can execute this script from cloud shell or a local machine. let us see the steps to execute the script from oracle cloud shell. steps to execute from cloud shell open oracle cloud shell to setup environment. press enter or click to view image in full size$ python3 -m venv python-venv $ pip3 install oci $ wget $ python3 cisreports.py -dt — output-to-bucket ‘demo’ note: it is assumed you have a bucket named demo available. if not, please create one. press enter or click to view image in full sizeyou can find output in (private) demo bucket as well. press enter or click to view image in full sizeyou can download the csv file and analyze it further to identify your tenancy compliance against cis benchmark. sample csv output press enter or click to view image in full sizethat’s all… summary ------- in summary, we started with downloading terraform template and used it to create a stack in oci resource manager. we fed various configuration details and finally validated our tenancy against the cis benchmark using the validation script. thank you… first of all, i am very thankful to the oracle a-team for creating the landing zone and validation script. also, thank you to oracle cloud documentation team for technical information. last but not least, i sincerely thank my peers (pierre picard, bassey paul, olatunji akingbade, sreeni sivasubramanyam) who helped me on this blog. hope you found this article useful. disclaimer: the views expressed on this document are my own and do not necessarily reflect the views of oracle. document reference ================== secure landing zone on oci architecture — “” oci landing zone quick start template — oci landing zone single vcn architecture — oci landing zone hub \\& spoke vcn architecture — you may want to refer ---------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Paid Article 3:\n",
      "in the realm of game development, security is a top priority. while no game can be entirely immune to hacking, we can make it significantly challenging. this comprehensive guide will provide you with advanced tools and common strategies to secure your unity games from hacking attempts. obfuscate your codes ==================== for android games built by unity with ill2cpp mode, you should obfuscate your game codes. although some said this is just to make your code hard to read, this is a necessary approach to prevent your game codes from being revealed to the public in plaintext. however, i won’t recommend you obfuscate all of your codes including all the classes because this may lead to performance issues or application crashes. it is better to obfuscate some codes that reveal secrets, server urls, or business logic, but how to obfuscate android games using unity? i found the following plugins that may be helpful for you. obfuscator ========== feature: supports il2cpp supports to renames classes, methods, parameters, fields, and properties.\n",
      "--------------------------------------------------------------------------------\n",
      "Paid Article 4:\n",
      "more press enter or click to view image in full sizei regret a lot of things in life. being too shy to ask that person out. not asking for a raise. i wish i learned how to code faster or studied harder in school. in life we tend to regret the past and think about the decisions we could make to improve where we are today. anxiety ------- i get anxious about what’s going to happen next. will i lose my job? how long will this pandemic last? how am i going to afford my mortgage? do i need to get married soon? in life i get anxious about the present because i’m uncertain what will happen next, whether things will get better or worse. emotions ======== normally people tend to regret the past or be anxious about the future. what if it was reversed and we were anxious about the past and regretful about the future? what would that look like? would our attitude about life change? if i was anxious about the past, then i would be worried about what happens next during an event in my life like asking someone out. if i was regretful of the future, then i would be upset at myself for making decisions i knew were bad for myself today like watching television over reading a book. but i already know what happens next in the past, and it didn’t turn out as bad as i thought. she said no and i’m still alive. as much as i would be regretful of the future, i can still make changes today to prevent putting myself in a situation like that. there’s nothing stopping me from going to the library and canceling my netflix subscription. this puts us in the present and reminds us that at one point today was considered the future and tomorrow it will be the past. today ===== we once had yesterday, and we don’t have tomorrow but today is here and we forget to appreciate it. at one point during college, we stressed over exams and grades. today we miss the social life and friends we made on that stressful adventure. tomorrow we don’t know if covid will lock us down further, but today we still have the opportunity to meet with our friends and family. the freedom of today is liberating, and we should never forget that. follow me on my quest to 30,000 followers on linkedin\n",
      "--------------------------------------------------------------------------------\n",
      "Paid Article 5:\n",
      "more press enter or click to view image in full sizei recently had the privilege to attend an interview with uber for the role of software engineer ii — frontend. it was an intense yet enriching experience, and i want to share it with you, especially for those aspiring to land their dream job at uber. in this article, i’ll break down each interview round, explaining the problem-solving approaches, coding questions, and key takeaways from each stage. i’ll also dive into a topic i initially missed, giving you a comprehensive guide to tackle the uber interview. interview overview ================== the interview process consisted of 5 rounds: 1. online assessment 2. data structures and algorithm problem solving 3. tech specialization coding 4. system design and architecture 5. leadership and collaboration let’s dive into each round with sample problem questions and detailed explanations. 1. online assessment (35 mins) =============================== format: a 35-minute test on codesignal with 2 coding questions and 8 multiple-choice questions based on javascript, html, and css. difficulty: easy to medium. coding question 1: playing with digits (easy) --------------------------------------------- the first problem was focused on digit manipulation and could be categorized under math. problem example: find the sum of digits of a number until the sum becomes a single digit. function sumofdigits(num) { while (num >= 10) { let sum = 0; while (num > 0) { sum += num % 10; num = math.floor(num / 10); } num = sum; } return num; } explanation: this function repeatedly sums the digits of a number until it results in a single-digit number. the key challenge is understanding how to manipulate digits efficiently within a loop. coding question 2: longest substring without repeating characters (medium) -------------------------------------------------------------------------- the second problem was more challenging, based on hashmap and the sliding window technique. problem example: similar to . function lengthoflongestsubstring(s) { let map = new map(); let maxlen = 0; let start = 0; for (let end = 0; end < s.length; end++) { if (map.has(s \\| \\| \\| \\| \\| more content at\n",
      "--------------------------------------------------------------------------------\n",
      "Paid Article 6:\n",
      "it begins innocently. a text message. a shared laugh. a confidant who just gets you. but what starts as a casual connection can spiral into something that shatters trust and leaves your relationship in ruins. emotional affairs often fly under the radar, disguised as “harmless” friendships. but their progression is stealthy, dangerous, and heartbreakingly real. here’s how an emotional affair unfolds, stage by stage, and why each step can feel harmless — until it’s not. 7 stages of emotional affairs ============================= photo by via pexels 1. the spark of curiosity -------------------------- at this stage, the connection feels innocent — like discovering someone who speaks the same “language” as you. it’s not necessarily romantic or intentional. they might notice something small about you, like how you take your coffee or your quirky sense of humor, and it feels refreshing. it’s just a connection, right? but this moment plants a seed. that spark of curiosity often grows because it feeds a…\n",
      "--------------------------------------------------------------------------------\n",
      "Paid Article 7:\n",
      "more press enter or click to view image in full sizewe all have been there, happily getting ready to go to office or workplace and we are in our best spirits, great mood and full of positivity, then we reach our office and we are pumped up to conquer the world, full of enthusiasm and then suddenly something awful happens, be it an email or an angry customer or certain argument with our colleagues and then it all goes down to the drain from there. one incident ruins our entire day or sometimes weeks, we respond to it with impulse and later regret it or think we could have said or behaved differently than we had on that incident so that consequences or results would have been different as they are now. it’s human nature to at times say or do rather you wish you hadn’t. but some people are impulsive often, maybe numerous times a day. acting that way can lead to troubles and regret. when impulsivity goes far beyond control it might be an issue: people suffering from impulsive behavior are displaying signs of aggressive behavior, agitation, annoying others \\& being easily distracted. what we are trying to discuss is how to avoid/control impulse behavior in the office or workplace environment. self control ================ press enter or click to view image in full sizethe change in the world starts with you first and foremost is self-awareness about yourself which is followed by discipline yourself to observe self-control when you know you have impulsive behavior often when we receive an email or are in the middle of a tense meeting, we want to respond right away and set the record straight. however, controlling impulses and staying aware of how your response to things makes others feel can avoid conflict in the workplace and keep the peace, and it’s not hard to know about yourself and how you react to certain situations and triggers, which brings us to the most important word in self-control world is triggers! what triggers your impulsive behavior? since you are on your way to improve your impulsivity you should start with triggers, which touch nerves that trigger you to behave in such a radical manner. is it a person, a gesture, a word, or an email from someone or a team, or it’s just a certain response, feedback, or action that you receive from someone? once you pinpoint your trigger point you can start working on your trigger response plans which is basically how to react or control your impulsivity the moment triggers kicks-in. plan your triggers let’s be honest here, you will not have only one trigger for the impulsive behavior you have to deal with triggers one at a time. write them down, and try to de-escalate one at a time, don’t try to do four or five at once, you will be exhausted. pick the most destructive first, then work through the list over time. set a behavioral method to control your impulse. it may be something as simple as taking five deep breaths before speaking or counting to 10, anything that will slow you down and keep you at the moment where you can exercise the listening skills while mentally performing hearing the facts and emotions separately during the triggers. create an impulse control tool ================================== the more self-aware you become about your triggers and how you manage your impulse control, the greater the chance to avoid unsuitable outbursts and poor decisions. when you have a moment to look back at what you said or did, you have a better understanding of how you were triggered and how your actions destruct the situation. you may take appropriate steps to limit the damage. once you identify the right triggers for your abrasive responses/ reactions then you can start taking steps towards curbing your impulses and then improve your responses in order to have better self-control and eventually keep you from on your actual goals for which you had invested your energy and resources. it is quite difficult to change another individual or situation, but you can manage how you choose to react or respond to a situation. you can take appropriate action to control impulse. as a result, you can choose to bigger person and achieve more cheerfulness, engagement, and success in your personal and professional life. here are some specific tools you can utilize to improve impulse control: stop and breathe before you react to a situation, respond to an individual or send an email. become aware of distractions that are preventing you from listening, remember that instant gratification is short-lived and is about “playing small.” you want to be a bigger person and maintain a healthy sense of humor. evaluate options — no response is sometimes the most powerful response in certain situations. try to keep facts and emotions separate when you listen and you respond. listen to hear instead of listening to respond to someone. avoid overpromising and under delivering and practice “present moment” thinking. don’t feel the compulsion to respond to every email or text immediately. if you feel that just write it without sending leave your desk or mobile for 10 mins and come back and revisit the same email your perspective will change. keep a check and improve ============================ as famously said: quality of life depends on the gap between stimulus \\& response. prepare well \\& take time to respond to stimulus so you have better control of the situation rather than regret after firing an arrow incorrectly or misdirecting an arrow for some with impulsivity issues, there are far bigger problems at work. you might find yourself suddenly quitting your job over one rude email or agreement with a colleague, only to later regret it or even being fired if your impulsivity has you go against company protocol or upsetting treasured clients. in the long-term, this can mean you are either often unemployed or left in positions that are beneath your potential but involve less interaction with others and thus less of a chance your impulsivity will rear its head. your tone should vary depending upon the situation \\& response but it should always be in your control, never assume that everyone should have the same mindset or needs, your tone while responding should always accommodating rather being assertive, an accommodating tone invites lots of collaboration from the audience and should be used all the time. you need to keep a check on your impulse control measures and improve on them if you think certain measures are no longer working you need to go imaginative, but the idea is to keep a check and improve on your impulsive behavior. we encourage you to continue your journey toward self-awareness and practicing better impulse control. take charge of your success in life and the workplace and build a positive leadership reputation. press enter or click to view image in full sizewe publish short and interesting articles every fortnight so do subscribe or follow us on linkedin, twitter, facebook, and instagram so that you don’t miss out on our next article again, if you are still thinking to change your job \\& you need a new resume and cover letter visit feel free to write to us\n",
      "--------------------------------------------------------------------------------\n",
      "Paid Article 8:\n",
      "more press enter or click to view image in full sizeas a receptionist in a busy office building, i see a lot of people come and go all day long. around 4:30 is my favorite time of the day. it isn’t time for me to go home, but it is the time for you to stop by to drop off and pick up the packages. you are the hot delivery guy from united express. every day you flirt with me, and today is no different. “good afternoon, beautiful! how is the world treating you today?” you ask with a wink. i bite my lip before responding. “oh, you know. same ‘ol, same ‘ol. how has your day been?” “long and tiring, but it is better now that i am here. i wonder why…” i giggle and blush. you set a pile of packages and envelopes on my desk. i reach out to sign for them and you stop me. “i have to grab more. someone decided to make my day difficult with all of these packages. i guess that is ok. i get to see your face light up twice when you see me walk through the door.” does he actually know that i look forward to seeing him every day? “is it ok if i move these to the back counter. i just want to make sure there is enough room for all of them. we also have some going out.” “i will get those when i come back.” you leave to go grab the rest of the packages. i love how your uniform fits. as you said, i smiled when you returned. “good afternoon, beautiful. how is the world treating you? do you ever feel deja vu?” your joke makes me laugh out loud. tears are streaming down my face. “you have the most adorable laugh.” i blush and bite my lip. “thank you. you have a great sense of humor.” “well, thank you,” you say as i sign your tablet and you grab the outgoing packages. “guess i will see you tomorrow?” “there isn’t anywhere else that i could possibly be,” i say with a smile. you walk out the front door to your delivery truck. i grab the rest of the packages and put them on the back counter. as i am sitting back down at my desk, i notice that you missed one package. shoot. my boss said that it was imperative that this one went out today. i grab it and run for the door in an effort to catch you. luckily, you are still sitting out front in your truck. “hey, you forgot something.” “i didn’t forget anything. i needed a way to get you out here.” “huh? why did you need me to be out here? i mean, i get off of work soon…” “i hope i don’t sound too forward, but i want to see you get off in other ways.” i look down at my watch and notice that it is quitting time. “let me grab my purse and clock out. i promise i will be right back.” i turn and walk back to my desk. good thing i made it back inside, my boss was heading out the front door. “did that package go out today?” “yes, sir. he almost forgot it, but i ran after him and gave it to him.” “very good. have a good evening. see you tomorrow.” “see you tomorrow, sir.” phew! that was close. he almost caught me away from my desk again. i clock out, grab my purse, and head for the door. you are still waiting for me. why do delivery guys look so hot sitting behind the wheel of their truck? i walk up to the driver’s side and you smile at me. “well, there you are. i was getting worried that you weren’t coming back.” “my boss was heading out when i got back inside.” “whoops. i hope i didn’t get you in trouble.” “not at all. he was just telling me to have a good evening.” “in that case, get in here,” you say with a seductive smile. i head around to the passenger side of the truck. “not that side, over here. get in with me.” you pat your lap to suggest that you want me on your lap. “i don’t think i have ever had an offer to sit on a guy’s lap while he was in the driver’s seat.” i climb up to the doorway of the truck. i must have looked puzzled. “straddle my lap, good girl. i need to touch you.” i bite my lip as i hitch up my skirt and swing one leg over your lap. i nuzzle into your neck as you embrace me. your sweat and cologne mixed together creates such an arousing scent. you run your hands up and down my back before settling on my hips. i squirm as i nuzzle in closer. you chuckle and say, “i take it you like being really close.” “i love the feel of you against me, and i can’t get enough of your scent.” i feel you buck your hips beneath me. you pull my blouse from my skirt and run your hands up my back. i feel my bra loosen as you unclasp it and it makes me gasp. you bring both of your hands up under my bra and grasp my tits. i moan into your ear. “you seem to like my touch, don’t you?” “i…i do,” i start to grind on your lap. “i think you like touching me. your cock is hard.” “i may like kissing you, too. i need to find out, you know, for science.” your joke makes me giggle. “i think i can allow that. i mean, it is for science and all.” “good.” you lean in to kiss me. soft and gentle at first, then gradually becoming more passionate. i run my fingers through your hair. we break the kiss and you say, “you know, we probably shouldn’t be parked in front of the building.” you reach over and slide the truck door closed. “grab onto me, i’m going to move the truck to the back of the parking lot. we are less likely to draw attention over there.” i embrace you tightly as you start the truck and move it to the back of the parking lot along the tree line. as the truck is moving, the vibrations make me want to grind on you more. fuck. why is this so hot and why am i turning into a little slut? you nuzzle my neck and say, “now, where were we…,” you start to kiss down my neck, “oh, that’s right…i was about to do this…” i never expected you to be this rough. you grab my blouse and rip it open. buttons scatter across the floor of your truck. i gasp as the cool air hits my skin. you lower the blouse off of my shoulders and pull my bra off. “mmm. that is much better,” you say before growling and taking one of my nipples into your mouth. my mouth drops open and i moan. your hands support my back as i lean back onto the steering wheel. you release my nipple and say, “i can’t let the other one get jealous.” i moan again as you take the other nipple into your mouth. you are taking my breath away, but i grab your shirt and try to pull it over your head. you don’t release your mouth from me right away. i gaze into your eyes. i can barely form words. “please…please…please…” “what are you begging for, little one?” “mmmm. i want your skin on mine. i need the warmth of your chest on mine.” “so needy. i guess i can let you do that.” in one motion, you pull the shirt over your head. my hands are drawn to your flesh like a magnet. i run my hands up and down your chest. “mmm. good girl. my muscles are so sore and your hands feel like heaven.” i pull myself toward you and embrace you. the skin on skin is just what i needed. my stress is starting to melt away and my guard is coming down further. “get in the back.” i am kind of shocked at this command, “excuse me?” “i want to fuck you in the back of the truck. get in the back.” i try to cover myself up, but you aren’t having any part of that. as i get off of your lap, you smack my ass. i stop covering myself and gasp. “that’s better. don’t cover up,” you say as you smack my ass again. we get to the back of the truck and you pin me against one of the walls. “don’t fucking move. i need to worship your gorgeous body.” you kiss me hard. our tongues tangle with one another. we break the kiss and you say, “good fucking girl.” this makes me shiver. no one has ever called me that before. you kiss my neck and my legs almost give out. a moan escapes my lips. “you seem to like this. i bet that pussy is so wet for me.” all i can do is whimper. “use your words,” you growl. i gasp before responding, “yes, sir.” “mmm. good fucking girl.” i can’t move, not that i would want to. your body holds me against the wall. you look into my eyes and say in a low voice, “don’t move from this wall. you may grab onto me if you need to.” i wonder why you are saying this and i soon find out. you start kissing down my body. i grab at your head partially to steady myself, partially because i don’t want you to ever stop. this feels so good. i start to squirm. my pussy is throbbing so much that i need a release. “someone seems to be enjoying herself.” “aaah…yes, sir. don’t stop. please don’t stop.” you chuckle before continuing your journey down my body. as you kiss my tummy, i flinch. “are you ticklish?” “a little, sir, but mostly i am very self-conscious of my tummy.” “you are beautiful. every part of you turns me on. i have had to show so much restraint over the past few months of interacting with you every day.” you run your hands down the outside of my thighs then back up the inside. “spread those legs, good girl, i need to taste you.” i spread my legs apart wider and i feel you lift my skirt. your nimble fingers pull my panties to the side before you start to lick my pussy. i become a whimpering mess. i grasp your head tighter and pull you into me. as i loosen my grip you say, “mmm. you taste so good. i could lick your pussy and worship your body all day. but right now, i need my cock inside you.” before standing back up, you thrust 2 fingers inside of me. “you are so fucking tight.” you stand back up and kiss me deeply. i can taste myself on your lips. you guide me over to the side of the truck. “over here, good girl. i want to fuck you while you are bent over on one of these shelves.” i do as i am told and walk over to one of the shelves. you push me down onto one of the shelves. your mouth trails kisses up my back to my neck before growling in my ear, “you had better hang on. i want you so bad that i am not going to be gentle.” i whimper and say, “i don’t want you to be.” “good fucking girl.” you pull my skirt up and my panties down. your hand feels so good on my ass. i am snapped back to reality when you smack my ass hard. i hear you unbutton your work pants and lower the zipper. you drop your pants to the floor and i hear you open a condom. “i am prepared. i have wanted to do this with you for a while.” you tap your cock against my pussy and i mewl while wiggling my ass. “i need your cock so bad. you have me so turned on.” “mmm. let’s see what we can do about that, shall we?” you line your cock up with my pussy and thrust only the tip in. i wriggle trying to get you to thrust the rest of your cock in. there is a touch of evil in your laugh. “not yet, needy little slut. you will be teased first and then fucked relentlessly.” you thrust in a little further and i moan, “i want you so bad, sir. please…” “you are going to make this hard for me, aren’t you?” you thrust slowly in and out of me. never going in fully. you continue to tease me and i am a whining mess. i claw at the shelf that i am leaning on, just trying to reach my tits or down to my clit so that i can get some relief. you have me pinned. “no touching yourself, slut. only i can touch you.” you reach across my back and grab my throat as you thrust all of the way into me and stop. “thank you, sir. thank you for filling my needy little pussy completely. mmmm.” you keep me pinned like this. before thrusting again, you kick my feet apart and push in deeper. fuck, i just can’t get enough of this. i clench my pussy down on your cock. “damn, you feel so good this tight but i want you to cum. do you think you can?” i nod my head, but then remember to use my words. “yes, sir.” with your hand at my throat, you start rubbing my clit with the other one. you slowly slide your cock all the way back out then slam it all the way back in. then you tease me by going really slow and gentle, all the while rubbing my clit at a fast pace. “ah….mmmm…please let me cum, sir.” “just what i have been waiting for. cum for me. cum right now!” as your slow paced thrusting continues, i clench down on your cock even harder as i cum. i scream out, “fuck! thank you, sir. thank you for letting me cum.” “good fucking girl!” you slide out of me, turn me around, pick me up, and slam me into the wall at the front of the truck. i squeal, “no one has ever picked me up like that.” “well, i am about to fuck you like this,” you growl as you thrust into me and fuck me hard. you start kissing my neck and end up biting it. i have never had sex this aggressive, but i think i could get used to it. my nails dig into your back as you are relentless. you are so powerful like this. there is something animalistic about being fucked like this. i can feel every muscle in your body moving as you fuck me and it is turning me on even more. you growl, “fuck i am about to cum. cum for me again, good girl. milk me and make me cum.” how can i say no to that? with your growls in my ear i cum again, harder than the last time. i feel your body tense and i know you are close. i coax you “cum for me, sir. i need your cum.” you start to twitch inside of me while screaming out, “good fucking girl,” so loudly that anyone driving by on the highway probably heard you. that is perfectly fine with me. i made you cum like that. as you are coming back down from your orgasm, you bite my shoulder and nibble at my ear. “wow, i might need to fuck you again tomorrow. what do you think, good girl? do you think you would fuck me again in this truck?” “hmm. maybe. i might want to try something more daring next time. maybe against the truck outside, where people passing by can see us. i will be sure to wear a short skirt tomorrow.” you help me gather my clothes and get them back on. being the gentleman you are, you drive your truck over to my car so i don’t have to walk with my blouse falling off since you ripped all of the buttons off. “if you want me to get your blouse fixed…” “i will take care of it, and i will remember to wear a blouse without buttons tomorrow. you have a tendency of getting aggressive,” i giggle. “so, same time, same place tomorrow.” “absolutely. i can’t wait. maybe tomorrow we can do dinner or something after.” “sure thing. i might be thinking about dessert after that,” you say with a chuckle. i smile. that might be on my mind, too. i’ll never admit it. “i will be looking forward to your delivery tomorrow.”\n",
      "--------------------------------------------------------------------------------\n",
      "Paid Article 9:\n",
      "more press enter or click to view image in full size dr. anthony fauci, director of the national institute of allergy and infectious diseases, testifies at a hearing of the u.s. house committee on energy and commerce. credit: / getty images the coronavirus fantastic four — , md, director of the national institute of allergy and infectious diseases; robert redfield, md, director of the centers for disease control and prevention (cdc); stephen hahn, md, commissioner of the food and drug administration (fda); and brett giroir, md, assistant secretary for health — today to testify before the house energy and commerce committee about where the united states currently stands with the . here are five key takeaways from the meeting. 1. we are on-track for a vaccine to be available by the beginning of 2021 fauci confirmed that the first phase 3 trial for a , developed by the drug company moderna, will begin in july, and if all goes well, it could begin to be administered to the public early next year. while there is still no guarantee the vaccine will work against the virus in the general population, fauci says he is “cautiously optimistic” given the data so far. several other vaccine options are also in various stages of development and testing, and preliminary results from animal studies are promising. there has already been a substantial financial investment made toward manufacturing a vaccine in order to scale up production and distribution if and when one is approved. however, fauci and hahn reinforced that the only risk being taken is a financial one, not a scientific one. no corners will be cut in terms of safety and efficacy criteria, and vaccines will be assessed by the fda with the normal rigor before one is approved. 2. testing capacity has accelerated since march, not slowed down despite comments in recent days by president trump that he’s urged health experts to limit testing, all four men stated that they had received no such directive from the white house. giroir said the u.s. has performed 27 million tests so far, and they anticipate conducting 40 to 50 million tests by this fall. 3. a new test is coming that can simultaneously diagnose the coronavirus and influenza the pending flu season coinciding with a second wave of covid-19 was a big topic of conversation. redfield said that the cdc has been working on a single test that will screen for both the coronavirus and influenza a and b so that doctors can diagnose and treat patients faster. the experts also urged people to get the flu vaccine to mitigate the risk of at least one deadly viral outbreak this winter. “this single act will save lives,” redfield said. 4. decisions about reopening schools will have to be made on a regional level fauci emphasized that we live in a big, heterogeneous country, and the outbreak looks very different in different areas. as a result, decisions about reopening will have to be done by city and county, not at a national or even statewide level. if infection rates are low in a given area, schools there may be able to reopen normally in the fall. in regions where cases are spiking and community spread is present, schools may have to adopt new schedules with morning and afternoon shifts or alternate groups of students on different days. “there is no one size fits all,” fauci says, and it’s up to local officials to make decisions based on the recommendations from the cdc. 5. we cannot relax our personal prevention vigilance, everyone is responsible for this pandemic following questions from representatives from texas and florida, two states that are currently seeing their highest case rates yet, fauci urged people not to congregate in crowds and to wear masks in public. “don’t throw caution to the wind!” he entreated. addressing young adults specifically — who have been implicated in many of the recent outbreaks — he asked them to think not only about themselves and their own risk for infection, but the role they play in spreading the virus to others, particularly those who are more vulnerable. he says that even if young people think they’re at low risk for a serious infection, by propagating the outbreak they’re hurting their neighbors, friends, and relatives.\n",
      "--------------------------------------------------------------------------------\n",
      "Paid Article 10:\n",
      "more press enter or click to view image in full size photo: /getty images it’s been about four weeks since demonstrators began protesting the murder of george floyd and the racist police brutality that has killed countless black people across the country. during that time, concerns were raised that the protests would lead to a spike in cases. that data took a while to surface because it takes roughly three to 14 days after infection for symptoms to show up. now, the numbers and early analyses are beginning to trickle in, and so far they suggest that protests have not been a major factor in case increases. experts have made it clear that it will be because the protests began around the same time that the country began reopening. the protests also began soon after memorial day, when many people relaxed their social distancing efforts. new research that broadly compares the number of positive cases in cities that did and didn’t host protests, however, is revealing some interesting patterns. this month, researchers with the national bureau of economic research published a showing that the black lives matters protests didn’t cause a spike in covid-19 cases in the three weeks after they began — and it offers an unexpected reason for that observation. the paper, which hasn’t yet been peer-reviewed, analyzed anonymized cellphone tracking data from safegraph, together with data from the centers for disease control and prevention on local prevalence. the researchers used the phone data to compare the social distancing behavior of people in 315 of the largest u.s. cities, dividing the cities into those that held protests and those that didn’t. on the whole, people in cities that had protests generally increased their social distancing, they report. even though protesters in those cities were obviously not socially distancing, nonprotesters living in those cities, “perhaps due to fear of violence from police clashes or general unrest, may have chosen to avoid public spaces while protests were underway,” they write. the net effect is that cities that had protests generally didn’t have an overall rise in cases. the data, they say, shows “no evidence” that net covid-19 cases increased after the protests began. “we conclude that predictions of broad negative public health consequences of black lives matter protests were far too narrowly conceived,” they write. zooming out to assess the general population, they note that “public speech and public health did not trade off against each other in this case.” there is some local data about new infections from sites that tested protesters, though in most cases experts have yet to determine what they mean in the context of local historical data. in boston’s tests of more than 17,000 protesters, , which massachusetts gov. charlie baker said is “reasonably consistent” with state numbers overall. seattle announced on june 6 that of 3,000 protesters tested, had tested positive. the wall street journal reported last week that in minneapolis, tested at pop-up sites tested positive as of june 22, and 0.99% of an additional 8,500 protesters tested through health care providers and other sites had positive tests. south carolina activists on sunday that four organizers, three photographers, and six protesters who had marched in the city of columbia between may 30 and june 17 had tested positive, warning others not to join demonstrations until they got tested. the los angeles police department, meanwhile, has seen a spike in cases, the los angeles times on tuesday. positive cases within the lapd have increased from 170 to 206 in the past week, a 21% increase and twice the rate of change observed over the course of the pandemic, according to lapd chief michel moore. he cited the protests as part of the reason for the increase, noting that officers were “challenged” by face coverings as they tried to communicate with one another. officers with the lapd, like many other police departments, as spikes in more than half the states illustrate, the pandemic shows no signs of abating in the united states. in some cities, the protests don’t, either. this confluence is making it hard for some people, including protesters, to get tested. california is experiencing a huge spike in cases, with a record-breaking 6,600 infections reported tuesday. in los angeles county, where protests are continuing, officials confirmed on tuesday that there were no at more than 40 sites run by the city, county, and state. when the protests began, los angeles county was one of several municipalities harshly criticized for in response to looting and rioting. last week, the star tribune that concerns over testing shortages in minneapolis were also making it hard for protesters to get tested. some clinics would not test people who were asymptomatic. experts have pointed out that protesters tend to be and therefore may not show signs of severe disease, even if they are infected. previously on the medium coronavirus blog, i that we should direct our resources toward responding to new cases rather than puzzling over whether they’re being caused by the protests. that was two weeks ago, but it still holds true now, because the situation hasn’t changed. as more data from protester-specific testing sites is released, we’ll get a better picture of the role protests have played in the spread of the coronavirus. in the meantime, the focus must be on making testing and contact tracing available in the places that need it — the bare minimum required of a responsible public health response.\n",
      "--------------------------------------------------------------------------------\n",
      "Paid Article 11:\n",
      "more i didn’t ask if i could use the picture. that’s the doane family, by the way: michael, sarah, claudine, and guillaume. i stole the photo from sarah’s facebook page. i only know one of the people in it, though — the guy on the left — michael. and i only met him once. he came to tulsa one weekend and, hilariously, spent most of it projectile vomiting at the doubletree tulsa downtown. maybe it was something he ate, maybe it was a trip out to south tulsa, where i showed him the praying hands at oral roberts university. i never got tired of reminding him how he left his mark on tulsa. michael was an atheist, but take another look at the picture above. if you can’t see his joy at the inexplicable, transcendent love in and around his life, look again. like most of the friendship around these parts, mine with michael was almost exclusively online, occasionally on the phone. he was a novelist (, ), as well as wrote books to help businesses increase productivity (i have no idea). he taught, he lectured, he screamed at injustice. he hated bad politics and loved christopher hitchens. i can’t think of a better basis for a friendship. we were both contributing comments to the the politics blog with charles p. pierce at esquire.com and we began commiserating over the state of just about everything. michael told me of my writing, “look, ba, i like your stuff, but everyone is outraged; so if you’re going to keep writing about politics, find an angle — your angle — otherwise, you’re just tossing your anger on a pyre that’s already burning out of control.” months later, so fed up, he told me he was leaving america. everybody i know says that, but he did. first he went to south africa, then to france, where his wife, claudine, is from and where they met some forty years ago. somewhere along the way, there was cancer — there always seems to be cancer along the way — and he went to get treatment in a country that does such things better than we do. he used to call me on sunday mornings from angers, france, and we’d spend hours talking about writing, his nurses, his love of his son’s homemade french fries, and the connections between civility and chaos and men and women — not to be redundant. we’d talk on facetime or skype — he’d be on a sofa — and because he had cancer, i literally would see less of him each time we talked. “i’m not afraid of death,” he told me the last time i spoke to him, “just the dying.” cancer loves the dying part. i trusted michael with the most important thing i ever wrote: four days and a year later, a book about the four days following the death of paul, my son. i asked michael if he’d be my editor. he agreed but only if i agreed to think of it as a book and not just a diary — only if i agreed to tell the story and not just wallow in a father’s grief. he was both kind and brutal, for he knew readers wouldn’t necessarily care about the things that were tearing me apart. i had to tell the story. i had written a scene in an early draft where i tell paul that if only i did something else, if only i were sculptor or a gardener, i could have made something for him, planted something for him, instead of just writing about him. i wrote, “i could have planted a tree for you and watched it grow.” michael called, furious. “you will not have a fucking paul tree in this book!” he screamed. i could hear him banging something. “it’s not a greeting card.” he was right. i took it out. along with dedicating the book to my girlfriend, i thanked michael for changing the way i looked at myself and the way i looked at writing. when friends die — and they are, increasingly — i remind myself how lucky i am to have been here when they were. i was here when michael was. death is a motherfucker. life isn’t. look at that picture again.\n",
      "--------------------------------------------------------------------------------\n",
      "Paid Article 12:\n",
      "more press enter or click to view image in full size photo: /getty images trump sees nothing but good news in the coronavirus. unlike other world leaders with better health and economic results, who aim to limit death and economic loss with masks and containment, trump’s approach is a mix of lying, denial, and spin. his latest “i’ll say anything” is that we should stop testing to reduce cases (he insists he’s not joking and is actually reducing testing sites) and that the death rate is declining. don’t celebrate just yet. we were at the train stop called “abject denial” back in february, as cases went undetected through the community. then, we didn’t test because trump wouldn’t prepare and we couldn’t see the deaths about to come. now it’s willful—he’s telling supporters not to get tested. i found the incompetence of february slightly better than purposeful negligence of june, but i suppose it’s a matter of taste. now, trump plans to end support of testing sites on june 30. because, again, if we can’t see it, how could it be happening? and how could people not tested infect others? (people from outside the u.s., please, the laughing is very impolite.) eventually, those cases moved from younger people with no or slight symptoms to the older population, and people started to get hospitalized and die. we remember that, right? so when cases moved from the first states to the next ones, like texas, florida, and arizona, were they ready with contact tracers, testing, and isolation? nope. they instead crowed that the first states were irresponsible, and they needed to get back to business without those silly things. now 26 states are headed in the wrong direction. seven states are on the mend. and the rest are in the middle. is the first reaction to case growth caution? nope, it’s a combination of lying, cheating, and repositioning. florida with its sudden rash of pneumonia, firing honest officials, reclassifying data, and now not reporting icu capacity has taken a novel approach to public health. going through the trouble of reclassifying cause of death to lead the public to think something is safe is energy that could be spent elsewhere — like denying people medicaid or the right to vote. so little time for everything. how do they choose? in fact, cases do spread as dr. anthony fauci said, and some percentage of people are hospitalized. you see, you can’t isolate old people, sick people, or people of color until there’s a vaccine. and even if you could, it’s not very nice. as of today, seven states are reporting new highs in hospitalizations. arizona, arkansas, california, north carolina, south carolina, and texas. and 20 states don’t fully report hospitalizations. but why would that be important? it is true that more people will survive hospitalization than a few months ago. that is great news as therapies to treat covid-19 complications are being discovered. but there is something that is not talked about nearly enough. the long-term effects of covid-19 are unknown and scary. with sars-cov-1, high percentages of people have mental health problems and chronic fatigue years later. so far, sars-cov-2 recoveries are long and troubling, and conditions like ground-glass opacification, mental acuity, fog, and more are already being reported. and we’ve been at this only a short while. when trump tells young people and governors not to test and not to wear masks, the likelihood of the virus finding people to kill or severely hamper goes up. when governors falsify data, advantage virus. for some, losing 120,000 people and likely a good deal more in three months would put a damper on the presidency. maybe attend a funeral or two. thank some nurses. talk to some widows. maybe look like you care, and tell people to be cautious. i don’t think the country is expecting much at this point out of the president. but the bar is still higher than adding fuel to spread the fire of the virus.\n",
      "--------------------------------------------------------------------------------\n",
      "Paid Article 13:\n",
      "more do you have a real problem with cheat meals? you have a cheat meal (either planned or unplanned), you eat it with a combination of pleasure and guilt, and then either kick your entire diet to the curb, thinking “i blew this meal, i may as well just blow the rest of the week/month/year, and get back on track later.” or, after the cheat meal, you practice ridiculous restriction, like the that christian bale used to lose a ridiculous amount of weight for the machinist, so that he could look unhealthy. press enter or click to view image in full sizeif that’s you, i get you. a lot of my personal training clients often come to me, and “confess” their cheat meal. almost like “forgive me father for i have sinned.” then we do some undercover work, and find strategies that clients can use that matches both their physiology, as well as their psychology. because i’m not there with you 1-on-1, personalizing your own strategy around cheat meals, here are 20 strategies that i’ve used with clients in the past, that helped them have their cake and eat it too (literally!). although if your issue is emotional eating, and you really need help with accountability, just to see whether we can work together. if you prefer to watch the video version of this article, check it out: and if you prefer to read, here are the cheat meal strategies, in no particular order: tip #1: exercise before your cheat meal ======================================== if you know you’re going to be having a cheat meal, exercise before it. doesn’t matter how much time before the meal, just burn some calories. strength training, cardio, whatever. if you were going to exercise anyway, either lengthen your workout, or add a second mini-workout, that’s a bit easier than the first workout. tip #2: exercise after your cheat meal ======================================= some people, if they exercise before their cheat meal, they feel like they now have permission to eat whatever they want. they might have burned 300 calories during their exercise, but end up putting away 600 calories more than they would have otherwise. for these folks, it’s better to exercise after a cheat meal, because you feel like every bite will make you have to exercise that much longer. either exercise a few hours later, once the food has settled, or the next day. tip #3: exercise both before and after your cheat meal ======================================================= let’s say it’s a whopper of a meal, and one exercise session just doesn’t amount to a larger percentage of that meal — try doing it before and after. tip #4: plan what you’ll eat before your eat it ================================================ if you’re going out to a restaurant, check out their menu online before you go. this way, before the waitress can even give you the menu, give your order. the reason is if you wait until you’re already in the restaurant, you’ll be hungry. and when you’re hungry, you’ll usually go for the tastiest meal. unfortunately, that’s rarely the healthiest meal. now sure, cheat meals are usually not that healthy to begin with, but nonetheless, you can make better choices when you’re full, and not hungry. tip #5: eat less before ======================== at the end of the day, fat loss, muscle gain, and weight maintenance is . so if you know that you’re allowed 2500 calories per day, and that delicious cheat meal will be 1500 calories, just divide up the remaining 1000 calories between the 2 meals before your cheat meal. tip #6: eat less after ======================= for some people, if they eat less before, they feel super hungry (or super entitled to extra calories) when the cheat meal does come, and end up putting away more food than they planned. for these folks, it’s easier to eat less after, because you don’t feel a sense of deprivation. you’re just full from that large cheat meal, so you naturally want to eat less. i personally prefer this strategy. my girlfriend prefers the former. no right or wrong way. whatever suits your preferences and personality. tip #7: eat slowly =================== this works for a couple of reasons. first, there’s about a 15–20-minute delay between when you’re full, and when that sensation actually registers in your brain. if you’re eating quickly, you put away more food, so you go from hungry to stuffed/bloated. you miss that “full” stage. if you eat slowly, you catch that signal, and stop eating when you’re satisfied. the second reason this works is because your savour and enjoy the food more, so you don’t need to eat as much of it. tip #8: drink water with your meal =================================== water occupies room in your stomach, and your stomach fills up. since water has no calories, you get fuller with less food, but the food that you did fill up on was very satisfying. tip #9: minimize the variety ============================= ever notice how when you’re at a buffet, you finished eating everything — the meat, the pasta, the breads, the sushi, and you’re completely stuffed. you couldn’t eat another bite of the “main course.” but you still have room for dessert? that’s because hunger is regulated by the brain as much as it is by the stomach. so if you minimize the variety of your meal, you eat less. craving chocolate cake as a cheat meal? then just eat the chocolate cake. nothing else. eat a lot of chocolate cake. until you’re satisfied. you’ll still eat less than if you ate pizza plus chocolate cake. limit your entire meal to just 2–3 foods. tip #10: eat healthy stuff first ================================= i have what i call my “mandarin strategy.” i love buffets just as much as the next person. but i don’t like getting fat. so i fill up on healthy stuff. my first go-around is salmon and broccoli. and i get progressively less healthy with each round. second round is beef and potatoes. third round is sushi and/or pizza. fourth round is dessert. by the time i get to the third and fourth rounds, i’m relatively full from the first 2 rounds, i don’t have much room for rounds 3 and 4. they’re still super satisfying, i just don’t feel like having much of them. no sense of deprivation. tip #11: have a… ================= to read the rest of this article, .\n",
      "--------------------------------------------------------------------------------\n",
      "Paid Article 14:\n",
      "the art of flutter ================== the art of flutter is a beginner-friendly medium series illustrating different key topics and insights about and . this series will also introduce , a backend-as-a-service (baas) platform as part of the and how to use flutter with firebase using for in production at scale. the current state of flutter web -------------------------------- in this article, i will share with you my insights and observations about the current state of flutter as well as the support for web software development. lastly, i will share key takeaways in flutter web. > trends in the global trends, flutter remains one of the most promising and trending open-source projects with many developers around the globe who are adopting the framework for their software development projects in mobile and web…\n",
      "--------------------------------------------------------------------------------\n",
      "Paid Article 15:\n",
      "more “value” for product managers can mean a variety of different things. it is also what makes the position quite challenging. you need to be able to recognize different versions of it and measure them against each other. here is where the challenge begins! you often need to compare oranges to apples and make a call which one should be prioritized. to make it even more challenging, some aspects of the value are not measurable, throwing some of the data-drivenness out of the window. the ability to make such a call is called “product sense”, but this is a story for another time. please note, that understanding value is not only relevant for backlog prioritization! it’s also very important to any product manager’s day! you only have a few hours a day dedicated to working and no matter how much overtime you throw in, there will always be more to be done. thus, you need to be smart and dedicate your time to activities that will bring the most value! thus, for my first premium medium article and the last entry of 2021, let’s look at those different varieties of product manager’s values to see what we are up against! direct metrics’ growth ========================== this most basic, business-driven aspect of the “value” concept. it will often drive your product goals, it’s easy (and mandatory!) to measure and for less experienced product managers it represents the only type of value they pursue. in extreme cases, a task won’t be allowed to a backlog without a metric impact hypothesis. it’s most often achieved by introducing new features and improving existing ones. it’s also adding features that your competitors have and you don’t! it’s relatively easy to measure and one can always use a/b testing to confirm the change’s impact on the product. preventing probable future drop in metrics ========================================== we’re rather early in the article and it is getting complicated already! this is where a crystal ball is needed, as you want to prevent future loss. any product is like a house; if not maintained and fixed, it will eventually collapse. this aspect translated into quality that the client/user often doesn’t get to appreciate until it’s too late. simply put, it’s all the work that needs to be done in order to get the product working now and in the future. it’s to ensure it’s stable and can support all the clients you can take on board (but not all in the world! it’s overkill!) to achieve this kind of value, you will have your development team fixing bugs, making products scalable, and introducing/replacing old technologies in favor of newer ones. while fixing bugs is easy to grasp (though not always easy to prioritize), making investments in tech can be hard. you will often fix a problem that has not arrived yet, but you are strongly betting it’s just a matter of time. thus, without access to parallel worlds where the investment was not made, it’s hard to measure the product impact and prove the investment was sound, to begin with! regardless, it’s often a job that has to happen to deliver the ungraspable value and prevent clients from leaving in the (not too distant?) future! ability to track the impact of the work ======================================= product managers tend to say “if something is not measured, it doesn’t exist”. however, measuring stuff is often not easy and straightforward. how do you for example measure if a call was “fun” in skype? at the same time, being able to measure the impact is an important aspect of every goal. you want to focus your efforts on this! therefore, this aspect of value is not only about investing time into creating tracking dashboards and implementing tracking software. it’s also spending long hours defining what needs to be measured, how to do it, and documenting your thinking. speaking of the tracking dashboards! this should always be an element of any work you have ready before the update is live. you need to define the metrics you wish to measure, define the predicted product impact, etc - lots of work to be done (generating lots of value) in order to be on track with the product change’s impact from day one! growing stakeholders’ trust =========================== now, you can be the most brilliant product manager in the world, making the best calls and having the greatest ideas. however, if you do not have your stakeholders’ support and people in your company don’t understand your decision, your work will be misunderstood. thus, you need to dedicate time to ensure your efforts are transparent and it’s easy to trace your steps and support your decisions. it also means you have to make effort to keep your promises and be able to say “no” to stakeholders’ requests without antagonizing them. this means dedicating time for extra meetings, emails, documentation, and emotional energy to make sure you are surrounded by, if not friends, then at least allies. increasing the speed of development =================================== the product’s development speed will always be too slow! be ready for that. but this doesn’t mean that you shouldn’t try to make it quicker. while you, as the product manager, might not be able to directly contribute to fixing this issue, you have the power of deciding what lands in the backlog and sprint. thus, you can encourage your development team to investigate any process bottlenecks and propose solutions. it’s up to you if you accept the investment! it might work out and bear fruit later or the change might be hardly noticeable. improving morale and joy at work ================================ creating software and working in general is not about a bunch of slaves building piramids in agony. it’s great to have a sense of vision, mission and have joy along the way. thus, sometimes the best thing you can do for a product is to make sure people who shape it, enjoy what they are doing and each others’ company. skipping the obvious elements of empathy and being an honest man, a team that likes their work will act as junior product managers! they will contribute great ideas, warn you about possible dangers and, in general, will be passionate about their work. thus, you may find that often an outing together, or simply a coffee together during lunch will provide real long term benefits! great products will be done by great teams. you are the one that can ensure teams realize their greatness! complying with regulations ========================== maybe it’s obvious to mention, but sometimes you will need to work with legal and make the product worse to simply secure its future. i can ensure you no product manager was excited to introduce gdpr regulations or extend the cookie controls to the users. but it had to be done, otherwise the penalities would have killed many products. it might have heard the product, but ignoring it would harm product even more! finding the right perspective ============================= finally, any creative, brainstorming work, making swot exercises: when done in the right time, can unravel great new ideas and perspectives. if you get yourself deep into mostly sprint management work, you will soon become a project manager. a product manager has to have an eagle eye vision on the product and the market! though often day to day duties might get us away from strategic, long term thinking, this is a mistake. sometimes, skipping a meeting to do some research or thinking will really bring you more ideas and a better perspective! you just need to remember to do it and don’t treat it as a 3rd grade task. it will often be fruitless, i admit! but pursuing micromanaging and attending dozens of meetings where you might be needed for 5 minutes are the real product management times wasters! there you have it! here is my list of possible value aspects for product managers. did you like the list? do you agree with it? do you see any other value aspects i failed to mention? looking forward to a discussion on the comments! 📌do you know how to become a great product manager? visit to learn everything you need to land this $100,000 per annum job! use the code “mediumfollower” on checkout to receive a 25% discount on all my courses :) or, simply follow this link: 📌 would you like me to review your resume? or prepare you for a job interview? book a session here: 📌 follow me on: linkedin: youtube: twitter: instagram:\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def preprocess_markdown_for_embedding(markdown_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Cleans and strips markdown content, leaving behind only the semantic text\n",
    "    ready for an embedding model.\n",
    "\n",
    "    Args:\n",
    "        markdown_text: The raw markdown string.\n",
    "\n",
    "    Returns:\n",
    "        A cleaned text string.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- 1. Initial Cleaning and Normalization ---\n",
    "\n",
    "    # 1.1 REMOVE LINKS AND IMAGE TAGS: Remove the pattern [text](url) and ![text](url)\n",
    "    text = re.sub(r'\\!?\\[.*?\\]\\s*\\(.*?\\)', '', markdown_text, flags=re.DOTALL)\n",
    "    text = re.sub(r'Zoom image will be displayed', '', text)\n",
    "    text = re.sub(r'http[s]?://miro.medium.com/v2/resize:.*?\\.png', '', text)\n",
    "\n",
    "    # 1.2 Remove Extraneous Backslashes (e.g., escaping in \\- or \\.)\n",
    "    text = re.sub(r'\\\\-', '-', text)\n",
    "    text = re.sub(r'\\\\([`*_{}\\[\\]()#+.!])', r'\\1', text)\n",
    "    \n",
    "    # 1.3 Normalize Newlines: Convert multiple newlines/whitespace into a single space\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # --- 2. Markdown Structure Stripping ---\n",
    "\n",
    "    # 2.1 Remove Headings (Setext style: === or --- lines)\n",
    "    text = re.sub(r'\\n[=-]{2,}\\s*$', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # 2.2 Remove Blockquotes/Code Fences (Markers: > and ```)\n",
    "    text = re.sub(r'^\\s*>\\s?', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'```[a-zA-Z]*\\s*', ' ', text)\n",
    "    text = re.sub(r'`', ' ', text)\n",
    "    \n",
    "    # 2.3 Remove List Markers (e.g., 1. or - or *)\n",
    "    text = re.sub(r'^\\s*\\d+\\.\\s', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'^\\s*[\\-\\*]\\s', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # 2.4 Remove Emphasis Markers (e.g., **, *, __, _)\n",
    "    text = re.sub(r'(\\*\\*|__)', '', text) # Bold/Strong\n",
    "    text = re.sub(r'(\\*|_)', '', text)    # Italic/Emphasis\n",
    "\n",
    "    # 2.5 Remove remaining HTML tags (like '<hibernate-mapping>') which are often in code\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    \n",
    "    # --- 3. Final Text Polishing ---\n",
    "\n",
    "    # 3.1 Normalize Whitespace again: Collapse all multiple spaces into one\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    # 3.2 Lowercasing (Optional but recommended for many embedding models)\n",
    "    text = text.lower()\n",
    "\n",
    "    return text\n",
    "\n",
    "## preprocess sample\n",
    "# Process 15 free articles\n",
    "sample_free_texts = free_articles_df[\"full_article_text\"].iloc[:15]\n",
    "preprocessed_free_chunks = [preprocess_markdown_for_embedding(text) for text in sample_free_texts]\n",
    "\n",
    "# Process 15 paid articles\n",
    "sample_paid_texts = paid_articles_df[\"full_article_text\"].iloc[:15]\n",
    "preprocessed_paid_chunks = [preprocess_markdown_for_embedding(text) for text in sample_paid_texts]\n",
    "\n",
    "# Print preprocessed free articles\n",
    "print(\"Preprocessed Free Articles:\")\n",
    "for i, text in enumerate(preprocessed_free_chunks, 1):\n",
    "    print(f\"Free Article {i}:\")\n",
    "    print(text)\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# Print preprocessed paid articles\n",
    "print(\"Preprocessed Paid Articles:\")\n",
    "for i, text in enumerate(preprocessed_paid_chunks, 1):\n",
    "    print(f\"Paid Article {i}:\")\n",
    "    print(text)\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afc4c987",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45cb537d9ebf45dbb913a5f602c47e1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Moritz\\Desktop\\Medium-Mining\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Moritz\\.cache\\huggingface\\hub\\models--prdev--mini-gte. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e16e5e71b604c41b7acb234aadd007f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/214 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c72cca1e9974a0091c6e3edb767f6ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09a2c273ddc24235a946701be790e168",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1c0cbdc9a234751b570189cae9704be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/551 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce9f3aa3182c45e5b626964aa30f199e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/265M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3650a84f73c4704a790ba1ec075d23d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0fedd6dc8f445c789df7f631a9a6b01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aed3a3fc5904fec837a7acd50b27c46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72dbf4bf56784205a87e3b9d382d3fc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9db7ef3b286415ea85daa62ed5a5c28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = SentenceTransformer('prdev/mini-gte') # A good balance of speed and performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326e37fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c217e6d7bb0844909b42008297aa5f43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2871 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: torch.Size([22965, 768])\n"
     ]
    }
   ],
   "source": [
    "# Select all free articles\n",
    "articles_to_embed = free_articles_df\n",
    "\n",
    "# Preprocess the text for each article\n",
    "preprocessed_texts = articles_to_embed[\"full_article_text\"].apply(preprocess_markdown_for_embedding).tolist()\n",
    "\n",
    "# Encode the preprocessed texts to get embeddings\n",
    "embeddings_free = model.encode(preprocessed_texts, convert_to_tensor=True, device=device, batch_size=8, show_progress_bar=True)\n",
    "\n",
    "print(f\"Embeddings shape: {embeddings_free.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87183709",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6253abfaf5514d5fb4240a4bc56e143b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/611 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: torch.Size([12220, 768])\n"
     ]
    }
   ],
   "source": [
    "# Select all free articles\n",
    "articles_to_embed = paid_articles_df\n",
    "\n",
    "# Preprocess the text for each article\n",
    "preprocessed_texts = articles_to_embed[\"full_article_text\"].apply(preprocess_markdown_for_embedding).tolist()\n",
    "\n",
    "# Encode the preprocessed texts to get embeddings\n",
    "embeddings_paid = model.encode(preprocessed_texts, convert_to_tensor=True, device=device, batch_size=20, show_progress_bar=True)\n",
    "\n",
    "print(f\"Embeddings shape: {embeddings_paid.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7391da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the embeddings to a file\n",
    "torch.save(embeddings_free.cpu(), 'embeddings_free.pt')\n",
    "torch.save(embeddings_paid.cpu(), 'embeddings_paid.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a91c82d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([22965, 768]), torch.Size([12220, 768]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_free = torch.load('embeddings_free.pt').to(device)\n",
    "embeddings_paid = torch.load('embeddings_paid.pt').to(device)\n",
    "embeddings_free.shape, embeddings_paid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc2cabf",
   "metadata": {},
   "source": [
    "### Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfae229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total embeddings: 35185, Labels: 35185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Moritz\\AppData\\Local\\Temp\\ipykernel_3304\\3599275938.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  free_articles_df['title'] = free_articles_df['title'].str.replace('\\n', ' ', regex=False).str.replace('\\t', ' ', regex=False).str.strip().fillna('No Title')\n",
      "C:\\Users\\Moritz\\AppData\\Local\\Temp\\ipykernel_3304\\3599275938.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  free_articles_df['title'] = free_articles_df['title'].str.replace(r'\\s+', ' ', regex=True).str.strip().fillna('No Title')\n",
      "C:\\Users\\Moritz\\AppData\\Local\\Temp\\ipykernel_3304\\3599275938.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  paid_articles_df['title'] = paid_articles_df['title'].str.replace(r'\\s+', ' ', regex=True).str.strip().fillna('No Title')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Combine embeddings from free and paid articles\n",
    "embeddings_combined = torch.cat([embeddings_free, embeddings_paid], dim=0).cpu().numpy()\n",
    "\n",
    "# Create labels: 'free' for free articles, 'paid' for paid articles\n",
    "num_free = embeddings_free.shape[0]\n",
    "num_paid = embeddings_paid.shape[0]\n",
    "labels = ['free'] * num_free + ['paid'] * num_paid\n",
    "\n",
    "print(f\"Total embeddings: {embeddings_combined.shape[0]}, Labels: {len(labels)}\")\n",
    "\n",
    "# replace any newlines or tabs in titles\n",
    "free_articles_df['title'] = free_articles_df['title'].str.replace('\\n', ' ', regex=False).str.replace('\\t', ' ', regex=False).str.strip().fillna('No Title')\n",
    "\n",
    "# replace even more aggressively\n",
    "free_articles_df['title'] = free_articles_df['title'].str.replace(r'\\s+', ' ', regex=True).str.strip().fillna('No Title')\n",
    "paid_articles_df['title'] = paid_articles_df['title'].str.replace(r'\\s+', ' ', regex=True).str.strip().fillna('No Title')\n",
    "\n",
    "# Create titles list\n",
    "titles = free_articles_df['title'].tolist() + paid_articles_df['title'].tolist()\n",
    "\n",
    "os.makedirs('logs', exist_ok=True)\n",
    "\n",
    "# Write metadata to TSV file with label and title\n",
    "with open('logs/metadata.tsv', 'w', encoding='utf-8') as f:\n",
    "    f.write('label\\ttitle\\n')\n",
    "    for label, title in zip(labels, titles):\n",
    "        f.write(f'{label}\\t{title}\\n')\n",
    "\n",
    "# Create logs directory if it doesn't exist\n",
    "os.makedirs('logs', exist_ok=True)\n",
    "\n",
    "# Create TensorFlow variable for embeddings and save checkpoint\n",
    "embeddings_var = tf.Variable(embeddings_combined, name='embeddings')\n",
    "checkpoint = tf.train.Checkpoint(embeddings=embeddings_var)\n",
    "checkpoint.save(os.path.join('logs', 'embeddings.ckpt'))\n",
    "\n",
    "# Configure the projector\n",
    "config = projector.ProjectorConfig()\n",
    "embedding_config = config.embeddings.add()\n",
    "embedding_config.tensor_name = 'embeddings/.ATTRIBUTES/VARIABLE_VALUE'\n",
    "embedding_config.metadata_path = 'metadata.tsv'\n",
    "projector.visualize_embeddings('logs', config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medium-mining-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
