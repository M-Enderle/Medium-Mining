{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Medium-Mining Project","text":"A comprehensive tool for scraping and analyzing Medium articles"},{"location":"#overview","title":"Overview","text":"<p>Medium-Mining is a sophisticated Python-based toolset designed for extracting, storing, and analyzing content from Medium.com. This project offers a powerful way to collect data from Medium's vast repository of articles, comments, and associated metadata for research, analysis, or archival purposes.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Efficient Sitemap Crawling: Asynchronously retrieve article URLs from Medium's sitemaps</li> <li>Comprehensive Article Scraping: Extract detailed content including text, author information, claps, comments, and more</li> <li>Robust Database Storage: Store all data in a structured DuckDB database for efficient querying</li> <li>Configurable Execution: Control execution parameters like concurrency, rate limiting, and browser visibility</li> <li>Detailed Logging: Track progress and diagnose issues with comprehensive logging</li> <li>Support for All Article Types: Handle both member-only and public articles</li> <li>Performance Metrics: Monitor scraping performance with built-in metrics</li> </ul>"},{"location":"#project-components","title":"Project Components","text":"<p>The project consists of two main components:</p> <ol> <li>Sitemap Scraper: </li> <li>Crawls Medium's XML sitemaps</li> <li>Collects and stores article URLs</li> <li> <p>Efficiently handles large sitemaps with batch processing</p> </li> <li> <p>Article Scraper:</p> </li> <li>Extracts complete article content and metadata</li> <li>Captures comments and user interactions</li> <li>Uses browser automation with Playwright for rendering JavaScript content</li> <li>Manages browser sessions for optimal performance</li> </ol>"},{"location":"#getting-started","title":"Getting Started","text":"<p>To get started with Medium-Mining, check out the Installation guide, then proceed to the Quick Start section for basic usage instructions.</p>"},{"location":"#use-cases","title":"Use Cases","text":"<p>Medium-Mining is designed for:</p> <ul> <li>Content Analysis: Study writing styles, trends, and popular topics</li> <li>Sentiment Analysis: Analyze article content and comment sentiment</li> <li>Research: Collect data for academic or market research</li> <li>Content Aggregation: Gather articles for creating curated collections</li> <li>Temporal Analysis: Track how content changes over time</li> </ul>"},{"location":"#sample-results","title":"Sample Results","text":"<p>The following visualization shows the distribution of article publication dates from a sample scrape:</p> <p> </p> Distribution of Medium articles by publication date"},{"location":"#important-notice","title":"Important Notice","text":"<p>This tool is provided for research and educational purposes only. Always adhere to Medium's Terms of Service and robots.txt when using this software. Be respectful of Medium's servers by using reasonable rate limiting.</p>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the MIT License - see the LICENSE file for details.</p>"},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes to the Medium-Mining project will be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"changelog/#unreleased","title":"[Unreleased]","text":""},{"location":"changelog/#added","title":"Added","text":"<ul> <li>Comprehensive MkDocs documentation with Material theme</li> <li>Detailed architecture documentation</li> <li>API reference documentation</li> <li>Expanded installation and usage guides</li> <li>FAQ and troubleshooting sections</li> </ul>"},{"location":"changelog/#010-2024-04-01","title":"[0.1.0] - 2024-04-01","text":""},{"location":"changelog/#added_1","title":"Added","text":"<ul> <li>Initial release of Medium-Mining</li> <li>Sitemap scraper for extracting article URLs</li> <li>Article scraper with Playwright for content extraction</li> <li>DuckDB database for storage</li> <li>Basic Jupyter notebook for analysis</li> <li>Command-line interface for both scrapers</li> <li>Screenshot capture for verification</li> <li>Support for extracting comments</li> <li>Configurable execution parameters</li> <li>Logging and metrics tracking</li> <li>README with project overview and basic usage instructions</li> </ul>"},{"location":"getting-started/","title":"Getting Started","text":""},{"location":"getting-started/#installation","title":"Installation","text":"<ol> <li> <p>Clone the repository:    <pre><code>git clone https://github.com/username/Medium-Mining.git\ncd Medium-Mining\n</code></pre></p> </li> <li> <p>Install dependencies using Poetry:    <pre><code>poetry install\n</code></pre></p> </li> <li> <p>Initialize the database:    <pre><code>poetry run python -m database.database\n</code></pre></p> </li> </ol>"},{"location":"getting-started/#usage","title":"Usage","text":""},{"location":"getting-started/#scraping-medium-sitemaps","title":"Scraping Medium Sitemaps","text":"<p>To scrape Medium sitemaps for article URLs:</p> <pre><code>poetry run python -m scraper sitemap [--timeout 0.05] [--verbose]\n</code></pre> <p>Options: - <code>--timeout</code>: Time to wait between requests (default: 0.05 seconds) - <code>--verbose</code>: Enable verbose logging</p>"},{"location":"getting-started/#scraping-medium-articles","title":"Scraping Medium Articles","text":"<p>To scrape article content and metadata:</p> <pre><code>poetry run python -m scraper article [--url-count 50] [--headless true] [--workers 1]\n</code></pre> <p>Options: - <code>--url-count</code>: Number of article URLs to scrape (default: 50) - <code>--headless</code>: Run browser in headless mode (default: true) - <code>--workers</code>: Number of worker processes (default: 1)</p>"},{"location":"getting-started/#development","title":"Development","text":"<p>For code formatting:</p> <pre><code>poetry run black src/ &amp;&amp; poetry run isort src/\n</code></pre>"},{"location":"architecture/database-schema/","title":"Database Schema","text":"<p>Medium-Mining uses a DuckDB database with an SQLAlchemy ORM layer to store and manage data. This document provides a detailed overview of the database schema and relationships.</p>"},{"location":"architecture/database-schema/#database-configuration","title":"Database Configuration","text":"<p>The database is configured in the <code>src/database/database.py</code> file:</p> <pre><code>DATABASE_URL = \"duckdb:///medium_articles.duckdb\"  # Persistent storage\nBase = declarative_base()\nengine = create_engine(DATABASE_URL, echo=False)\nSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\n</code></pre>"},{"location":"architecture/database-schema/#entity-relationship-diagram","title":"Entity Relationship Diagram","text":"<p>The following diagram illustrates the relationships between the database tables:</p> <pre><code>erDiagram\n    Sitemap ||--o{ URL : contains\n    URL ||--o| MediumArticle : describes\n    MediumArticle ||--o{ Comment : has\n\n    Sitemap {\n        int id PK\n        string sitemap_url\n        int articles_count\n    }\n\n    URL {\n        int id PK\n        string url\n        string last_modified\n        string change_freq\n        float priority\n        int sitemap_id FK\n        datetime last_crawled\n        string crawl_status\n    }\n\n    MediumArticle {\n        int id PK\n        int url_id FK\n        string title\n        string author_name\n        string date_published\n        string date_modified\n        text description\n        string publisher\n        string is_free\n        string claps\n        int comments_count\n        string tags\n        text full_article_text\n    }\n\n    Comment {\n        int id PK\n        int article_id FK\n        string username\n        text text\n        text full_text\n        string claps\n        boolean references_article\n    }</code></pre>"},{"location":"architecture/database-schema/#tables-and-models","title":"Tables and Models","text":""},{"location":"architecture/database-schema/#sitemap-model","title":"<code>Sitemap</code> Model","text":"<p>The <code>Sitemap</code> model represents a Medium sitemap that has been processed:</p> <pre><code>class Sitemap(Base):\n    __tablename__ = \"sitemaps\"\n    sitemap_id_seq = Sequence(\"sitemap_id_seq\")\n\n    id = Column(\n        \"id\",\n        Integer,\n        sitemap_id_seq,\n        server_default=sitemap_id_seq.next_value(),\n        primary_key=True,\n    )\n    sitemap_url = Column(String(255), unique=True, nullable=False)\n    articles_count = Column(Integer)\n</code></pre> Field Type Description <code>id</code> Integer Primary key for the sitemap <code>sitemap_url</code> String URL of the sitemap (unique) <code>articles_count</code> Integer Number of articles in the sitemap"},{"location":"architecture/database-schema/#url-model","title":"<code>URL</code> Model","text":"<p>The <code>URL</code> model represents an article URL extracted from a sitemap:</p> <pre><code>class URL(Base):\n    __tablename__ = \"urls\"\n    url_id_seq = Sequence(\"url_id_seq\")\n\n    id = Column(\n        \"id\",\n        Integer,\n        url_id_seq,\n        server_default=url_id_seq.next_value(),\n        primary_key=True,\n    )\n\n    url = Column(String(255), unique=True, nullable=False)\n    last_modified = Column(String(50))\n    change_freq = Column(String(50))\n    priority = Column(Float)\n    sitemap_id = Column(Integer, ForeignKey(\"sitemaps.id\"))\n    last_crawled = Column(DateTime, nullable=True)\n    crawl_status = Column(String(50), nullable=True)\n</code></pre> Field Type Description <code>id</code> Integer Primary key for the URL <code>url</code> String The article URL (unique) <code>last_modified</code> String Last modification date from sitemap <code>change_freq</code> String Change frequency from sitemap (e.g., \"daily\", \"monthly\") <code>priority</code> Float Priority from sitemap (0.0-1.0) <code>sitemap_id</code> Integer Foreign key to the Sitemap table <code>last_crawled</code> DateTime When the URL was last crawled <code>crawl_status</code> String Status of the last crawl (e.g., \"Successful\", \"Failed\")"},{"location":"architecture/database-schema/#mediumarticle-model","title":"<code>MediumArticle</code> Model","text":"<p>The <code>MediumArticle</code> model represents a Medium article with its content and metadata:</p> <pre><code>class MediumArticle(Base):\n    __tablename__ = \"medium_articles\"\n    article_id_seq = Sequence(\"article_id_seq\")\n\n    id = Column(\n        \"id\",\n        Integer,\n        article_id_seq,\n        server_default=article_id_seq.next_value(),\n        primary_key=True,\n    )\n    url_id = Column(Integer, ForeignKey(\"urls.id\"))\n    title = Column(String(255), nullable=False)\n    author_name = Column(String(100))\n    date_published = Column(String(50))\n    date_modified = Column(String(50))\n    description = Column(Text)\n    publisher = Column(String(100))\n    is_free = Column(String(50))\n    claps = Column(String(20))\n    comments_count = Column(Integer)\n    tags = Column(String(255))\n    full_article_text = Column(Text)\n</code></pre> Field Type Description <code>id</code> Integer Primary key for the article <code>url_id</code> Integer Foreign key to the URL table <code>title</code> String Article title <code>author_name</code> String Name of the article author <code>date_published</code> String Original publication date <code>date_modified</code> String Last modification date <code>description</code> Text Article description or subtitle <code>publisher</code> String Publisher name (e.g., a Medium publication) <code>is_free</code> String Access type (e.g., \"Public\", \"Member-Only\", \"Paid\") <code>claps</code> String Number of claps (likes) <code>comments_count</code> Integer Number of comments <code>tags</code> String Comma-separated list of article tags <code>full_article_text</code> Text Complete article content"},{"location":"architecture/database-schema/#comment-model","title":"<code>Comment</code> Model","text":"<p>The <code>Comment</code> model represents a comment on a Medium article:</p> <pre><code>class Comment(Base):\n    __tablename__ = \"comments\"\n    comment_id_seq = Sequence(\"comment_id_seq\")\n\n    id = Column(\n        \"id\",\n        Integer,\n        comment_id_seq,\n        server_default=comment_id_seq.next_value(),\n        primary_key=True,\n    )\n    article_id = Column(Integer, ForeignKey(\"medium_articles.id\"))\n    username = Column(String(100))\n    text = Column(Text)\n    full_text = Column(Text)\n    claps = Column(String(20))\n    references_article = Column(Boolean)\n</code></pre> Field Type Description <code>id</code> Integer Primary key for the comment <code>article_id</code> Integer Foreign key to the MediumArticle table <code>username</code> String Username of the commenter <code>text</code> Text Main text content of the comment <code>full_text</code> Text Complete comment with any formatting or quotes <code>claps</code> String Number of claps (likes) on the comment <code>references_article</code> Boolean Whether the comment quotes the article"},{"location":"architecture/database-schema/#database-initialization","title":"Database Initialization","text":"<p>The database and tables are created using the <code>setup_database()</code> function in <code>database.py</code>:</p> <pre><code>def setup_database():\n    \"\"\"Create the database and tables if they don't exist.\"\"\"\n    try:\n        Base.metadata.create_all(engine)\n        print(\"Database and tables created successfully.\")\n    except Exception as e:\n        print(f\"Error creating database: {e}\")\n</code></pre> <p>This function is called when the database module is run directly:</p> <pre><code>if __name__ == \"__main__\":\n    # Set up the database\n    setup_database()\n</code></pre>"},{"location":"architecture/database-schema/#session-management","title":"Session Management","text":"<p>Sessions for database operations are managed using the <code>get_session()</code> function:</p> <pre><code>def get_session():\n    \"\"\"Get a SQLAlchemy session for database operations.\"\"\"\n    return SessionLocal()\n</code></pre> <p>This function is used throughout the application to obtain a session for database operations.</p>"},{"location":"architecture/database-schema/#query-examples","title":"Query Examples","text":""},{"location":"architecture/database-schema/#fetch-random-urls-for-processing","title":"Fetch Random URLs for Processing","text":"<pre><code>def fetch_random_urls(session, count=None) -&gt; List[Tuple[int, str]]:\n    \"\"\"Fetch random URLs from the database.\"\"\"\n    query = session.query(URL.id, URL.url).filter(URL.last_crawled == None)\n\n    # Use the provided count, or fall back to the global URLS_TO_FETCH\n    limit = count if count is not None else URLS_TO_FETCH\n    query = query.order_by(func.random()).limit(limit)\n\n    logger.debug(f\"Fetching {limit} random URLs from database\")\n    return query.all()\n</code></pre>"},{"location":"architecture/database-schema/#update-url-status-after-crawling","title":"Update URL Status After Crawling","text":"<pre><code>def update_url_status(session, url_id: int, success: bool):\n    \"\"\"Update the URL's last_crawled timestamp and crawl_status.\"\"\"\n    try:\n        url = session.query(URL).filter(URL.id == url_id).first()\n        if url:\n            url.last_crawled = datetime.now()\n            url.crawl_status = \"Successful\" if success else \"Failed\"\n            session.commit()\n            logger.debug(f\"Updated URL {url_id} status: {success}\")\n    except Exception as e:\n        session.rollback()\n        logger.error(f\"DB error for URL {url_id}: {e}\")\n</code></pre>"},{"location":"architecture/database-schema/#persist-article-data","title":"Persist Article Data","text":"<pre><code>def persist_article_data(session, url_id: int, metadata: Dict[str, Any]) -&gt; bool:\n    \"\"\"Save article metadata and comments to the database.\"\"\"\n    try:\n        # Code to save article data\n        # ...\n        session.commit()\n        return True\n    except Exception as e:\n        session.rollback()\n        logger.error(f\"Failed to save article: {e}\")\n        return False\n</code></pre>"},{"location":"architecture/database-schema/#database-performance-considerations","title":"Database Performance Considerations","text":"<ul> <li>Batch Operations: For bulk data insertion, use SQLAlchemy's <code>bulk_save_objects</code> method to improve performance</li> <li>Indexing: Key columns like <code>url</code> in the <code>URL</code> table are indexed for faster queries</li> <li>Connection Pooling: SQLAlchemy's connection pooling is used to manage database connections efficiently</li> <li>Session Scope: Sessions are created and closed for each operation to avoid resource leaks</li> <li>Transactions: All operations use transactions to ensure data consistency</li> <li>Error Handling: All database operations include error handling and rollback in case of exceptions</li> </ul>"},{"location":"architecture/overview/","title":"Architecture Overview","text":"<p>Medium-Mining follows a modular architecture designed for extensibility, maintainability, and performance. This document provides a high-level overview of the system's architecture, its major components, and how they interact.</p>"},{"location":"architecture/overview/#high-level-architecture","title":"High-Level Architecture","text":"<p>The Medium-Mining system is composed of several major components:</p> <pre><code>graph TD\n    A[Command Line Interface] --&gt; B[Sitemap Scraper]\n    A --&gt; C[Article Scraper]\n    B --&gt; D[Database]\n    C --&gt; D\n    D --&gt; E[Analysis Tools]\n\n    classDef primary fill:#4f80e1,stroke:#333,stroke-width:2px,color:white\n    classDef secondary fill:#5ea7b1,stroke:#333,stroke-width:2px,color:white\n    classDef storage fill:#597d75,stroke:#333,stroke-width:2px,color:white\n    classDef analysis fill:#d68e6f,stroke:#333,stroke-width:2px,color:white\n\n    class A primary\n    class B,C secondary\n    class D storage\n    class E analysis</code></pre>"},{"location":"architecture/overview/#component-descriptions","title":"Component Descriptions","text":""},{"location":"architecture/overview/#command-line-interface","title":"Command Line Interface","text":"<p>The CLI serves as the entry point for the application, providing a user-friendly way to interact with the scrapers:</p> <ul> <li>Implemented in <code>src/scraper/__main__.py</code></li> <li>Uses Python's <code>argparse</code> library for command-line argument parsing</li> <li>Provides subcommands for different operations:</li> <li><code>sitemap</code>: Run the sitemap scraper</li> <li><code>article</code>: Run the article scraper</li> <li>Each subcommand has its own set of parameters for customization</li> </ul>"},{"location":"architecture/overview/#sitemap-scraper","title":"Sitemap Scraper","text":"<p>The Sitemap Scraper is responsible for discovering Medium article URLs by crawling Medium's XML sitemaps:</p> <ul> <li>Implemented in <code>src/scraper/scrape_sitemaps.py</code></li> <li>Uses Python's standard libraries to parse XML</li> <li>Features:</li> <li>Retrieves the master sitemap index</li> <li>Processes individual sitemaps</li> <li>Extracts article URLs and metadata</li> <li>Stores results in the database</li> <li>Handles rate limiting and error recovery</li> </ul>"},{"location":"architecture/overview/#article-scraper","title":"Article Scraper","text":"<p>The Article Scraper extracts content and metadata from individual Medium articles:</p> <ul> <li>Implemented in <code>src/scraper/scrape_articles.py</code> and <code>src/scraper/medium_helpers.py</code></li> <li>Uses Playwright for browser automation</li> <li>Features:</li> <li>Multi-threaded processing for improved performance</li> <li>Extracts article text, metadata, and comments</li> <li>Detects member-only vs. public articles</li> <li>Captures screenshots for verification</li> <li>Provides detailed performance metrics</li> </ul>"},{"location":"architecture/overview/#database-layer","title":"Database Layer","text":"<p>The Database layer provides persistent storage and retrieval capabilities:</p> <ul> <li>Implemented in <code>src/database/database.py</code></li> <li>Uses SQLAlchemy ORM with DuckDB as the backend</li> <li>Features:</li> <li>Model definitions for sitemaps, URLs, articles, and comments</li> <li>Session management</li> <li>Database initialization and migration</li> <li>ACID transactions for data integrity</li> </ul>"},{"location":"architecture/overview/#analysis-tools","title":"Analysis Tools","text":"<p>The Analysis tools provide capabilities for extracting insights from the collected data:</p> <ul> <li>Implemented in Jupyter notebooks in the <code>notebooks/</code> directory</li> <li>Uses pandas, matplotlib, seaborn, and other data science libraries</li> <li>Features:</li> <li>Descriptive statistics</li> <li>Visualization of trends and patterns</li> <li>Natural language processing for content analysis</li> <li>Export capabilities for external tools</li> </ul>"},{"location":"architecture/overview/#data-flow","title":"Data Flow","text":"<p>The following diagram illustrates the flow of data through the Medium-Mining system:</p> <pre><code>sequenceDiagram\n    participant User\n    participant CLI\n    participant SitemapScraper\n    participant ArticleScraper\n    participant Database\n    participant AnalysisTools\n\n    User-&gt;&gt;CLI: Run sitemap scraper\n    CLI-&gt;&gt;SitemapScraper: Invoke with parameters\n    SitemapScraper-&gt;&gt;Medium: Request master sitemap\n    Medium-&gt;&gt;SitemapScraper: Return sitemap index\n    SitemapScraper-&gt;&gt;Medium: Request individual sitemaps\n    Medium-&gt;&gt;SitemapScraper: Return sitemap data\n    SitemapScraper-&gt;&gt;Database: Store sitemap data\n    SitemapScraper-&gt;&gt;Database: Store URL data\n\n    User-&gt;&gt;CLI: Run article scraper\n    CLI-&gt;&gt;ArticleScraper: Invoke with parameters\n    ArticleScraper-&gt;&gt;Database: Request unprocessed URLs\n    Database-&gt;&gt;ArticleScraper: Return URLs\n    ArticleScraper-&gt;&gt;Medium: Request articles\n    Medium-&gt;&gt;ArticleScraper: Return article content\n    ArticleScraper-&gt;&gt;Database: Store article data\n    ArticleScraper-&gt;&gt;Database: Store comment data\n\n    User-&gt;&gt;AnalysisTools: Run analysis notebook\n    AnalysisTools-&gt;&gt;Database: Query data\n    Database-&gt;&gt;AnalysisTools: Return query results\n    AnalysisTools-&gt;&gt;User: Display visualizations and insights</code></pre>"},{"location":"architecture/overview/#technology-stack","title":"Technology Stack","text":"<p>Medium-Mining leverages several technologies to accomplish its goals:</p> Component Technologies Scraping Python, requests, Playwright, asyncio Database SQLAlchemy, DuckDB Analysis pandas, matplotlib, seaborn, scipy Development Poetry, Black, isort Documentation MkDocs, Material for MkDocs"},{"location":"architecture/overview/#design-principles","title":"Design Principles","text":"<p>The Medium-Mining architecture adheres to several key design principles:</p>"},{"location":"architecture/overview/#modularity","title":"Modularity","text":"<p>The system is divided into separate components with well-defined responsibilities, making it easier to maintain and extend:</p> <ul> <li>Scraper components are independent of each other</li> <li>Database models are defined separately from scraping logic</li> <li>Analysis tools are separate from data collection</li> </ul>"},{"location":"architecture/overview/#configurability","title":"Configurability","text":"<p>Each component can be configured to adapt to different requirements:</p> <ul> <li>Command-line arguments for controlling scraper behavior</li> <li>Database connection parameters for different storage backends</li> <li>Analysis parameters for different types of insights</li> </ul>"},{"location":"architecture/overview/#scalability","title":"Scalability","text":"<p>The architecture is designed to handle large volumes of data efficiently:</p> <ul> <li>Asynchronous processing in the sitemap scraper</li> <li>Multi-threaded processing in the article scraper</li> <li>Batch operations for database efficiency</li> <li>DuckDB for efficient analytical queries</li> </ul>"},{"location":"architecture/overview/#resilience","title":"Resilience","text":"<p>The system includes mechanisms for handling errors and recovering from failures:</p> <ul> <li>Retry logic for network operations</li> <li>Graceful shutdown for interruptions</li> <li>Error handling at multiple levels</li> <li>Transaction management for data integrity</li> </ul>"},{"location":"architecture/overview/#extensibility","title":"Extensibility","text":"<p>Medium-Mining's architecture is designed to be extensible. Here are some ways to extend the system:</p>"},{"location":"architecture/overview/#adding-new-scraper-types","title":"Adding New Scraper Types","text":"<ol> <li>Create a new Python module in the <code>src/scraper/</code> directory</li> <li>Implement the scraper logic</li> <li>Add a new subcommand to the CLI in <code>__main__.py</code></li> </ol>"},{"location":"architecture/overview/#enhancing-data-extraction","title":"Enhancing Data Extraction","text":"<ol> <li>Add new extraction functions to <code>medium_helpers.py</code></li> <li>Update the article processing logic in <code>scrape_articles.py</code></li> <li>Modify the database models to store the new data</li> </ol>"},{"location":"architecture/overview/#adding-new-analysis-techniques","title":"Adding New Analysis Techniques","text":"<ol> <li>Create a new Jupyter notebook in the <code>notebooks/</code> directory</li> <li>Implement the analysis logic using the database models</li> <li>Add visualizations and insights extraction</li> </ol>"},{"location":"architecture/overview/#performance-considerations","title":"Performance Considerations","text":"<p>The architecture includes several optimizations for performance:</p> <ul> <li>Asynchronous I/O: The sitemap scraper uses <code>asyncio</code> for concurrent network operations</li> <li>Multi-threading: The article scraper uses multiple threads for parallel processing</li> <li>Batch Operations: Database operations are performed in batches when possible</li> <li>Connection Pooling: SQLAlchemy's connection pooling reduces connection overhead</li> <li>Analytical Database: DuckDB is optimized for analytical queries on large datasets</li> </ul>"},{"location":"architecture/overview/#security-considerations","title":"Security Considerations","text":"<p>While Medium-Mining is a research tool, it includes several security considerations:</p> <ul> <li>Rate Limiting: Configurable delays between requests to avoid overwhelming Medium's servers</li> <li>Isolated Browser Contexts: Each browser instance runs in an isolated context</li> <li>Error Handling: Comprehensive error handling to prevent data leakage</li> <li>Input Validation: Command-line arguments are validated before use</li> </ul>"},{"location":"architecture/overview/#next-steps","title":"Next Steps","text":"<ul> <li>Learn more about the Project Structure</li> <li>Explore the Database Schema</li> <li>Understand the Data Flow in more detail</li> </ul>"},{"location":"architecture/project-structure/","title":"Project Structure","text":"<p>This document outlines the organization of the Medium-Mining project, explaining the purpose of each directory and key files.</p>"},{"location":"architecture/project-structure/#directory-structure","title":"Directory Structure","text":"<pre><code>Medium-Mining/\n\u251c\u2500\u2500 docs/                  # Documentation files\n\u251c\u2500\u2500 notebooks/             # Jupyter notebooks for analysis\n\u251c\u2500\u2500 screenshots/           # Screenshots taken during scraping (created at runtime)\n\u251c\u2500\u2500 src/                   # Source code\n\u2502   \u251c\u2500\u2500 database/          # Database models and utilities\n\u2502   \u2502   \u2514\u2500\u2500 database.py    # Database configuration and models\n\u2502   \u2514\u2500\u2500 scraper/           # Web scraping modules\n\u2502       \u251c\u2500\u2500 __main__.py    # Command line interface entry point\n\u2502       \u251c\u2500\u2500 medium_helpers.py  # Helper functions for scraping Medium\n\u2502       \u251c\u2500\u2500 scrape_articles.py # Article scraper implementation\n\u2502       \u2514\u2500\u2500 scrape_sitemaps.py # Sitemap scraper implementation\n\u251c\u2500\u2500 .gitignore             # Git ignore file\n\u251c\u2500\u2500 CLAUDE.md              # Project guidelines for Claude AI\n\u251c\u2500\u2500 LICENSE                # Project license\n\u251c\u2500\u2500 README.md              # Project overview and documentation\n\u251c\u2500\u2500 medium_articles.duckdb # Database file (created at runtime)\n\u251c\u2500\u2500 mkdocs.yml             # MkDocs configuration\n\u251c\u2500\u2500 poetry.lock            # Poetry lock file with exact dependencies\n\u2514\u2500\u2500 pyproject.toml         # Poetry project configuration\n</code></pre>"},{"location":"architecture/project-structure/#key-components","title":"Key Components","text":""},{"location":"architecture/project-structure/#source-code-src","title":"Source Code (<code>src/</code>)","text":"<p>The <code>src/</code> directory contains all the Python source code for the project, organized into modules:</p>"},{"location":"architecture/project-structure/#database-module","title":"<code>database/</code> Module","text":"<ul> <li>Purpose: Defines database models and provides utilities for database operations</li> <li>Key Files:</li> <li><code>database.py</code>: Contains SQLAlchemy models, connection setup, and utility functions</li> </ul>"},{"location":"architecture/project-structure/#scraper-module","title":"<code>scraper/</code> Module","text":"<ul> <li>Purpose: Implements web scraping functionality for Medium</li> <li>Key Files:</li> <li><code>__main__.py</code>: Entry point for the command-line interface</li> <li><code>medium_helpers.py</code>: Helper functions specific to Medium website structure</li> <li><code>scrape_articles.py</code>: Implementation of the article scraper</li> <li><code>scrape_sitemaps.py</code>: Implementation of the sitemap scraper</li> </ul>"},{"location":"architecture/project-structure/#analysis-notebooks-notebooks","title":"Analysis Notebooks (<code>notebooks/</code>)","text":"<ul> <li>Purpose: Contains Jupyter notebooks for analyzing the scraped data</li> <li>Key Files:</li> <li><code>analyze_scraping_results.ipynb</code>: Main analysis notebook with examples</li> </ul>"},{"location":"architecture/project-structure/#documentation-docs","title":"Documentation (<code>docs/</code>)","text":"<ul> <li>Purpose: Contains comprehensive documentation for the project</li> <li>Structure:</li> <li>Organized into sections by topic (getting started, modules, architecture, etc.)</li> <li>Uses MkDocs with Material theme for rendering</li> </ul>"},{"location":"architecture/project-structure/#configuration-files","title":"Configuration Files","text":"<ul> <li>pyproject.toml: Project metadata and dependencies managed by Poetry</li> <li>poetry.lock: Locked dependencies for reproducible environments</li> <li>mkdocs.yml: Configuration for the documentation site</li> </ul>"},{"location":"architecture/project-structure/#runtime-generated-files","title":"Runtime Generated Files","text":"<ul> <li>medium_articles.duckdb: DuckDB database file created at runtime</li> <li>screenshots/: Directory containing screenshots captured during article scraping</li> </ul>"},{"location":"architecture/project-structure/#module-relationships","title":"Module Relationships","text":"<p>The following diagram illustrates the relationships between the main modules:</p> <pre><code>graph TD\n    A[CLI - __main__.py] --&gt; B[Sitemap Scraper]\n    A --&gt; C[Article Scraper]\n    C --&gt; D[Medium Helpers]\n    B --&gt; E[Database]\n    C --&gt; E\n    D --&gt; E\n    E --&gt; F[Analysis Notebooks]</code></pre>"},{"location":"architecture/project-structure/#dependencies","title":"Dependencies","text":"<p>The project's dependencies are managed using Poetry and defined in <code>pyproject.toml</code>:</p> <ul> <li>Core Dependencies:</li> <li><code>sqlalchemy</code>: ORM for database operations</li> <li><code>requests</code>: HTTP client for sitemap scraping</li> <li><code>playwright</code>: Browser automation for article scraping</li> <li> <p><code>duckdb</code> &amp; <code>duckdb-engine</code>: Database and SQLAlchemy dialect</p> </li> <li> <p>Analysis Dependencies:</p> </li> <li><code>pandas</code>: Data manipulation</li> <li><code>seaborn</code>: Data visualization</li> <li> <p><code>scipy</code>: Scientific computing</p> </li> <li> <p>Development Dependencies:</p> </li> <li><code>black</code>: Code formatting</li> <li><code>isort</code>: Import sorting</li> <li> <p><code>ipykernel</code>: Jupyter kernel for notebooks</p> </li> <li> <p>Documentation Dependencies:</p> </li> <li><code>mkdocs</code>: Documentation generator</li> <li><code>mkdocs-material</code>: Material theme for MkDocs</li> <li><code>pymdown-extensions</code>: Extended Markdown features</li> <li><code>mkdocstrings</code>: API documentation from docstrings</li> </ul>"},{"location":"architecture/project-structure/#entry-points","title":"Entry Points","text":"<p>The project provides the following entry points:</p>"},{"location":"architecture/project-structure/#command-line-interface","title":"Command Line Interface","text":"<pre><code># Run sitemap scraper\npython -m scraper sitemap [options]\n\n# Run article scraper\npython -m scraper article [options]\n</code></pre>"},{"location":"architecture/project-structure/#database-initialization","title":"Database Initialization","text":"<pre><code># Initialize the database\npython -m database.database\n</code></pre>"},{"location":"architecture/project-structure/#documentation","title":"Documentation","text":"<pre><code># Serve documentation site locally\nmkdocs serve\n\n# Build static documentation site\nmkdocs build\n</code></pre>"},{"location":"architecture/project-structure/#customization-points","title":"Customization Points","text":"<p>The project architecture includes several points for customization:</p> <ul> <li>Database Models: Add or modify models in <code>database.py</code></li> <li>Scraping Logic: Extend scraping capabilities in <code>medium_helpers.py</code></li> <li>CLI Options: Add new command-line options in <code>__main__.py</code></li> <li>Analysis: Create new notebooks in the <code>notebooks/</code> directory</li> </ul>"},{"location":"architecture/project-structure/#package-organization","title":"Package Organization","text":"<p>The project uses Poetry's package management with explicit package declarations:</p> <pre><code>[tool.poetry]\npackages = [\n    { include = \"scraper\", from = \"src\" },\n    { include = \"database\", from = \"src\" }\n]\n</code></pre> <p>This ensures that when the project is installed, the modules are available as:</p> <pre><code>from scraper import scrape_articles\nfrom database import database\n</code></pre>"},{"location":"architecture/project-structure/#development-workflow","title":"Development Workflow","text":"<p>The typical development workflow involves:</p> <ol> <li>Making changes to the source code</li> <li>Formatting with Black and isort:    <pre><code>poetry run black src/ &amp;&amp; poetry run isort src/\n</code></pre></li> <li>Testing changes by running the appropriate command</li> <li>Documenting changes in the relevant documentation files</li> <li>Committing changes with descriptive messages</li> </ol>"},{"location":"faq/common-issues/","title":"Common Issues","text":"<p>This page covers common issues you might encounter when using Medium-Mining and provides solutions.</p>"},{"location":"faq/common-issues/#installation-issues","title":"Installation Issues","text":""},{"location":"faq/common-issues/#poetry-dependency-resolution-failures","title":"Poetry Dependency Resolution Failures","text":"<p>Issue: Poetry fails to resolve dependencies with an error like <code>Could not find a matching version of package X</code>.</p> <p>Solution: 1. Update Poetry: <code>poetry self update</code> 2. Clear Poetry's cache: <code>poetry cache clear --all pypi</code> 3. Try using more flexible version constraints in <code>pyproject.toml</code> 4. Ensure Python version matches requirements (3.13+)</p>"},{"location":"faq/common-issues/#playwright-installation-failures","title":"Playwright Installation Failures","text":"<p>Issue: Playwright browser installation fails with permission errors or missing dependencies.</p> <p>Solution: <pre><code># Install manually with elevated permissions\nsudo pip install playwright\nsudo playwright install --with-deps chromium\n</code></pre></p> <p>For specific operating systems, see the Playwright installation docs.</p>"},{"location":"faq/common-issues/#duckdb-installation-issues","title":"DuckDB Installation Issues","text":"<p>Issue: DuckDB fails to install or import with cryptic errors.</p> <p>Solution: 1. Install required system dependencies:    <pre><code># Ubuntu/Debian\nsudo apt-get install -y python3-dev\n\n# macOS\nbrew install python\n</code></pre> 2. Try installing a specific version: <code>pip install duckdb==1.1.0</code> 3. For M1/M2 Macs, ensure you're using Python built for arm64</p>"},{"location":"faq/common-issues/#scraping-issues","title":"Scraping Issues","text":""},{"location":"faq/common-issues/#sitemap-scraper-failures","title":"Sitemap Scraper Failures","text":"<p>Issue: Sitemap scraper fails with connection errors.</p> <p>Solution: 1. Check your internet connection 2. Increase timeout values: <code>--timeout 0.2</code> 3. Verify Medium's sitemap structure hasn't changed 4. Check the <code>sitemap_scraper.log</code> for detailed error messages</p>"},{"location":"faq/common-issues/#article-scraper-browser-errors","title":"Article Scraper Browser Errors","text":"<p>Issue: Article scraper fails with browser-related errors like: - <code>Error: Browser closed unexpectedly</code> - <code>Error: Timeout 30000ms exceeded</code> - <code>Error: Target closed</code></p> <p>Solution: 1. Ensure Playwright is properly installed: <code>playwright install --with-deps chromium</code> 2. Reduce the number of concurrent workers: <code>--workers 2</code> 3. Run in non-headless mode for debugging: <code>--headless false</code> 4. Check system resources (memory, CPU) 5. Examine the screenshots in the <code>screenshots/</code> directory</p>"},{"location":"faq/common-issues/#medium-detection-and-blocking","title":"Medium Detection and Blocking","text":"<p>Issue: Medium detects the scraper as a bot and shows captchas or blocks access.</p> <p>Solution: 1. Increase delays between requests 2. Reduce concurrency 3. Modify browser configuration in <code>scrape_articles.py</code>:    <pre><code>context = browser.new_context(\n    viewport={\"width\": 1280, \"height\": 800},  # More standard desktop size\n    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n    locale=\"en-US\",\n)\n</code></pre> 4. Use rotating IP addresses or proxies 5. Run in non-headless mode and manually solve captchas</p>"},{"location":"faq/common-issues/#article-data-extraction-issues","title":"Article Data Extraction Issues","text":"<p>Issue: The scraper fails to extract certain article elements (comments, text, etc.).</p> <p>Solution: 1. Medium may have changed their UI structure 2. Update the selectors in <code>medium_helpers.py</code> 3. For comments issues, check if comments are loaded dynamically 4. Try running in non-headless mode to visually inspect the page 5. Increase wait times for page elements to load</p>"},{"location":"faq/common-issues/#database-issues","title":"Database Issues","text":""},{"location":"faq/common-issues/#database-connection-errors","title":"Database Connection Errors","text":"<p>Issue: Unable to connect to the database with errors like: - <code>OperationalError: unable to open database file</code> - <code>Permission denied</code></p> <p>Solution: 1. Check file permissions on the database file 2. Ensure the directory exists and is writable 3. Try connecting with a different tool to isolate the issue 4. Use absolute paths in <code>DATABASE_URL</code></p>"},{"location":"faq/common-issues/#database-schema-errors","title":"Database Schema Errors","text":"<p>Issue: Errors like <code>no such table</code> or <code>no such column</code> when querying.</p> <p>Solution: 1. Ensure the database was properly initialized: <code>python -m database.database</code> 2. Check if you're using the latest version of the code 3. For migration issues, consider:    <pre><code># Recreate tables (caution: this will delete existing data)\nBase.metadata.drop_all(engine)\nBase.metadata.create_all(engine)\n</code></pre></p>"},{"location":"faq/common-issues/#data-integrity-issues","title":"Data Integrity Issues","text":"<p>Issue: Data inconsistencies or missing relationships.</p> <p>Solution: 1. Check that transactions are properly committed 2. Use a database browser to examine the data 3. Add validation logic to data insertion functions 4. Use SQLAlchemy's relationship constraints 5. Implement proper error handling with rollbacks</p>"},{"location":"faq/common-issues/#analysis-issues","title":"Analysis Issues","text":""},{"location":"faq/common-issues/#jupyter-notebook-kernel-errors","title":"Jupyter Notebook Kernel Errors","text":"<p>Issue: Jupyter notebook fails to start or connect to the kernel.</p> <p>Solution: 1. Install ipykernel: <code>pip install ipykernel</code> 2. Register the kernel: <code>python -m ipykernel install --user</code> 3. Restart Jupyter: <code>jupyter notebook</code> 4. Try using JupyterLab instead: <code>jupyter lab</code></p>"},{"location":"faq/common-issues/#data-visualization-errors","title":"Data Visualization Errors","text":"<p>Issue: Matplotlib or Seaborn plots fail to display.</p> <p>Solution: 1. Ensure the appropriate backend is set: <code>%matplotlib inline</code> 2. Check for missing dependencies: <code>pip install matplotlib seaborn</code> 3. Restart the kernel 4. Try alternative plotting libraries: <code>import plotly.express as px</code></p>"},{"location":"faq/common-issues/#performance-issues-with-large-datasets","title":"Performance Issues with Large Datasets","text":"<p>Issue: Analysis is slow or crashes with large datasets.</p> <p>Solution: 1. Use sampling: <code>df.sample(n=1000)</code> or <code>query.limit(1000)</code> 2. Optimize SQL queries with proper indexing 3. Use DuckDB's analytical capabilities 4. Process data in chunks 5. Consider dedicated data analysis tools for very large datasets</p>"},{"location":"faq/common-issues/#system-resource-issues","title":"System Resource Issues","text":""},{"location":"faq/common-issues/#high-memory-usage","title":"High Memory Usage","text":"<p>Issue: The scraper consumes excessive memory, especially with multiple workers.</p> <p>Solution: 1. Reduce the number of concurrent workers: <code>--workers 2</code> 2. Process fewer URLs at a time: <code>--url-count 20</code> 3. Close browser contexts promptly after use 4. Monitor memory usage with tools like <code>htop</code> or Activity Monitor 5. Implement garbage collection where appropriate</p>"},{"location":"faq/common-issues/#cpu-utilization","title":"CPU Utilization","text":"<p>Issue: The scraper uses 100% CPU, slowing down the system.</p> <p>Solution: 1. Reduce the number of concurrent workers 2. Introduce delays between processing: <code>time.sleep(1)</code> 3. Run with lower process priority:    <pre><code># Linux/macOS\nnice -n 10 poetry run python -m scraper article\n\n# Windows PowerShell\nStart-Process -Priority BelowNormal -FilePath python -ArgumentList \"-m scraper article\"\n</code></pre></p>"},{"location":"faq/common-issues/#disk-space-issues","title":"Disk Space Issues","text":"<p>Issue: The database file grows very large.</p> <p>Solution: 1. Process data in smaller batches 2. Implement data retention policies 3. Archive or export old data 4. Use database compression features 5. Consider database vacuum/optimization:    <pre><code>engine.execute(\"VACUUM\")\n</code></pre></p>"},{"location":"faq/common-issues/#command-line-interface-issues","title":"Command-Line Interface Issues","text":""},{"location":"faq/common-issues/#command-not-found","title":"Command Not Found","text":"<p>Issue: Getting <code>command not found</code> or similar errors when running commands.</p> <p>Solution: 1. Ensure Poetry environment is activated: <code>poetry shell</code> 2. Use the full command with Poetry: <code>poetry run python -m scraper article</code> 3. Check that the package is properly installed: <code>poetry install</code> 4. Verify the correct Python version is being used</p>"},{"location":"faq/common-issues/#command-line-argument-errors","title":"Command-Line Argument Errors","text":"<p>Issue: Arguments not being recognized or incorrect behavior.</p> <p>Solution: 1. Double-check argument syntax 2. Use <code>--help</code> to see available options: <code>python -m scraper article --help</code> 3. For boolean flags, use explicit values: <code>--headless true</code> 4. Check for version mismatches between code and documentation</p>"},{"location":"faq/common-issues/#debugging-techniques","title":"Debugging Techniques","text":""},{"location":"faq/common-issues/#enabling-verbose-logging","title":"Enabling Verbose Logging","text":"<p>To get more information about what's happening:</p> <pre><code># For sitemap scraper\npoetry run python -m scraper sitemap --verbose\n\n# For general Python debugging\nPYTHONVERBOSE=1 poetry run python -m scraper article\n</code></pre>"},{"location":"faq/common-issues/#using-screenshots","title":"Using Screenshots","text":"<p>The article scraper automatically takes screenshots. Examine them to understand what the browser is seeing:</p> <pre><code># Open the screenshots directory\nopen screenshots/\n</code></pre>"},{"location":"faq/common-issues/#database-inspection","title":"Database Inspection","text":"<p>Directly examine the database with DuckDB's CLI:</p> <pre><code>duckdb medium_articles.duckdb\n\n# Then run SQL queries\nSELECT COUNT(*) FROM urls;\nSELECT COUNT(*) FROM medium_articles;\n</code></pre>"},{"location":"faq/common-issues/#network-debugging","title":"Network Debugging","text":"<p>To debug network issues, capture network traffic:</p> <pre><code># Linux\nsudo tcpdump -n -s 0 host medium.com -w medium_traffic.pcap\n\n# macOS\nsudo tcpdump -n -s 0 host medium.com -w medium_traffic.pcap\n\n# Windows\n# Use Wireshark\n</code></pre>"},{"location":"faq/common-issues/#process-monitoring","title":"Process Monitoring","text":"<p>Monitor the process during execution:</p> <pre><code># Linux/macOS\nhtop -p $(pgrep -f \"python.*scraper\")\n\n# Windows\n# Use Task Manager\n</code></pre>"},{"location":"faq/common-issues/#advanced-troubleshooting","title":"Advanced Troubleshooting","text":"<p>For issues that persist after trying the solutions above:</p> <ol> <li> <p>Debug Mode: Run Python with debugging enabled:    <pre><code>python -m pdb -m scraper article\n</code></pre></p> </li> <li> <p>Code Inspection: Temporarily modify the code to add debug prints:    <pre><code>import logging\nlogging.basicConfig(level=logging.DEBUG)\n</code></pre></p> </li> <li> <p>Interactive Debugging: Use IPython for interactive debugging:    <pre><code>import IPython; IPython.embed()  # Add this line where you want to inspect\n</code></pre></p> </li> <li> <p>Simplified Reproduction: Create a minimal example that reproduces the issue</p> </li> <li> <p>GitHub Issues: Search for or create an issue on the GitHub repository</p> </li> </ol>"},{"location":"faq/faq/","title":"Frequently Asked Questions","text":"<p>This page answers common questions about the Medium-Mining project.</p>"},{"location":"faq/faq/#general-questions","title":"General Questions","text":""},{"location":"faq/faq/#what-is-medium-mining","title":"What is Medium-Mining?","text":"<p>Medium-Mining is a Python-based project designed to extract and analyze content from Medium.com. It includes tools for scraping article URLs from sitemaps, extracting detailed content and metadata from articles, and analyzing the collected data.</p>"},{"location":"faq/faq/#what-can-i-do-with-medium-mining","title":"What can I do with Medium-Mining?","text":"<p>Medium-Mining enables you to:</p> <ul> <li>Collect article URLs from Medium's sitemaps</li> <li>Extract article content, metadata, and comments</li> <li>Store the data in a structured database</li> <li>Analyze the data using provided notebooks</li> <li>Export the data for use with external tools</li> </ul>"},{"location":"faq/faq/#is-using-medium-mining-legal","title":"Is using Medium-Mining legal?","text":"<p>Medium-Mining is provided for research and educational purposes only. Always check Medium's Terms of Service before using this tool. Some considerations:</p> <ul> <li>Always include reasonable delays between requests</li> <li>Respect Medium's robots.txt file</li> <li>Do not use the data for commercial purposes without proper authorization</li> <li>Do not republish Medium content without permission</li> </ul>"},{"location":"faq/faq/#what-are-the-system-requirements","title":"What are the system requirements?","text":"<ul> <li>Python 3.13 or newer</li> <li>2GB+ of RAM</li> <li>Sufficient disk space for the database (size depends on the amount of data collected)</li> <li>Internet connection</li> <li>Operating system: Windows, macOS, or Linux</li> </ul>"},{"location":"faq/faq/#installation-questions","title":"Installation Questions","text":""},{"location":"faq/faq/#why-does-poetry-installation-fail","title":"Why does Poetry installation fail?","text":"<p>If you encounter issues installing dependencies with Poetry, try:</p> <ol> <li>Check your Python version: <code>python --version</code> (ensure it's 3.13+)</li> <li>Update Poetry: <code>poetry self update</code></li> <li>Clear Poetry cache: <code>poetry cache clear pypi --all</code></li> <li>Try with verbose output: <code>poetry install -v</code></li> </ol>"},{"location":"faq/faq/#why-does-playwright-installation-fail","title":"Why does Playwright installation fail?","text":"<p>Playwright installation issues are usually related to missing system dependencies. Try:</p> <pre><code># Install Playwright manually\npip install playwright\nplaywright install --with-deps chromium\n</code></pre> <p>For Linux users, you might need additional packages. See the installation guide for details.</p>"},{"location":"faq/faq/#can-i-use-medium-mining-without-poetry","title":"Can I use Medium-Mining without Poetry?","text":"<p>Yes, you can install the dependencies manually using pip:</p> <pre><code>pip install -r requirements.txt\npip install playwright\nplaywright install\n</code></pre> <p>Note that this approach might not guarantee the exact versions specified in <code>pyproject.toml</code>.</p>"},{"location":"faq/faq/#usage-questions","title":"Usage Questions","text":""},{"location":"faq/faq/#how-many-urls-can-i-scrape","title":"How many URLs can I scrape?","text":"<p>The number of URLs you can scrape depends on:</p> <ul> <li>Your internet connection</li> <li>System resources</li> <li>Time constraints</li> <li>Ethical considerations regarding server load</li> </ul> <p>As a guideline, start with a small number (e.g., 50-100) and gradually increase as needed.</p>"},{"location":"faq/faq/#how-do-i-scrape-member-only-articles","title":"How do I scrape member-only articles?","text":"<p>Medium-Mining can detect member-only articles but cannot access their full content without a Medium membership. The scraper will still extract available metadata and indicate that the article is member-only.</p>"},{"location":"faq/faq/#how-do-i-increase-scraping-speed","title":"How do I increase scraping speed?","text":"<p>To improve scraping speed:</p> <ul> <li>Increase the number of worker threads with <code>--workers</code></li> <li>Use headless mode with <code>--headless true</code></li> <li>Run on a machine with a faster internet connection</li> <li>Optimize your database configuration</li> </ul> <p>However, always maintain ethical scraping practices with reasonable delays.</p>"},{"location":"faq/faq/#can-i-pause-and-resume-scraping","title":"Can I pause and resume scraping?","text":"<p>The scraper doesn't have built-in pause/resume functionality. However, since processed URLs are marked in the database, you can simply stop and restart the scraper, and it will continue with unprocessed URLs.</p>"},{"location":"faq/faq/#database-questions","title":"Database Questions","text":""},{"location":"faq/faq/#how-do-i-view-the-collected-data","title":"How do I view the collected data?","text":"<p>You can view the collected data in several ways:</p> <ol> <li>Use the provided Jupyter notebooks in the <code>notebooks/</code> directory</li> <li>Connect to the DuckDB database with a SQL client</li> <li>Use SQLAlchemy in your own Python scripts</li> <li>Export the data to CSV or JSON formats</li> </ol>"},{"location":"faq/faq/#how-large-can-the-database-become","title":"How large can the database become?","text":"<p>The database size depends on the number of articles you scrape and their content. As a rough estimate:</p> <ul> <li>Each sitemap entry: ~100 bytes</li> <li>Each URL entry: ~500 bytes</li> <li>Each article (with comments): ~10-100 KB</li> </ul> <p>A collection of 10,000 articles might require 100MB-1GB of storage.</p>"},{"location":"faq/faq/#can-i-use-a-different-database-backend","title":"Can I use a different database backend?","text":"<p>Yes, Medium-Mining uses SQLAlchemy, which supports multiple database backends. To use a different backend:</p> <ol> <li>Modify the <code>DATABASE_URL</code> in <code>src/database/database.py</code></li> <li>Install the appropriate database driver</li> <li>Update any database-specific code if needed</li> </ol> <p>For example, to use PostgreSQL:</p> <pre><code>DATABASE_URL = \"postgresql://username:password@localhost/medium_articles\"\n</code></pre>"},{"location":"faq/faq/#how-do-i-back-up-the-database","title":"How do I back up the database?","text":"<p>For DuckDB, you can simply copy the database file:</p> <pre><code>cp medium_articles.duckdb medium_articles_backup.duckdb\n</code></pre> <p>For other database backends, use the appropriate backup procedure for that database system.</p>"},{"location":"faq/faq/#scraping-questions","title":"Scraping Questions","text":""},{"location":"faq/faq/#why-do-i-get-timeout-error-when-scraping-articles","title":"Why do I get \"Timeout Error\" when scraping articles?","text":"<p>Timeout errors can occur for several reasons:</p> <ul> <li>Slow internet connection</li> <li>Medium's server is slow to respond</li> <li>The page has complex content that takes time to load</li> <li>Your system is under high load</li> </ul> <p>Try increasing the timeout parameter or reducing the number of concurrent workers.</p>"},{"location":"faq/faq/#why-does-the-scraper-show-failed-to-click-responses-button","title":"Why does the scraper show \"Failed to click responses button\"?","text":"<p>This warning appears when the scraper cannot find or click the \"See all responses\" button on a Medium article. This can happen if:</p> <ul> <li>The article has no comments</li> <li>Medium has changed their UI</li> <li>The button is not yet visible (needs scrolling)</li> </ul> <p>This warning is not critical and can be ignored if you're not primarily interested in comments.</p>"},{"location":"faq/faq/#how-do-i-handle-captcha-or-bot-detection","title":"How do I handle CAPTCHA or bot detection?","text":"<p>If Medium starts showing CAPTCHAs or blocking the scraper:</p> <ol> <li>Reduce the scraping frequency (increase timeouts)</li> <li>Run in non-headless mode to manually solve CAPTCHAs</li> <li>Try changing the browser configuration</li> <li>Use a different IP address or proxy</li> </ol>"},{"location":"faq/faq/#can-i-scrape-only-specific-topics-or-authors","title":"Can I scrape only specific topics or authors?","text":"<p>The current implementation scrapes URLs from sitemaps, which doesn't allow filtering by topic or author. However, you could:</p> <ol> <li>Scrape a broader set of articles</li> <li>Filter the results in your analysis phase</li> <li>Or modify the code to first extract URLs from specific topic or author pages</li> </ol>"},{"location":"faq/faq/#analysis-questions","title":"Analysis Questions","text":""},{"location":"faq/faq/#how-do-i-perform-custom-analysis","title":"How do I perform custom analysis?","text":"<p>To create your own analysis:</p> <ol> <li>Copy one of the example notebooks in the <code>notebooks/</code> directory</li> <li>Modify the queries and visualizations as needed</li> <li>Use SQLAlchemy to create custom queries</li> <li>Leverage pandas, matplotlib, and other data analysis libraries</li> </ol>"},{"location":"faq/faq/#can-i-use-nlp-tools-with-the-collected-data","title":"Can I use NLP tools with the collected data?","text":"<p>Yes, the article text is stored in the database and can be used with NLP tools:</p> <ol> <li>Extract the article text from the database</li> <li>Use libraries like NLTK, spaCy, or Transformers for analysis</li> <li>Implement techniques like:</li> <li>Topic modeling</li> <li>Sentiment analysis</li> <li>Named entity recognition</li> <li>Text summarization</li> </ol>"},{"location":"faq/faq/#how-do-i-visualize-the-results","title":"How do I visualize the results?","text":"<p>The notebooks include examples using matplotlib and seaborn. You can also:</p> <ul> <li>Use other Python visualization libraries like Plotly or Bokeh</li> <li>Export the data to tools like Tableau or Power BI</li> <li>Create custom visualizations based on your specific needs</li> </ul>"},{"location":"faq/faq/#can-i-export-the-data-for-use-with-other-tools","title":"Can I export the data for use with other tools?","text":"<p>Yes, you can export the data to various formats:</p> <ul> <li>CSV for spreadsheet applications</li> <li>JSON for web applications</li> <li>SQL dumps for other database systems</li> <li>Custom formats for specific tools</li> </ul> <p>See the Export Data guide for examples.</p>"},{"location":"faq/faq/#troubleshooting","title":"Troubleshooting","text":"<p>For solutions to common problems, see the Common Issues and Debugging guides.</p> <p>If your question isn't answered here, consider:</p> <ul> <li>Checking the GitHub Issues for similar problems</li> <li>Creating a new issue with a detailed description of your problem</li> <li>Contributing to this FAQ if you find a solution to a common problem</li> </ul>"},{"location":"getting-started/installation/","title":"Installation Guide","text":"<p>This guide will walk you through the process of setting up the Medium-Mining project on your system.</p>"},{"location":"getting-started/installation/#prerequisites","title":"Prerequisites","text":"<p>Before installing Medium-Mining, ensure you have the following prerequisites:</p> <ul> <li>Python: Version 3.13 or newer is required</li> <li>Poetry: For dependency management</li> <li>Git: For cloning the repository (optional)</li> </ul>"},{"location":"getting-started/installation/#installing-python","title":"Installing Python","text":"<p>If you don't have Python 3.13+ installed, you can download it from the official Python website.</p> <p>To verify your Python version, run:</p> <pre><code>python --version\n</code></pre>"},{"location":"getting-started/installation/#installing-poetry","title":"Installing Poetry","text":"<p>Medium-Mining uses Poetry for dependency management. If you don't have Poetry installed, follow these steps:</p> Linux/macOSWindows PowerShell <pre><code>curl -sSL https://install.python-poetry.org | python3 -\n</code></pre> <pre><code>(Invoke-WebRequest -Uri https://install.python-poetry.org -UseBasicParsing).Content | python -\n</code></pre> <p>Verify the installation:</p> <pre><code>poetry --version\n</code></pre>"},{"location":"getting-started/installation/#installing-medium-mining","title":"Installing Medium-Mining","text":"<p>There are two main ways to install Medium-Mining:</p>"},{"location":"getting-started/installation/#option-1-clone-from-github-recommended","title":"Option 1: Clone from GitHub (Recommended)","text":"<ol> <li>Clone the repository:</li> </ol> <pre><code>git clone https://github.com/M-Enderle/Medium-Mining.git\ncd Medium-Mining\n</code></pre> <ol> <li>Install dependencies using Poetry:</li> </ol> <pre><code>poetry install\n</code></pre> <ol> <li>Initialize Playwright browsers:</li> </ol> <pre><code>poetry run playwright install\n</code></pre>"},{"location":"getting-started/installation/#option-2-install-from-pypi-not-yet-available","title":"Option 2: Install from PyPI (not yet available)","text":"<p>Note</p> <p>This method will be available once the project is published to PyPI.</p> <pre><code>pip install medium-mining\n</code></pre>"},{"location":"getting-started/installation/#initialize-the-database","title":"Initialize the Database","text":"<p>Before using Medium-Mining, you need to initialize the database:</p> <pre><code>poetry run python -m database.database\n</code></pre> <p>This will create a SQLite database in the project directory called <code>medium_articles.duckdb</code>.</p>"},{"location":"getting-started/installation/#verifying-the-installation","title":"Verifying the Installation","text":"<p>To verify that Medium-Mining was installed correctly, you can run:</p> <pre><code>poetry run python -m scraper --help\n</code></pre> <p>You should see the help message with available commands and options.</p>"},{"location":"getting-started/installation/#system-specific-considerations","title":"System-Specific Considerations","text":""},{"location":"getting-started/installation/#windows","title":"Windows","text":"<p>On Windows, you might need to ensure that the Python Scripts directory is in your PATH.</p>"},{"location":"getting-started/installation/#macos","title":"macOS","text":"<p>On macOS, you might need to install additional dependencies if you encounter issues with Playwright:</p> <pre><code>brew install webkit\n</code></pre>"},{"location":"getting-started/installation/#linux","title":"Linux","text":"<p>On Linux, you might need to install additional dependencies for Playwright:</p> <pre><code># Ubuntu/Debian\napt-get update\napt-get install -y libwoff1 libopus0 libwebp6 libwebpdemux2 libenchant1c2a libgudev-1.0-0 libsecret-1-0 libhyphen0 libgdk-pixbuf2.0-0 libegl1 libnotify4 libxslt1.1 libevent-2.1-6 libgles2 libvpx5 libxcomposite1 libatk1.0-0 libatk-bridge2.0-0 libepoxy0 libgtk-3-0 libharfbuzz-icu0 libgstreamer-gl1.0-0 libgstreamer-plugins-bad1.0-0 libopenjp2-7 libgstreamer1.0-0 libgstreamer-plugins-base1.0-0 libjpeg-dev libpng-dev\n\n# CentOS/RHEL\nyum install -y pango.x86_64 libXcomposite.x86_64 libXcursor.x86_64 libXdamage.x86_64 libXext.x86_64 libXi.x86_64 libXtst.x86_64 cups-libs.x86_64 libXScrnSaver.x86_64 libXrandr.x86_64 GConf2.x86_64 alsa-lib.x86_64 atk.x86_64 gtk3.x86_64 ipa-gothic-fonts xorg-x11-fonts-100dpi xorg-x11-fonts-75dpi xorg-x11-utils xorg-x11-fonts-cyrillic xorg-x11-fonts-Type1 xorg-x11-fonts-misc\n</code></pre>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter any issues during installation, refer to the Common Issues section or open an issue on the GitHub repository.</p>"},{"location":"getting-started/installation/#common-installation-issues","title":"Common Installation Issues","text":""},{"location":"getting-started/installation/#poetry-environment-not-found","title":"Poetry Environment Not Found","text":"<p>If Poetry can't find the environment, try:</p> <pre><code>poetry env use $(which python3.13)\n</code></pre>"},{"location":"getting-started/installation/#playwright-installation-fails","title":"Playwright Installation Fails","text":"<p>If Playwright installation fails, try installing it manually:</p> <pre><code>pip install playwright\nplaywright install\n</code></pre>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<p>Once you have successfully installed Medium-Mining, proceed to the Quick Start guide to learn how to use the tool.</p>"},{"location":"getting-started/quick-start/","title":"Quick Start Guide","text":"<p>This guide will help you quickly get up and running with Medium-Mining to scrape and analyze Medium articles.</p>"},{"location":"getting-started/quick-start/#prerequisites","title":"Prerequisites","text":"<p>Before starting, ensure you have:</p> <ul> <li>Installed Medium-Mining following the Installation Guide</li> <li>Initialized the database as described in the installation instructions</li> <li>Installed Playwright browsers with <code>poetry run playwright install</code></li> </ul>"},{"location":"getting-started/quick-start/#basic-workflow","title":"Basic Workflow","text":"<p>The typical workflow with Medium-Mining consists of three main steps:</p> <ol> <li>Scrape article URLs from Medium's sitemaps</li> <li>Scrape content and metadata from individual articles</li> <li>Analyze the collected data</li> </ol>"},{"location":"getting-started/quick-start/#step-1-scrape-article-urls","title":"Step 1: Scrape Article URLs","text":"<p>The first step is to collect article URLs from Medium's sitemaps:</p> <pre><code>poetry run python -m scraper sitemap\n</code></pre> <p>This command will:</p> <ol> <li>Retrieve Medium's master sitemap index</li> <li>Process each individual sitemap</li> <li>Extract article URLs and store them in the database</li> <li>Skip sitemaps that have already been processed</li> </ol>"},{"location":"getting-started/quick-start/#options","title":"Options","text":"<p>You can customize the sitemap scraper with these options:</p> <pre><code># Set a custom timeout between requests (in seconds)\npoetry run python -m scraper sitemap --timeout 0.1\n\n# Enable verbose logging to console\npoetry run python -m scraper sitemap --verbose\n</code></pre>"},{"location":"getting-started/quick-start/#monitoring-progress","title":"Monitoring Progress","text":"<p>The sitemap scraper logs its progress to <code>sitemap_scraper.log</code>. You can monitor this log in real-time:</p> <pre><code>tail -f sitemap_scraper.log\n</code></pre>"},{"location":"getting-started/quick-start/#step-2-scrape-article-content","title":"Step 2: Scrape Article Content","text":"<p>Once you have collected article URLs, you can scrape the content and metadata:</p> <pre><code>poetry run python -m scraper article\n</code></pre> <p>This command will:</p> <ol> <li>Retrieve random unprocessed URLs from the database</li> <li>Open each URL in a browser (using Playwright)</li> <li>Extract article content, metadata, and comments</li> <li>Store the data in the database</li> <li>Take screenshots for verification</li> </ol>"},{"location":"getting-started/quick-start/#options_1","title":"Options","text":"<p>You can customize the article scraper with these options:</p> <pre><code># Specify the number of URLs to process\npoetry run python -m scraper article --url-count 50\n\n# Run browsers in headless mode (invisible)\npoetry run python -m scraper article --headless true\n\n# Use multiple worker threads for concurrent processing\npoetry run python -m scraper article --workers 4\n</code></pre>"},{"location":"getting-started/quick-start/#example-full-configuration","title":"Example: Full Configuration","text":"<p>Here's an example with all options set:</p> <pre><code>poetry run python -m scraper article --url-count 100 --headless true --workers 4\n</code></pre> <p>This will process 100 random URLs using 4 concurrent workers in headless mode.</p>"},{"location":"getting-started/quick-start/#monitoring-progress_1","title":"Monitoring Progress","text":"<p>The article scraper logs its progress to <code>article_scraper.log</code> and displays metrics on completion:</p> <pre><code>tail -f article_scraper.log\n</code></pre>"},{"location":"getting-started/quick-start/#step-3-analyze-the-data","title":"Step 3: Analyze the Data","text":"<p>After collecting data, you can analyze it using the provided Jupyter notebooks:</p> <ol> <li>Start Jupyter:</li> </ol> <pre><code>poetry run jupyter notebook\n</code></pre> <ol> <li>Open the analysis notebook:</li> </ol> <pre><code>notebooks/analyze_scraping_results.ipynb\n</code></pre> <ol> <li>Run the cells to explore the data</li> </ol>"},{"location":"getting-started/quick-start/#example-analysis","title":"Example Analysis","text":"<p>The notebook includes examples for:</p> <ul> <li>Counting the total number of collected URLs</li> <li>Visualizing article publication dates</li> <li>Analyzing change frequency patterns</li> <li>Exploring sitemap structure</li> <li>Correlating article metrics</li> </ul>"},{"location":"getting-started/quick-start/#custom-analysis","title":"Custom Analysis","text":"<p>You can create your own analysis by:</p> <ol> <li>Copying the example notebook</li> <li>Modifying the queries to extract the data you're interested in</li> <li>Creating custom visualizations</li> <li>Adding your own analytical techniques</li> </ol>"},{"location":"getting-started/quick-start/#complete-workflow-example","title":"Complete Workflow Example","text":"<p>Here's a complete example workflow:</p> <pre><code># 1. Initialize the database\npoetry run python -m database.database\n\n# 2. Scrape sitemaps with verbose output\npoetry run python -m scraper sitemap --verbose\n\n# 3. Scrape 200 articles with 4 workers\npoetry run python -m scraper article --url-count 200 --workers 4 --headless true\n\n# 4. Run Jupyter for analysis\npoetry run jupyter notebook notebooks/analyze_scraping_results.ipynb\n</code></pre>"},{"location":"getting-started/quick-start/#tips-for-effective-use","title":"Tips for Effective Use","text":""},{"location":"getting-started/quick-start/#performance-optimization","title":"Performance Optimization","text":"<ul> <li>Headless Mode: Use <code>--headless true</code> for faster processing</li> <li>Worker Threads: Use <code>--workers</code> to match your CPU cores (typically 4-8)</li> <li>Batch Size: Process manageable batches (50-100) at a time</li> <li>Database Indexing: The database schema includes indexes for efficient querying</li> </ul>"},{"location":"getting-started/quick-start/#resource-management","title":"Resource Management","text":"<ul> <li>Disk Space: Monitor database size, which grows with the number of articles</li> <li>Memory Usage: Each browser instance requires memory; adjust worker count accordingly</li> <li>CPU Usage: Multiple workers can lead to high CPU utilization</li> </ul>"},{"location":"getting-started/quick-start/#ethical-considerations","title":"Ethical Considerations","text":"<ul> <li>Rate Limiting: Use reasonable timeouts (0.05-0.1 seconds) between sitemap requests</li> <li>Concurrency: Avoid excessive concurrent requests to Medium's servers</li> <li>Purpose: Use the collected data for research and educational purposes only</li> </ul>"},{"location":"getting-started/quick-start/#next-steps","title":"Next Steps","text":"<p>Now that you're familiar with the basic workflow, you can:</p> <ul> <li>Learn more about the Sitemap Scraper</li> <li>Explore the Article Scraper in depth</li> <li>Understand the Database Schema</li> <li>Try advanced Data Analysis techniques</li> <li>Configure Medium-Mining for your specific needs</li> </ul>"},{"location":"getting-started/quick-start/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter issues, check:</p> <ul> <li>Logs: Review <code>sitemap_scraper.log</code> and <code>article_scraper.log</code></li> <li>Screenshots: Examine screenshots in the <code>screenshots/</code> directory</li> <li>Database: Verify data in the database using the analysis notebooks</li> <li>FAQ: See the FAQ and Common Issues sections</li> </ul>"},{"location":"modules/database/","title":"Database Module","text":"<p>The database module handles the storage and retrieval of scraped Medium data.</p>"},{"location":"modules/database/#initialization","title":"Initialization","text":"<p>Initialize the database with:</p> <pre><code>poetry run python -m database.database\n</code></pre>"},{"location":"modules/database/#module-structure","title":"Module Structure","text":"<ul> <li><code>database.py</code> - Contains database models, connection logic, and utility functions</li> </ul>"},{"location":"modules/database/#schema","title":"Schema","text":"<p>The database stores information about:</p> <ul> <li>Article URLs from sitemaps</li> <li>Article content and metadata</li> <li>Publishing information</li> <li>Tags and categories</li> </ul>"},{"location":"modules/database/#usage-example","title":"Usage Example","text":"<pre><code>from database.database import session, Article\n\n# Query all articles\narticles = session.query(Article).all()\n\n# Get articles with a specific tag\ntagged_articles = session.query(Article).filter(Article.tags.contains(['programming'])).all()\n</code></pre>"},{"location":"modules/scraper/","title":"Scraper Module","text":"<p>The scraper module contains functionality for scraping Medium sitemaps and articles.</p>"},{"location":"modules/scraper/#components","title":"Components","text":""},{"location":"modules/scraper/#sitemap-scraper","title":"Sitemap Scraper","text":"<p>The sitemap scraper extracts article URLs from Medium's sitemaps.</p> <pre><code>poetry run python -m scraper sitemap [--timeout 0.05] [--verbose]\n</code></pre>"},{"location":"modules/scraper/#article-scraper","title":"Article Scraper","text":"<p>The article scraper extracts content and metadata from Medium articles.</p> <pre><code>poetry run python -m scraper article [--url-count 50] [--headless true] [--workers 1]\n</code></pre>"},{"location":"modules/scraper/#module-structure","title":"Module Structure","text":"<ul> <li><code>__main__.py</code> - Entry point for the scraper CLI</li> <li><code>scrape_sitemaps.py</code> - Functionality for scraping sitemaps</li> <li><code>scrape_articles.py</code> - Functionality for scraping article content</li> <li><code>medium_helpers.py</code> - Helper functions for interacting with Medium</li> </ul>"},{"location":"modules/database/overview/","title":"Database Module Overview","text":"<p>The database module is a core component of the Medium-Mining project, providing persistent storage and retrieval capabilities for all scraped data. It uses SQLAlchemy ORM with DuckDB as the backend, offering a balance of simplicity, performance, and analytical capabilities.</p>"},{"location":"modules/database/overview/#key-components","title":"Key Components","text":"<p>The database module consists of the following key components:</p> <pre><code>graph TD\n    A[Database Configuration] --&gt; B[SQLAlchemy Models]\n    B --&gt; C[Sitemap Model]\n    B --&gt; D[URL Model]\n    B --&gt; E[MediumArticle Model]\n    B --&gt; F[Comment Model]\n    A --&gt; G[Session Management]\n    C --&gt; G\n    D --&gt; G\n    E --&gt; G\n    F --&gt; G\n    G --&gt; H[Database Operations]</code></pre>"},{"location":"modules/database/overview/#database-configuration","title":"Database Configuration","text":"<p>The database is configured in <code>src/database/database.py</code>:</p> <pre><code>DATABASE_URL = \"duckdb:///medium_articles.duckdb\"  # Persistent storage\nBase = declarative_base()\nengine = create_engine(DATABASE_URL, echo=False)\nSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\n</code></pre> <p>This configuration: - Uses DuckDB as the database engine - Stores data in a local file called <code>medium_articles.duckdb</code> - Disables SQL echo for production use - Creates a session factory for thread-safe database access</p>"},{"location":"modules/database/overview/#sqlalchemy-models","title":"SQLAlchemy Models","text":"<p>The module defines four main models:</p> <ol> <li>Sitemap: Represents a processed Medium sitemap</li> <li>URL: Represents an article URL extracted from a sitemap</li> <li>MediumArticle: Represents a scraped article with its content and metadata</li> <li>Comment: Represents a comment on an article</li> </ol> <p>Each model uses SQLAlchemy's declarative syntax to define tables, columns, and relationships.</p>"},{"location":"modules/database/overview/#session-management","title":"Session Management","text":"<p>Database sessions are managed through the <code>SessionLocal</code> factory and the <code>get_session()</code> utility function:</p> <pre><code>def get_session():\n    \"\"\"Get a SQLAlchemy session for database operations.\"\"\"\n    return SessionLocal()\n</code></pre> <p>This function is used throughout the application to obtain a session for database operations.</p>"},{"location":"modules/database/overview/#database-features","title":"Database Features","text":""},{"location":"modules/database/overview/#schema-definition","title":"Schema Definition","text":"<p>The database schema is defined using SQLAlchemy's declarative syntax:</p> <pre><code>class MediumArticle(Base):\n    __tablename__ = \"medium_articles\"\n    article_id_seq = Sequence(\"article_id_seq\")\n\n    id = Column(\n        \"id\",\n        Integer,\n        article_id_seq,\n        server_default=article_id_seq.next_value(),\n        primary_key=True,\n    )\n    url_id = Column(Integer, ForeignKey(\"urls.id\"))\n    title = Column(String(255), nullable=False)\n    # ... other columns ...\n</code></pre>"},{"location":"modules/database/overview/#relationships","title":"Relationships","text":"<p>The models define relationships between tables:</p> <ul> <li>Sitemap \u2192 URL: One-to-many (one sitemap contains many URLs)</li> <li>URL \u2192 MediumArticle: One-to-one (one URL corresponds to one article)</li> <li>MediumArticle \u2192 Comment: One-to-many (one article has many comments)</li> </ul>"},{"location":"modules/database/overview/#initialization","title":"Initialization","text":"<p>The database and tables are created using the <code>setup_database()</code> function:</p> <pre><code>def setup_database():\n    \"\"\"Create the database and tables if they don't exist.\"\"\"\n    try:\n        Base.metadata.create_all(engine)\n        print(\"Database and tables created successfully.\")\n    except Exception as e:\n        print(f\"Error creating database: {e}\")\n</code></pre>"},{"location":"modules/database/overview/#usage-examples","title":"Usage Examples","text":""},{"location":"modules/database/overview/#creating-a-session","title":"Creating a Session","text":"<pre><code>from database.database import get_session\n\n# Create a session\nsession = get_session()\n\ntry:\n    # Use the session for database operations\n    # ...\n    session.commit()\nexcept Exception as e:\n    session.rollback()\n    raise e\nfinally:\n    session.close()\n</code></pre>"},{"location":"modules/database/overview/#querying-data","title":"Querying Data","text":"<pre><code>from database.database import URL, MediumArticle, get_session\n\n# Create a session\nsession = get_session()\n\ntry:\n    # Query all URLs that haven't been crawled yet\n    uncrawled_urls = session.query(URL).filter(URL.last_crawled == None).all()\n\n    # Query articles with specific tags\n    python_articles = session.query(MediumArticle).filter(\n        MediumArticle.tags.like(\"%python%\")\n    ).all()\n\n    # Count articles by publisher\n    publisher_counts = session.query(\n        MediumArticle.publisher, func.count(MediumArticle.id)\n    ).group_by(MediumArticle.publisher).all()\nfinally:\n    session.close()\n</code></pre>"},{"location":"modules/database/overview/#inserting-data","title":"Inserting Data","text":"<pre><code>from database.database import URL, Sitemap, get_session\n\n# Create a session\nsession = get_session()\n\ntry:\n    # Create a new sitemap\n    sitemap = Sitemap(sitemap_url=\"https://medium.com/sitemap/example.xml\", articles_count=100)\n    session.add(sitemap)\n    session.flush()  # Flush to get the sitemap ID\n\n    # Create a new URL\n    url = URL(\n        url=\"https://medium.com/example-article\",\n        last_modified=\"2023-01-01\",\n        change_freq=\"monthly\",\n        priority=0.8,\n        sitemap_id=sitemap.id\n    )\n    session.add(url)\n\n    # Commit the transaction\n    session.commit()\nexcept Exception as e:\n    session.rollback()\n    raise e\nfinally:\n    session.close()\n</code></pre>"},{"location":"modules/database/overview/#batch-operations","title":"Batch Operations","text":"<p>For efficiency, the database module supports batch operations:</p> <pre><code>from database.database import URL, get_session\n\n# Create a session\nsession = get_session()\n\ntry:\n    # Create multiple URL objects\n    urls = [\n        URL(url=f\"https://medium.com/article-{i}\", sitemap_id=1)\n        for i in range(1000)\n    ]\n\n    # Add them in a batch\n    session.bulk_save_objects(urls)\n\n    # Commit the transaction\n    session.commit()\nexcept Exception as e:\n    session.rollback()\n    raise e\nfinally:\n    session.close()\n</code></pre>"},{"location":"modules/database/overview/#database-performance","title":"Database Performance","text":""},{"location":"modules/database/overview/#duckdb-benefits","title":"DuckDB Benefits","text":"<p>DuckDB offers several advantages for this project:</p> <ul> <li>Embedded Database: No need for a separate database server</li> <li>Column-Oriented: Efficient for analytical queries</li> <li>ACID Transactions: Ensures data integrity</li> <li>SQL Compatibility: Familiar query language</li> <li>SQLAlchemy Support: Easy integration with Python</li> </ul>"},{"location":"modules/database/overview/#optimization-techniques","title":"Optimization Techniques","text":"<p>The database module employs several optimization techniques:</p> <ol> <li>Batch Operations: Using <code>bulk_save_objects</code> for efficient batch inserts</li> <li>Connection Pooling: Through SQLAlchemy's session management</li> <li>Indexing: Primary keys and foreign keys are indexed</li> <li>Transaction Management: Proper use of commit and rollback</li> </ol>"},{"location":"modules/database/overview/#thread-safety","title":"Thread Safety","text":"<p>The database module is designed to be thread-safe:</p> <ul> <li>Session Factory: Each thread creates its own session</li> <li>Session Scope: Sessions are properly closed after use</li> <li>Connection Pooling: SQLAlchemy handles connection pooling</li> <li>Transactions: ACID transactions ensure consistency</li> </ul>"},{"location":"modules/database/overview/#extension-points","title":"Extension Points","text":"<p>The database module can be extended in several ways:</p>"},{"location":"modules/database/overview/#adding-new-models","title":"Adding New Models","text":"<p>To add a new model:</p> <ol> <li>Define a new class inheriting from <code>Base</code></li> <li>Define columns and relationships</li> <li>Run <code>setup_database()</code> to create the new table</li> </ol>"},{"location":"modules/database/overview/#using-a-different-database","title":"Using a Different Database","text":"<p>To use a different database backend:</p> <ol> <li>Change the <code>DATABASE_URL</code> to point to the new database</li> <li>Install the appropriate SQLAlchemy dialect</li> <li>Update any database-specific code</li> <li>Migrate existing data if needed</li> </ol>"},{"location":"modules/database/overview/#adding-indexes","title":"Adding Indexes","text":"<p>To optimize specific queries:</p> <ol> <li>Add indexes to frequently queried columns</li> <li>Use SQLAlchemy's <code>Index</code> class</li> <li>Consider performance implications for writes</li> </ol>"},{"location":"modules/database/overview/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Database Models in detail</li> <li>Explore Database Connections for advanced usage</li> <li>Understand Database Migrations for schema changes</li> </ul>"},{"location":"modules/scraper/article-scraper/","title":"Article Scraper","text":"<p>The Article Scraper is a sophisticated component responsible for extracting content and metadata from Medium articles. It uses Playwright for browser automation to render JavaScript content and extract data that would be difficult or impossible to obtain with traditional HTTP requests.</p>"},{"location":"modules/scraper/article-scraper/#architecture","title":"Architecture","text":"<p>The Article Scraper follows a multi-threaded architecture with a worker pool design pattern:</p> <pre><code>graph TD\n    A[Entry Point: main] --&gt; B[Initialize Environment]\n    B --&gt; C[Create Thread Pool]\n    B --&gt; D[Fetch Random URLs]\n    D --&gt; E[Populate Task Queue]\n    C --&gt; F[Worker Threads]\n    E --&gt; F\n    F --&gt; G[Process Articles]\n    G --&gt; H[Extract Metadata]\n    G --&gt; I[Extract Content]\n    G --&gt; J[Extract Comments]\n    H --&gt; K[Persist Data]\n    I --&gt; K\n    J --&gt; K\n    G --&gt; L[Take Screenshots]\n    K --&gt; M[Update Metrics]\n    M --&gt; N[Final Report]</code></pre>"},{"location":"modules/scraper/article-scraper/#key-components","title":"Key Components","text":""},{"location":"modules/scraper/article-scraper/#main-execution-flow-main-function","title":"Main Execution Flow (<code>main</code> function)","text":"<pre><code>def main():\n    \"\"\"Main execution function.\"\"\"\n    global start_time\n    start_time = time.time()\n\n    signal.signal(signal.SIGINT, handle_signal)\n    signal.signal(signal.SIGTERM, handle_signal)\n\n    threads = []\n    metrics_thread = None\n\n    try:\n        # Use a session factory for thread safety\n        session_factory = SessionLocal\n        url_data = []\n\n        with session_factory() as session:  # Get URLs in a separate session\n            url_data = fetch_random_urls(session)\n\n        logging.debug(\n            f\"Starting to process {len(url_data)} URLs with {MAX_CONCURRENT} workers\"\n        )\n        task_queue = Queue()\n\n        def create_browser(playwright):\n            return playwright.chromium.launch(\n                headless=HEADLESS,\n                args=[\"--disable-blink-features=AutomationControlled\"],\n            )\n\n        for i in range(MAX_CONCURRENT):\n            thread = Thread(\n                target=worker_thread,\n                args=(task_queue, create_browser, session_factory),\n                daemon=True,\n            )\n            threads.append(thread)\n            thread.start()\n\n        metrics_stop = Event()\n        metrics_thread = Thread(\n            target=quiet_metrics_monitor, args=(metrics_stop,), daemon=True\n        )\n        metrics_thread.start()\n\n        for i, url in enumerate(url_data):\n            task_queue.put((url, i))\n\n        for _ in range(MAX_CONCURRENT):\n            task_queue.put(None)\n\n        while not task_queue.empty() and not shutdown_event.is_set():\n            time.sleep(1)\n\n        if shutdown_event.is_set():\n            logging.warning(\"Shutting down gracefully...\")\n\n        for thread in threads:\n            thread.join(timeout=5)\n        metrics_stop.set()\n\n        if metrics_thread:\n            metrics_thread.join(timeout=2)\n\n    except Exception as e:\n        logging.error(f\"Unhandled error: {e}\", exc_info=True)\n    finally:\n        display_final_metrics()\n</code></pre>"},{"location":"modules/scraper/article-scraper/#worker-thread-management","title":"Worker Thread Management","text":"<p>The Article Scraper uses a thread pool to process multiple articles concurrently:</p> <pre><code>def worker_thread(task_queue, browser_factory, session_factory):\n    \"\"\"Worker thread to process URLs.\"\"\"\n    while not shutdown_event.is_set():\n        try:\n            task = task_queue.get(timeout=1)\n            if task is None:\n                break\n            url_data, worker_idx = task\n            with sync_playwright() as p:\n                browser = browser_factory(p)\n                try:\n                    with session_factory() as session:  # Create a session for each task\n                        process_article(url_data, browser, worker_idx, session)\n                finally:\n                    try:\n                        browser.close()\n                    except:\n                        pass  # Ignore browser close errors\n                task_queue.task_done()\n        except Exception as e:\n            if not shutdown_event.is_set():\n                logging.error(f\"Worker thread error: {e}\")\n</code></pre>"},{"location":"modules/scraper/article-scraper/#article-processing","title":"Article Processing","text":"<p>Each article is processed by extracting its content, metadata, and comments:</p> <pre><code>def process_article(url_data, browser, worker_idx: int, session: Session):\n    \"\"\"Process an article: extract, save, and screenshot.\"\"\"\n    if shutdown_event.is_set():\n        return\n\n    url_id, url = url_data\n    success = False\n    metadata = {}  # Initialize metadata\n\n    try:\n        context = browser.new_context(\n            viewport={\"width\": 390, \"height\": 844},  # iPhone 12\n            user_agent=\"Mozilla/5.0 (iPhone; CPU iPhone OS 14_4 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.3 Mobile/15E148 Safari/604.1\",\n            locale=random.choice([\"en-US\", \"en-GB\"]),\n            device_scale_factor=2.0,\n        )\n        context.add_init_script(\n            \"Object.defineProperty(navigator,'webdriver',{get:()=&gt;false});\"\n        )\n\n        with context.new_page() as page:\n            logging.debug(f\"Processing URL: {url}\")\n            page.goto(url, wait_until=\"networkidle\", timeout=30000)\n            page.mouse.wheel(0, random.randint(100, 300))\n\n            metadata = extract_metadata_and_comments(page)\n\n            # Move persist_article_data inside the try block\n            if persist_article_data(\n                session, url_id, metadata\n            ):  # Check the return value\n                success = True\n\n            filename = f\"{worker_idx}_{datetime.now().strftime('%Y%m%d%H%M%S')}.png\"\n            page.screenshot(path=str(SCREENSHOT_DIR / filename), full_page=True)\n            logging.debug(f\"Saved screenshot to {filename}\")\n\n            update_metrics()  # Update metrics only on success\n\n    except Exception as e:\n        logging.error(f\"Error on {url}\")\n    finally:\n        update_url_status(session, url_id, success)\n</code></pre>"},{"location":"modules/scraper/article-scraper/#important-features","title":"Important Features","text":""},{"location":"modules/scraper/article-scraper/#browser-configuration","title":"Browser Configuration","text":"<p>The Article Scraper uses Playwright with Chromium in a carefully configured environment to avoid detection as a bot:</p> <pre><code>context = browser.new_context(\n    viewport={\"width\": 390, \"height\": 844},  # iPhone 12\n    user_agent=\"Mozilla/5.0 (iPhone; CPU iPhone OS 14_4 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.3 Mobile/15E148 Safari/604.1\",\n    locale=random.choice([\"en-US\", \"en-GB\"]),\n    device_scale_factor=2.0,\n)\ncontext.add_init_script(\n    \"Object.defineProperty(navigator,'webdriver',{get:()=&gt;false});\"\n)\n</code></pre> <p>This configuration: - Sets the viewport to simulate a mobile device (iPhone 12) - Uses a mobile user agent - Randomizes the locale between US and UK English - Sets an appropriate device scale factor for high-resolution displays - Adds a script to hide the fact that it's a WebDriver-controlled browser</p>"},{"location":"modules/scraper/article-scraper/#graceful-shutdown","title":"Graceful Shutdown","text":"<p>The Article Scraper includes signal handlers for graceful shutdown:</p> <pre><code>def handle_signal(signum, frame):\n    \"\"\"Signal handler for graceful shutdown.\"\"\"\n    logging.warning(f\"Received signal {signum}, initiating shutdown...\")\n    shutdown_event.set()\n</code></pre> <p>This allows the scraper to cleanly terminate when interrupted (e.g., with Ctrl+C) by: 1. Setting a shutdown event flag 2. Allowing worker threads to complete their current tasks 3. Releasing resources properly (closing browsers, committing database changes) 4. Displaying final metrics before exit</p>"},{"location":"modules/scraper/article-scraper/#performance-metrics","title":"Performance Metrics","text":"<p>The Article Scraper tracks and reports performance metrics:</p> <pre><code>def display_final_metrics():\n    \"\"\"Display final performance metrics.\"\"\"\n    global completed_tasks, start_time\n    elapsed_time = time.time() - start_time\n    speed = (\n        completed_tasks / (elapsed_time / 60)\n        if completed_tasks &gt; 0 and elapsed_time &gt; 0\n        else 0\n    )\n\n    logging.info(f\"=== FINAL PERFORMANCE SUMMARY ===\")\n    logging.info(f\"Total processed: {completed_tasks} articles\")\n    logging.info(f\"Average speed: {speed:.2f} articles/minute\")\n    logging.info(f\"Total time: {elapsed_time/60:.1f} minutes\")\n    if speed &gt; 0:\n        logging.info(f\"Processing time per article: {60/speed:.2f} seconds\")\n</code></pre> <p>This provides valuable information about: - Total articles processed - Processing speed (articles per minute) - Total runtime - Average time per article</p>"},{"location":"modules/scraper/article-scraper/#screenshot-capture","title":"Screenshot Capture","text":"<p>For debugging and verification purposes, the Article Scraper captures screenshots of each article:</p> <pre><code>filename = f\"{worker_idx}_{datetime.now().strftime('%Y%m%d%H%M%S')}.png\"\npage.screenshot(path=str(SCREENSHOT_DIR / filename), full_page=True)\nlogging.debug(f\"Saved screenshot to {filename}\")\n</code></pre>"},{"location":"modules/scraper/article-scraper/#configuration-options","title":"Configuration Options","text":"<p>The Article Scraper can be configured through command-line arguments in <code>__main__.py</code>:</p> <pre><code># Article scraper subparser\narticle_parser = subparsers.add_parser(\"article\", help=\"Scrape Medium articles\")\narticle_parser.add_argument(\n    \"--url-count\", type=int, help=\"Number of URLs to process\"\n)\narticle_parser.add_argument(\n    \"--headless\",\n    type=bool,\n    help=\"Run browser in headless mode (default: False)\",\n    default=None,\n    action=\"store_true\"\n)\narticle_parser.add_argument(\n    \"--workers\",\n    type=int,\n    help=\"Number of concurrent workers (default: 1)\",\n)\n</code></pre> <p>These options provide flexibility to control: - How many URLs to process in a single run - Whether to run browsers in headless mode (invisible) or with visible windows - How many worker threads to use for concurrent processing</p>"},{"location":"modules/scraper/article-scraper/#example-usage","title":"Example Usage","text":"<pre><code># Basic usage with default settings\npoetry run python -m scraper article\n\n# Process 100 URLs with 4 concurrent workers in headless mode\npoetry run python -m scraper article --url-count 100 --workers 4 --headless true\n\n# Process 10 URLs with visible browser windows (for debugging)\npoetry run python -m scraper article --url-count 10 --headless false\n</code></pre>"},{"location":"modules/scraper/article-scraper/#advanced-topics","title":"Advanced Topics","text":""},{"location":"modules/scraper/article-scraper/#handling-member-only-articles","title":"Handling Member-Only Articles","text":"<p>The Article Scraper can detect if an article is behind Medium's paywall:</p> <pre><code># Check for paywall indicators\nif page.query_selector('div[aria-label=\"Post Preview\"]') or page.query_selector(\n    \"div.paywall-upsell-container\"\n):\n    article_data[\"is_free\"] = \"Member-Only\"\n</code></pre> <p>This information is stored in the database, allowing you to filter articles based on their access level.</p>"},{"location":"modules/scraper/article-scraper/#extracting-comments","title":"Extracting Comments","text":"<p>The Article Scraper includes sophisticated logic to extract comments, including clicking the \"See all responses\" button and scrolling to load all comments:</p> <pre><code>def click_see_all_responses(page: Page) -&gt; bool:\n    \"\"\"Click 'See all responses' button.\"\"\"\n    try:\n        button = page.query_selector('button:has-text(\"See all responses\")')\n        if button:\n            button.click(timeout=10000)\n            page.wait_for_load_state(\"load\", timeout=10000)\n            return True\n    except Exception as e:\n        logger.warning(f\"Failed to click responses button\")\n    return False\n\n\ndef scroll_to_load_comments(page: Page, max_scrolls: int = 100) -&gt; None:\n    \"\"\"Scroll to load all comments.\"\"\"\n    html = page.content()\n    for _ in range(max_scrolls):\n        try:\n            page.evaluate(\n                \"\"\"() =&gt; {\n                    const dialog = document.querySelector('div[role=\"dialog\"]');\n                    if (dialog) dialog.lastElementChild.firstElementChild.scrollBy(0, 20000);\n                }\"\"\"\n            )\n            page.wait_for_timeout(500)\n            page.wait_for_load_state(\"load\", timeout=5000)\n            if page.content() == html:\n                break\n            html = page.content()\n        except Exception as e:\n            logger.warning(f\"Scroll error: {e}\")\n            break\n</code></pre>"},{"location":"modules/scraper/article-scraper/#error-handling","title":"Error Handling","text":"<p>The Article Scraper includes comprehensive error handling at multiple levels:</p> <ol> <li>Worker Thread Level: Catches exceptions in the worker thread and logs errors</li> <li>Processing Level: Catches exceptions during article processing and updates the URL status accordingly</li> <li>Browser Level: Ensures browser resources are properly released even if errors occur</li> <li>Database Level: Uses transactions with commit/rollback to maintain data integrity</li> </ol>"},{"location":"modules/scraper/article-scraper/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about the Medium Helpers module for detailed extraction logic</li> <li>Review the Sitemap Scraper that provides URLs to the Article Scraper</li> <li>Explore Configuration options for customizing the Article Scraper behavior</li> </ul>"},{"location":"modules/scraper/overview/","title":"Scraper Module Overview","text":"<p>The scraper module is the core component of the Medium-Mining project, responsible for extracting data from Medium.com. It consists of two main components:</p> <ol> <li>Sitemap Scraper: Extracts article URLs from Medium's sitemaps</li> <li>Article Scraper: Extracts content and metadata from individual articles</li> </ol>"},{"location":"modules/scraper/overview/#module-architecture","title":"Module Architecture","text":"<p>The scraper module is designed with a modular architecture that separates concerns and enables flexible extension:</p> <pre><code>graph TD\n    A[__main__.py] --&gt; B[Command Line Interface]\n    B --&gt; C[Sitemap Scraper]\n    B --&gt; D[Article Scraper]\n    C --&gt; E[scrape_sitemaps.py]\n    D --&gt; F[scrape_articles.py]\n    E --&gt; G[Database]\n    F --&gt; H[medium_helpers.py]\n    H --&gt; G</code></pre>"},{"location":"modules/scraper/overview/#key-components","title":"Key Components","text":""},{"location":"modules/scraper/overview/#command-line-interface-__main__py","title":"Command Line Interface (<code>__main__.py</code>)","text":"<p>The entry point for the scraper module that provides a command-line interface for running the sitemap and article scrapers. It handles argument parsing, logging configuration, and dispatches to the appropriate scraper based on the command.</p> <pre><code># Basic usage\npython -m scraper sitemap  # Run sitemap scraper\npython -m scraper article  # Run article scraper\n</code></pre>"},{"location":"modules/scraper/overview/#sitemap-scraper-scrape_sitemapspy","title":"Sitemap Scraper (<code>scrape_sitemaps.py</code>)","text":"<p>Responsible for retrieving and processing Medium's XML sitemaps, extracting article URLs, and storing them in the database.</p> <p>Key features: - Asynchronous processing of multiple sitemaps - Intelligent handling of rate limiting - Efficient batch processing for database operations - Automatic detection and skipping of already processed sitemaps</p>"},{"location":"modules/scraper/overview/#article-scraper-scrape_articlespy","title":"Article Scraper (<code>scrape_articles.py</code>)","text":"<p>Extracts detailed content and metadata from Medium articles, including text, author information, publication date, claps, comments, and more.</p> <p>Key features: - Multi-threaded processing for improved performance - Browser automation using Playwright - Handling of both member-only and public articles - Screenshot capture for debugging and verification - Performance metrics tracking</p>"},{"location":"modules/scraper/overview/#helper-functions-medium_helperspy","title":"Helper Functions (<code>medium_helpers.py</code>)","text":"<p>Contains utility functions used by the article scraper to extract specific types of data from Medium articles and interact with the database.</p> <p>Key features: - Article text extraction - Metadata extraction from JSON-LD - Comment extraction and processing - Database interaction utilities</p>"},{"location":"modules/scraper/overview/#execution-flow","title":"Execution Flow","text":"<p>The following sequence diagram illustrates the high-level execution flow of the scraper module:</p> <pre><code>sequenceDiagram\n    participant User\n    participant CLI as Command Line Interface\n    participant SS as Sitemap Scraper\n    participant AS as Article Scraper\n    participant DB as Database\n    participant Medium as Medium.com\n\n    User-&gt;&gt;CLI: Execute command\n\n    alt Sitemap Scraping\n        CLI-&gt;&gt;SS: Run sitemap scraper\n        SS-&gt;&gt;Medium: Request master sitemap\n        Medium-&gt;&gt;SS: Return master sitemap\n        SS-&gt;&gt;Medium: Request individual sitemaps\n        Medium-&gt;&gt;SS: Return individual sitemaps\n        SS-&gt;&gt;DB: Store sitemap data &amp; URLs\n        SS-&gt;&gt;CLI: Return results\n    else Article Scraping\n        CLI-&gt;&gt;AS: Run article scraper\n        AS-&gt;&gt;DB: Fetch random URLs\n        DB-&gt;&gt;AS: Return URLs\n        AS-&gt;&gt;Medium: Request articles\n        Medium-&gt;&gt;AS: Return article content\n        AS-&gt;&gt;DB: Store article data\n        AS-&gt;&gt;CLI: Return results\n    end\n\n    CLI-&gt;&gt;User: Display results</code></pre>"},{"location":"modules/scraper/overview/#configuration-options","title":"Configuration Options","text":"<p>The scraper module provides several configuration options to customize its behavior:</p>"},{"location":"modules/scraper/overview/#sitemap-scraper-options","title":"Sitemap Scraper Options","text":"Option Default Description <code>--timeout</code> 0.05 Average timeout between requests in seconds <code>--verbose</code> False Enable verbose output to console"},{"location":"modules/scraper/overview/#article-scraper-options","title":"Article Scraper Options","text":"Option Default Description <code>--url-count</code> 50 Number of URLs to process <code>--headless</code> False Run browser in headless mode <code>--workers</code> 1 Number of concurrent workers"},{"location":"modules/scraper/overview/#usage-examples","title":"Usage Examples","text":""},{"location":"modules/scraper/overview/#sitemap-scraper","title":"Sitemap Scraper","text":"<pre><code># Basic usage\npoetry run python -m scraper sitemap\n\n# With custom timeout and verbose output\npoetry run python -m scraper sitemap --timeout 0.1 --verbose\n</code></pre>"},{"location":"modules/scraper/overview/#article-scraper","title":"Article Scraper","text":"<pre><code># Basic usage\npoetry run python -m scraper article\n\n# With custom URL count, headless mode, and multiple workers\npoetry run python -m scraper article --url-count 100 --headless true --workers 4\n</code></pre>"},{"location":"modules/scraper/overview/#extension-points","title":"Extension Points","text":"<p>The scraper module is designed to be extensible. Here are the main points where you can extend or customize its functionality:</p> <ul> <li>Custom Extractors: Extend the extraction capabilities by adding new functions to <code>medium_helpers.py</code></li> <li>Alternative Browsers: Modify the browser configuration in <code>scrape_articles.py</code> to use different browser types or configurations</li> <li>Additional Metadata: Add new fields to the database models and corresponding extraction logic</li> <li>Custom Processing: Implement post-processing logic for extracted data</li> </ul>"},{"location":"modules/scraper/overview/#next-steps","title":"Next Steps","text":"<ul> <li>Learn more about the Sitemap Scraper</li> <li>Learn more about the Article Scraper</li> <li>Explore the Medium Helpers functions</li> <li>Review the Configuration options</li> </ul>"}]}