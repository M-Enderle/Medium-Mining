\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{lmodern} % scalable fonts for microtype
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{float}
\usepackage{fancyhdr}
\usepackage{titlesec}
\usepackage{enumitem}
\usepackage{tikz}
\usepackage[numbers]{natbib}
\usepackage{minted}
\usepackage{csquotes}

% Page setup
\geometry{margin=2.5cm}
\setlength{\parskip}{0.6em}
\setlength{\parindent}{0pt}

% Header and footer
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{Medium Mining}
\fancyhead[R]{\thepage}
\renewcommand{\headrulewidth}{0.4pt}
\setlength{\headheight}{14pt}

% Section formatting (more academic look)
\titleformat{\section}{\large\bfseries}{\thesection}{0.75em}{}
\titleformat{\subsection}{\normalsize\bfseries}{\thesubsection}{0.6em}{}
\titleformat{\subsubsection}{\normalsize\itshape}{\thesubsubsection}{0.5em}{}

% Code listing setup
\definecolor{codegreen}{rgb}{0,0.5,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.97,0.97,0.95}
\definecolor{mediumpurple}{HTML}{5038A5}
\definecolor{mediumblue}{HTML}{005F99}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{mediumblue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showstringspaces=false,
    tabsize=2
}
\lstset{style=mystyle}

% Hyperlinks
\hypersetup{
    colorlinks=true,
    linkcolor=mediumpurple,
    citecolor=mediumblue,
    urlcolor=mediumblue,
    pdftitle={Comparative Analysis of Paid and Free Articles on Medium.com},
    pdfauthor={Moritz Enderle}
}

% Custom commands
\newcommand{\todo}[1]{\textcolor{red}{\textbf{TODO:} #1}}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\important}[1]{\textbf{#1}}

\title{\Huge\bfseries Comparative Analysis of Paid and Free Articles on Medium.com}
\author{Florian Eder, Moritz Enderle}

\date{\today}

\begin{document}
\maketitle
\thispagestyle{empty}

\newpage
\tableofcontents
\newpage

\section*{Abstract}
\addcontentsline{toc}{section}{Abstract}

\todo{Write an abstract}

\section{Data Acquisition}

There is currently no publicly available dataset of articles on Medium.com we could use for our analysis. 
The platform also does not provide an official API for data access.
Therefore, we had to build a custom data acquisition pipeline to scrape articles from Medium.com, focusing on both paid and free content.

\subsection{Legal Considerations}

As this process involves web scraping, which is strictly prohibited by Medium's Terms of Service, we got in touch with Medium's legal department to clarify the situation and obtain permission for our academic project.

Although we received permission to proceed, we ensured our scraping methodology does not overload or negatively impact Medium's services. This includes:
\begin{itemize}
    \item Respectful crawling delays between requests
    \item Compliance with robots.txt directives where applicable
    \item Limiting the total number of scraped articles to a reasonable amount
\end{itemize}

All collected data is used exclusively for academic research purposes and will not be redistributed or commercialized.

\subsection{Page Discovery}

Traditional content discovery methods are mostly based around spiders crawling links from one page to another. However, this approach would introduce a significant bias, as most articles on Medium.com are linked only to other articles within the same field (e.g. technology articles link to other technology articles). This would lead to a dataset that is not representative of the overall article distribution on Medium.com.

This led us to discover Medium's sitemaps as a more suitable approach for page discovery. The main sitemap is located at \code{https://medium.com/sitemap/sitemap.xml} and contains references to 20440 individual sitemaps, each containing up to 7000 URLs. In total, this results in 32 million URLs [as of March 2025]. We completed the whole process over 6 hours with a Gaussian distributed delay with mean 0.05 seconds. From these URLs, we were able to filter out a portion of non-article URLs (e.g. user profiles, tag pages, etc.).
This finally resulted in 14702 individual sitemaps and a total of just over 30 Million URLs.

\subsection{Scraping the Articles}

The articles on Medium.com are dynamically loaded using JavaScript, which means a simple HTTP request to fetch the HTML content is not sufficient. To overcome this issue, employed playwright, a high level APIU for browser control, to fully render the pages before extracting the relevant data.

Due to the high number of articles to be scraped, we parallelized the process using multiple worker processes. Each worker operates independently, fetching URLs from the database, rendering the pages, and extracting the relevant data. 

We extracted most data directly from the rendered HTML using CSS selectors. However, some data points are embedded in JSON-LD structured data within the page, which we parsed to extract additional metadata.

For member-only (paid) articles, we implemented a method to access the full content by purchasing the Medium membership and using the associated cookies during scraping. 

\subsection{Database Storage}

We stored data in DuckDB, a columnar database optimized for analytical queries. We designed the schema to include tables for sitemaps, URLs, articles, authors, and comments, with relationships maintained through foreign keys. This enables efficient querying for our comparative analysis, as illustrated in Figure~\ref{fig:er_diagram}.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{images/ER.png}
    \label{fig:database_schema}
    \caption{Entity-relationship diagram of the scraping data model.}
    \label{fig:er_diagram}
\end{figure}

\subsection{Monitoring}

We implemented telemetry to track scraping health via structured worker logs aggregated into daily roll-ups for latency, error, and retry trends. We also configured the pipeline to push batch metrics to Weights \& Biases such as success ratios, premium hit rates, and fed into dashboards with alert thresholds.

\newpage

\section{Data Analysis}

\subsection{Dataset Overview}

Our initial scraping process, initiated from the 32 million URLs derived from the sitemaps, culminated in a raw dataset of $\mathbf{65,248}$ fully scraped articles.

\subsubsection*{Ensuring a Fair Comparison}

Medium.com introduced the ability for paid memberships in early 2017. As we want to ensure comparability between free and paid articles, taking into account any articles published before this date would introduce a significant bias. Furthermore, to allow for sufficient adoption of the commercial plan by authors and stabilize publication patterns, we only consider articles published after the first of January 2020. This filtering is illustrated in Figure~\ref{fig:articles_per_month}, which shows the distribution of articles per month from 2020 to 2025, highlighting the increasing proportion of paid content over time.

This filtering step resulted in a final analysis dataset of $\mathbf{33,510}$ articles published between January 2020 and May 2025, contributed by $\mathbf{24,639}$ unique authors, and containing a total of $\mathbf{83,064}$ responses.

Of this final, cleaned dataset, $\mathbf{33.6\%}$ ($\mathbf{11,262}$ articles) were classified as member-only (paid), and $\mathbf{66.4\%}$ ($\mathbf{22,248}$ articles) were free. This composition provides a strong basis for comparative statistical analysis. The primary data points successfully extracted for this analysis include the article's text, estimated reading time, clap count, response count, author follower count, and premium status.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{images/articles_per_month.png}
    \caption{Distribution of scraped articles per month (2020-2025), segmented by premium status. The chart illustrates the growing relative proportion of member-only content in the dataset over time.}
    \label{fig:articles_per_month}
\end{figure}

\subsection{Descriptive Statistics}

Table \ref{tab:descriptive_stats} presents the core descriptive statistics for the engagement and length metrics, segmented by the article's premium status. These statistics highlight a clear difference in average engagement, particularly in clap count, where paid articles appear to significantly outperform free articles.

\begin{table}[H]
    \centering
    \caption{Descriptive Statistics for Key Metrics (Post-Jan 2020)}
    \label{tab:descriptive_stats}
    \begin{tabular}{lrrrrr}
        \toprule
        \textbf{Metric} & \textbf{Status} & \textbf{N} & \textbf{Mean} & \textbf{Median} & \textbf{Std. Dev.} \\
        \midrule
        \textbf{Clap Count} & Paid & 11,262 & 407.9 & 207.0 & 519.0 \\
        & Free & 22,248 & 122.4 & 40.0 & 272.1 \\
        \midrule
        \textbf{Response Count} & Paid & 11,262 & 4.9 & 2.0 & 7.0 \\
        & Free & 22,248 & 1.2 & 0.0 & 3.1 \\
        \midrule
        \textbf{Reading Time (min)} & Paid & 11,262 & 6.0 & 5.0 & 2.9 \\
        & Free & 22,248 & 5.9 & 5.0 & 3.2 \\
        \bottomrule
    \end{tabular}
    \vspace{0.5em}
\end{table}

\subsubsection{Hypothesis Testing (Two-Sample t-tests)}

Due to the observed differences in mean engagement metrics and reading time, we performed independent two-sample t-tests to formally assess the statistical significance of these differences. Given the large sample sizes, the Central Limit Theorem allows us to proceed with t-tests despite the non-normality and heteroscedasticity of the underlying populations, focusing on differences in the sample means.

The null hypothesis ($H_0$) for each test is that there is no difference between the means of paid and free articles for a given metric ($\mu_{paid} = \mu_{free}$).

The results of the t-tests, presented in Table \ref{tab:t_test_results}, reveal statistically significant differences between paid and free articles for all examined metrics. For clap count and response count, the t-statistics exceed 50, with p-values effectively at zero, indicating extremely strong evidence against the null hypothesis of no difference in means. This suggests that paid articles have substantially higher levels of engagement compared to free articles. Conversely, for reading time, the t-statistic of 2.26 yields a p-value of 0.0235, which, while below the conventional 0.05 threshold, indicates a marginally significant difference, implying that paid articles are, on average, slightly longer in reading time than free ones. These findings underscore the potential impact of premium status on content engagement and length within the Medium.com ecosystem.

\begin{table}[H]
    \centering
    \caption{Two-Sample T-Test Results for Key Metrics}
    \label{tab:t_test_results}
    \begin{tabular}{lrr}
        \toprule
        \textbf{Metric} & \textbf{t-statistic} & \textbf{p-value} \\
        \midrule
        Clap Count & 54.690392 & < 0.001 \\
        Response Count & 53.863954 & < 0.001 \\
        Reading Time (min) & 2.264893 & 0.023528 \\
        \bottomrule
    \end{tabular}
\end{table}

\todo{maybe extend this part?}

\section{Topic Modeling}

\subsection{Introduction}

Topic modeling using embeddings represents a modern approach to uncovering thematic structures in text data, leveraging dense vector representations to capture semantic similarities. Unlike traditional methods like Latent Dirichlet Allocation (LDA), embedding-based techniques utilize pre-trained language models to generate contextual embeddings, allowing for more nuanced topic discovery that accounts for word polysemy and context.

In our analysis of Medium.com articles, we use embedding-based topic modeling to compare thematic distributions between paid and free content. This method also enables us to identify clusters of semantically similar articles and assess whether certain topics offer a higher propensity for paid content.

\subsection{Data Preprocessing \& Generation of Embeddings}

Preprocessing for embedding-based models is not as intensive as for traditional NLP models, as embeddings inherently capture semantic relationships. However, the scraped content was saved in a markdown format, which included various non-textual elements such as images, code snippets, and formatting syntax. To ensure the quality of the text data used for topic modeling, we implemented a preprocessing pipeline which removed these non-textual elements, retaining only the core textual content of each article. This step was crucial to prevent noise from affecting the embedding generation and subsequent topic modeling.

For generating embeddings, we utilized the \code{prdev/mini-gte} model developed by QTACK \cite{minigte} in early 2025. This distilled version of the original General Text Embeddings (GTE) model \cite{GTE} achieved remarkable performance in the MTEB v2 English Benchmark \cite{muennighoff2022mteb}, outperforming larger models while ensuring efficient use of memory and computational resources. This was especially suitable for our large dataset of Medium articles, allowing us to generate high-quality embeddings without prohibitive resource consumption.

\subsection{Methodology}

We conduct topic modeling through density-based clustering on a low-dimensional manifold learned from sentence embeddings. The overall workflow consists of the following steps:

\paragraph{Inputs and balanced sampling}
Two embedding matrices are loaded: one for free articles and one for member-only articles. A metadata file provides per-article labels and titles. To mitigate class imbalance, we apply stratified sampling, drawing \code{N\_SAMPLES\_PER\_CLASS = 11000} articles per class.

\paragraph{Manifold learning}
We reduce the sampled embeddings to three dimensions using UMAP \cite{mcinnes2020umapuniformmanifoldapproximation} with \code{n\_neighbors = 20}, \code{min\_dist = 0} and \code{n\_components = 3}. The choice of a small \code{min\_dist} emphasizes dense, well-separated aggregates to facilitate subsequent density-based clustering. The 3D space can be visualized easily while retaining sufficient structure for clustering.

\paragraph{Density-based clustering}
We fit DBSCAN \cite{DBSCAN} on the 3D UMAP coordinates using \code{eps = 0.3} and \code{min\_samples = 150}. DBSCAN is non-parametric in the number of clusters and explicitly models noise (label \code{-1}), which is appropriate for heterogeneous web text where some points do not belong to any dense theme. Cluster labels are assigned per point and carried forward for analysis.

\paragraph{Topic labeling from representative titles}
For interpretability, we derive short human-readable topic names for clusters. For each non-noise cluster, we select 50 representative article titles by sampling uniformly across the cluster's UMAP extent. We then label each cluster, trying to capture its thematic essence.

\paragraph{2D visualization}
We further map the non-noise 3D UMAP coordinates to two dimensions with t-SNE (\code{perplexity = 30}, \code{learning\_rate = 200}, \code{random\_state = 42}). This creates a 2D scatter plot where clusters can be visually separated and annotated with convex hulls. With color, we indicate the within-cluster free-to-paid article ratio.

\paragraph{Hypothesis tests within clusters}
Let $Y_i \in \{0,1\}$ be the indicator for article $i$ being free (1) or paid (0). Let $p_0$ denote the overall free proportion in the full embedding population ($p_0 = 0.5$), computed as the number of embedded free articles divided by the total across both classes. For each non-noise cluster $c$, we test the null hypothesis $H_0: \mathbb{E}[Y\,|\,c] = p_0$ against the two-sided alternative using a one-sample t-test on the cluster's $Y$ values. We report cluster size, sample mean $\hat p_c$, t-statistic, and p-value in Table~\ref{tab:topic_proportions}.

\subsection{Results}

Figure \ref{fig:2d_clusters} presents the 2D t-SNE visualization of the clustered articles, with color coding to indicate the proportion of free articles within each cluster. A clear gradient emerges from left to right, with clusters on the left predominantly free (green) and those on the right predominantly paid (red). The thematic labels assigned to each cluster reveal, that this gradient corresponds to a shift from tech-based topics (e.g., software engineering, cloud computing, AI) to more personal and lifestyle-oriented themes (e.g., relationships, personal growth, health).

We can further group the identified clusters into higher-level themes based on their labels. Table~\ref{tab:higher_level_clusters} summarizes these themes, along with their sizes, average share of free articles, and variances. In this table, the previously observed gradient is reinforced: technical themes such as "Emerging Tech \& Engineering" exhibit higher average ratio (0.635), while personal themes like "Health \& Well-being" show a lower ratio (0.167).

A special mention goes to the "Crypto \& Web3" cluster, which stands out with the highest mean free proportion of 0.862. This suggests that articles in this domain are mostly free.

Statistical significance was assessed for each cluster using one-sample t-tests comparing the proportion of free articles within the cluster to the overall population proportion of 0.5. Almost all clusters exhibited highly significant deviations (p < 0.001), indicating that the distribution of free versus paid articles is not uniform across topics. This further supports the notion that the topic of an article is strongly associated with its premium status on Medium.com.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{images/2D_Cluster_Free_Paid_Ratio.png}
    \caption{2D Visualization of Topic Clusters illustrating the Ratio of Free to Paid Articles. The Color Gradient from Green to Red Indicates Increasing Proportion of Paid Articles within Each Cluster.}
    \label{fig:2d_clusters}
\end{figure}

\begin{table}[H]
    \centering
        \label{tab:higher_level_clusters}
        \begin{tabular}{l r r r}
            \toprule
            \textbf{Theme} & \textbf{Size} & \textbf{Mean Free} & \textbf{Variance} \\
            \midrule
            Crypto \& Web3 & \textbf{1,489} & \textbf{0.862} & \textbf{0.119} \\
            Emerging Tech \& Engineering & \textbf{5,746} & \textbf{0.635} & \textbf{0.225} \\
            Product, Business \& Growth & \textbf{1,973} & \textbf{0.521} & \textbf{0.239} \\
            Society, Culture \& Global & \textbf{1,487} & \textbf{0.331} & \textbf{0.217} \\
            Personal \& Emotional Life & \textbf{2,515} & \textbf{0.240} & \textbf{0.183} \\
            Health \& Well-being & \textbf{492} & \textbf{0.167} & \textbf{0.144} \\
            Creativity \& Curiosity & \textbf{1,358} & \textbf{0.520} & \textbf{0.216} \\
            \bottomrule
        \end{tabular}
        \caption{Summary of Higher-Level Topic Clusters with Mean Free Proportions}
\end{table}

\subsection{Discussion}

Our embedding-based topic modeling reveals a pronounced thematic divide on Medium.com, with technical domains like "Crypto \& Web3" and "Emerging Tech \& Engineering" predominantly free (e.g., 86.2\% free in crypto), potentially to foster community engagement and attract a broad readership. Conversely, personal and lifestyle themes such as "Health \& Well-being" and "Relationship Dynamics" lean heavily toward paid content (e.g., 16.7\% free in health), suggesting authors monetize intimate or specialized narratives. The t-SNE visualization's left-to-right gradient underscores this shift from collaborative tech discourse to introspective storytelling, supported by highly significant t-test results (p < 0.001 for most clusters), indicating topic choice strongly predicts premium status.

Beyond engagement disparities, this pattern may reflect Medium's ecosystem dynamics: free tech articles could serve as lead magnets for building author followings, while paid personal content caters to readers seeking depth or exclusivity. However, limitations include potential biases in embedding models toward mainstream topics and the subjective nature of cluster labeling. 

\section{AI Generated Content}

\todo{Flo's Teil}

\section{Conclusion}

\todo{Write a conclusion}

\newpage

\appendix
\section{Appendix}

\begin{table}[H]
    \centering
    \label{tab:topic_proportions}
    \begin{tabular}{lrrrr}
        \toprule
        \textbf{Topic} & \textbf{Size} & \textbf{Mean Ratio} & \textbf{t-statistic} & \textbf{p-value} \\
        \midrule
        Community \& Crypto Updates & \textbf{1489} & \textbf{0.138} & \textbf{40.563} & \textbf{0.000} \\
        Tech Insights & \textbf{765} & \textbf{0.278} & \textbf{13.663} & \textbf{0.000} \\
        Sports and Identity & \textbf{155} & \textbf{0.284} & \textbf{5.949} & \textbf{0.000} \\
        Cloud \& DevOps & \textbf{1004} & \textbf{0.291} & \textbf{14.586} & \textbf{0.000} \\
        Mobile Development Patterns & \textbf{477} & \textbf{0.338} & \textbf{7.496} & \textbf{0.000} \\
        Digital Design \& Tech & \textbf{528} & \textbf{0.343} & \textbf{7.603} & \textbf{0.000} \\
        Data Engineering Insights & \textbf{489} & \textbf{0.354} & \textbf{6.755} & \textbf{0.000} \\
        Frontend \& Mobile Dev & \textbf{564} & \textbf{0.363} & \textbf{6.735} & \textbf{0.000} \\
        Software Engineering Practices & \textbf{694} & \textbf{0.393} & \textbf{5.746} & \textbf{0.000} \\
        Career Growth Strategies & \textbf{457} & \textbf{0.407} & \textbf{4.042} & \textbf{0.000} \\
        AI \& Future Tech & \textbf{1720} & \textbf{0.441} & \textbf{4.952} & \textbf{0.000} \\
        Product Management Dynamics & 216 & 0.458 & 1.226 & 0.221 \\
        Programming Concepts & 354 & 0.475 & 0.957 & 0.339 \\
        Global Challenges \& Conspiracy & 165 & 0.552 & -1.326 & 0.187 \\
        Curious Collections & \textbf{539} & \textbf{0.579} & \textbf{-3.704} & \textbf{0.000} \\
        Future \& Innovation & \textbf{180} & \textbf{0.583} & \textbf{-2.261} & \textbf{0.025} \\
        Entertainment \& Culture & \textbf{344} & \textbf{0.599} & \textbf{-3.735} & \textbf{0.000} \\
        Creator Resources Hub & \textbf{383} & \textbf{0.606} & \textbf{-4.229} & \textbf{0.000} \\
        Tech Reviews \& Tips & \textbf{264} & \textbf{0.648} & \textbf{-5.015} & \textbf{0.000} \\
        Controversial Figures \& Events & \textbf{603} & \textbf{0.703} & \textbf{-10.910} & \textbf{0.000} \\
        Existential Encounters & \textbf{210} & \textbf{0.710} & \textbf{-6.672} & \textbf{0.000} \\
        Personal Growth Insights & \textbf{1021} & \textbf{0.721} & \textbf{-15.725} & \textbf{0.000} \\
        Financial Insights \& Opportunities & \textbf{197} & \textbf{0.746} & \textbf{-7.920} & \textbf{0.000} \\
        Warfare \& Geopolitics & \textbf{153} & \textbf{0.765} & \textbf{-7.694} & \textbf{0.000} \\
        Health \& Wellness & \textbf{292} & \textbf{0.784} & \textbf{-11.788} & \textbf{0.000} \\
        Relationship Dynamics & \textbf{1097} & \textbf{0.820} & \textbf{-27.503} & \textbf{0.000} \\
        Personal Growth \& Finance & \textbf{370} & \textbf{0.832} & \textbf{-17.098} & \textbf{0.000} \\
        Dark Lives \& Secrets & \textbf{200} & \textbf{0.870} & \textbf{-15.520} & \textbf{0.000} \\
        Relationship Dynamics \& Trauma & \textbf{187} & \textbf{0.872} & \textbf{-15.155} & \textbf{0.000} \\
        \bottomrule
    \end{tabular}
    \caption{Proportion of Free Articles per Topic with Cluster Size and T-Test Results}
\end{table}

\newpage

\bibliographystyle{plain}
\bibliography{references}

\end{document}