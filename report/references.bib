@misc{minigte,
  author={QTACK},
  title={mini-gte: A Compact Text Embedding Model},
  year={2024},
  howpublished={Hugging Face},
  url={https://huggingface.co/prdev/mini-gte},
  note={Accessed: 2025-10-17}
}

@misc{GTE,
      title={Towards General Text Embeddings with Multi-stage Contrastive Learning}, 
      author={Zehan Li and Xin Zhang and Yanzhao Zhang and Dingkun Long and Pengjun Xie and Meishan Zhang},
      year={2023},
      eprint={2308.03281},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2308.03281}, 
}

@article{muennighoff2022mteb,
  author = {Muennighoff, Niklas and Tazi, Nouamane and Magne, Lo{\"\i}c and Reimers, Nils},
  title = {MTEB: Massive Text Embedding Benchmark},
  publisher = {arXiv},
  journal={arXiv preprint arXiv:2210.07316},
  year = {2022},
  url = {https://arxiv.org/abs/2210.07316},
  doi = {10.48550/ARXIV.2210.07316},
}

% Flo :D

@misc{openaiIntroducingChatGPT,
	author = {OpenAI},
	title = {{I}ntroducing {C}hat{G}{P}{T}},
	howpublished = {OpenAI},
    url={https://openai.com/index/chatgpt/},
	year = {2022},
	note = {[Accessed: 2025-10-22]},
}

@misc{raid,
      title={RAID: A Shared Benchmark for Robust Evaluation of Machine-Generated Text Detectors}, 
      author={Liam Dugan and Alyssa Hwang and Filip Trhlik and Josh Magnus Ludan and Andrew Zhu and Hainiu Xu and Daphne Ippolito and Chris Callison-Burch},
      year={2024},
      eprint={2405.07940},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.07940}, 
}

@misc{huggingfaceSuperAnnotateaidetectorlowfprHugging,
	author = {SuperAnnotate},
	title = {{S}uper{A}nnotate/ai-detector-low-fpr},
	howpublished = {Hugging Face},
  url={https://huggingface.co/SuperAnnotate/ai-detector-low-fpr},
	year = {2024},
	note = {[Accessed: 2025-10-24]},
}

@phdthesis{languagetool,
    author = {Naber, Daniel},
    year = {2003},
    month = {08},
    day = {28},
    pages = {},
    title = {A Rule-Based Style and Grammar Checker},
    school = {Technische Fakultät, Universität Bielefeld},
    url = {https://www.danielnaber.de/languagetool/download/style_and_grammar_checker.pdf}
}

@misc{mcinnes2020umapuniformmanifoldapproximation,
      title={UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction}, 
      author={Leland McInnes and John Healy and James Melville},
      year={2020},
      eprint={1802.03426},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1802.03426}, 
}

@inproceedings{DBSCAN,
  author = {Ester, Martin and Kriegel, Hans-Peter and Sander, J\"{o}rg and Xu, Xiaowei},
  title = {A density-based algorithm for discovering clusters in large spatial databases with noise},
  year = {1996},
  publisher = {AAAI Press},
  abstract = {Clustering algorithms are attractive for the task of class identification in spatial databases. However, the application to large spatial databases rises the following requirements for clustering algorithms: minimal requirements of domain knowledge to determine the input parameters, discovery of clusters with arbitrary shape and good efficiency on large databases. The well-known clustering algorithms offer no solution to the combination of these requirements. In this paper, we present the new clustering algorithm DBSCAN relying on a density-based notion of clusters which is designed to discover clusters of arbitrary shape. DBSCAN requires only one input parameter and supports the user in determining an appropriate value for it. We performed an experimental evaluation of the effectiveness and efficiency of DBSCAN using synthetic data and real data of the SEQUOIA 2000 benchmark. The results of our experiments demonstrate that (1) DBSCAN is significantly more effective in discovering clusters of arbitrary shape than the well-known algorithm CLAR-ANS, and that (2) DBSCAN outperforms CLARANS by a factor of more than 100 in terms of efficiency.},
  booktitle = {Proceedings of the Second International Conference on Knowledge Discovery and Data Mining},
  pages = {226–231},
  numpages = {6},
  keywords = {handling nlj4-275oise, efficiency on large spatial databases, clustering algorithms, arbitrary shape of clusters},
  location = {Portland, Oregon},
  series = {KDD'96}
}

@misc{jelodar2018latentdirichletallocationlda,
      title={Latent Dirichlet Allocation (LDA) and Topic modeling: models, applications, a survey}, 
      author={Hamed Jelodar and Yongli Wang and Chi Yuan and Xia Feng and Xiahui Jiang and Yanchao Li and Liang Zhao},
      year={2018},
      eprint={1711.04305},
      archivePrefix={arXiv},
      primaryClass={cs.IR},
      url={https://arxiv.org/abs/1711.04305}, 
}

@article{tacl_a_00325,
    author = {Dieng, Adji B. and Ruiz, Francisco J. R. and Blei, David M.},
    title = {Topic Modeling in Embedding Spaces},
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {8},
    pages = {439-453},
    year = {2020},
    month = {07},
    abstract = {Topic modeling analyzes documents to learn meaningful patterns of words. However, existing topic models fail to learn interpretable topics when working with large and heavy-tailed vocabularies. To this end, we develop the embedded topic model (etm), a generative model of documents that marries traditional topic models with word embeddings. More specifically, the etm models each word with a categorical distribution whose natural parameter is the inner product between the word’s embedding and an embedding of its assigned topic. To fit the etm, we develop an efficient amortized variational inference algorithm. The etm discovers interpretable topics even with large vocabularies that include rare words and stop words. It outperforms existing document models, such as latent Dirichlet allocation, in terms of both topic quality and predictive performance.},
    issn = {2307-387X},
    doi = {10.1162/tacl_a_00325},
    url = {https://doi.org/10.1162/tacl_a_00325},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00325/1923074/tacl_a_00325.pdf},
}



